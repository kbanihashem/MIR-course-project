[
{"id": "f90720ed12e045ac84beb94c27271d6fb8ad48cf", "title": "The Lottery Ticket Hypothesis: Training Pruned Neural Networks", "authors": ["Jonathan Frankle", "Michael Carbin"], "date": "2018", "abstract": "Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets,\" meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. \nThis paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. \nThe lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.", "references": ["https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Data-free-Parameter-Pruning-for-Deep-Neural-Srinivas-Babu/b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "https://www.semanticscholar.org/paper/Understanding-Dropout-Baldi-Sadowski/cc46229a7c47f485e090857cbab6e6bf68c09811", "https://www.semanticscholar.org/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f", "https://www.semanticscholar.org/paper/ThiNet%3A-A-Filter-Level-Pruning-Method-for-Deep-Luo-Wu/049fd80f52c0b1fa4d532945d95a24734b62bdf3", "https://www.semanticscholar.org/paper/Diversity-Networks%3A-Neural-Network-Compression-Mariet-Sra/2dfef5635c8c44431ca3576081e6cfe6d65d4862", "https://www.semanticscholar.org/paper/A-Deep-Neural-Network-Compression-Pipeline%3A-Huffman-Han-Mao/397de65a9a815ec39b3704a79341d687205bc80a", "https://www.semanticscholar.org/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "https://www.semanticscholar.org/paper/Optimal-Brain-Surgeon-and-general-network-pruning-Hassibi-Stork/e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724"]},
{"id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992", "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova"], "date": "2019", "abstract": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. \nBERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).", "references": ["https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectional-Peters-Ammar/0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "https://www.semanticscholar.org/paper/Semi-Supervised-Sequence-Modeling-with-Cross-View-Clark-Luong/0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "https://www.semanticscholar.org/paper/Dissecting-Contextual-Word-Embeddings%3A-Architecture-Peters-Neumann/ac11062f1f368d97f4c826c317bf50dcc13fdb59", "https://www.semanticscholar.org/paper/QANet%3A-Combining-Local-Convolution-with-Global-for-Yu-Dohan/8c1b00128e74f1cd92aede3959690615695d5101", "https://www.semanticscholar.org/paper/GLUE%3A-A-Multi-Task-Benchmark-and-Analysis-Platform-Wang-Singh/93b8da28d006415866bf48f9a6e06b5242129195", "https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600", "https://www.semanticscholar.org/paper/Character-Level-Language-Modeling-with-Deeper-Al-Rfou-Choe/b9de9599d7241459db9213b5cdd7059696f5ef8d", "https://www.semanticscholar.org/paper/U-Net%3A-Machine-Reading-Comprehension-with-Questions-Sun-Li/27e98e09cf09bc13c913d01676e5f32624011050", "https://www.semanticscholar.org/paper/Skip-Thought-Vectors-Kiros-Zhu/6e795c6e9916174ae12349f5dc3f516570c17ce8"]},
{"id": "397de65a9a815ec39b3704a79341d687205bc80a", "title": "A Deep Neural Network Compression Pipeline: Pruning, Quantization, Huffman Encoding", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "date": "2015", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, We introduce a three stage pipeline: pruning, quantization and Huffman encoding, that work together to reduce the storage requirement of neural networks by 35\u00d7 to 49\u00d7 without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman encoding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9\u00d7 to 13\u00d7; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35\u00d7, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG16 by 49\u00d7 from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory, which has 180\u00d7 less access energy.", "references": ["https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Compressing-Deep-Convolutional-Networks-using-Gong-Liu/e7bf9803705f2eb608db1e59e5c7636a3f171916", "https://www.semanticscholar.org/paper/Memory-Bounded-Deep-Convolutional-Networks-Collins-Kohli/2a4117849c88d4728c33b1becaa9fb6ed7030725", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/Compressing-Neural-Networks-with-the-Hashing-Trick-Chen-Wilson/efb5032e6199c80f83309fd866b25be9545831fd", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327", "https://www.semanticscholar.org/paper/Deep-Fried-Convnets-Yang-Moczulski/27a99c21a1324f087b2f144adc119f04137dfd87"]},
{"id": "2dfef5635c8c44431ca3576081e6cfe6d65d4862", "title": "Diversity Networks: Neural Network Compression Using Determinantal Point Processes", "authors": ["Zelda Mariet", "Suvrit Sra"], "date": "2015", "abstract": "We introduce Divnet, a flexible technique for learning networks with diverse neurons. Divnet models neuronal diversity by placing a Determinantal Point Process (DPP) over neurons in a given layer. It uses this DPP to select a subset of diverse neurons and subsequently fuses the redundant neurons into the selected ones. Compared with previous approaches, Divnet offers a more principled, flexible technique for capturing neuronal diversity and thus implicitly enforcing regularization. This enables effective auto-tuning of network architecture and leads to smaller network sizes without hurting performance. Moreover, through its focus on diversity and neuron fusing, Divnet remains compatible with other procedures that seek to reduce memory footprints of networks. We present experimental results to corroborate our claims: for pruning neural networks, Divnet is seen to be notably superior to competing approaches.", "references": ["https://www.semanticscholar.org/paper/Data-free-Parameter-Pruning-for-Deep-Neural-Srinivas-Babu/b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Efficient-Sampling-for-k-Determinantal-Point-Li-Jegelka/d7bfd8a283a7ed8b43255cfd04909484cd3ade28", "https://www.semanticscholar.org/paper/Reshaping-deep-neural-network-for-fast-decoding-by-He-Fan/87d810fcea61068e8b29f2b75fa1cbb00c190bea", "https://www.semanticscholar.org/paper/Determinantal-Point-Processes-for-Machine-Learning-Kulesza-Taskar/48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016", "https://www.semanticscholar.org/paper/k-DPPs%3A-Fixed-Size-Determinantal-Point-Processes-Kulesza-Taskar/ec46bcbced500820521e9f65b0f9ffef5a83ae11", "https://www.semanticscholar.org/paper/Second-Order-Derivatives-for-Network-Pruning%3A-Brain-Hassibi-Stork/a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "https://www.semanticscholar.org/paper/A-Determinantal-Point-Process-Latent-Variable-Model-Snoek-Zemel/31f88db95eb5c66b95cd7335b0cd4f27f0f271f2", "https://www.semanticscholar.org/paper/Compressing-Neural-Networks-with-the-Hashing-Trick-Chen-Wilson/efb5032e6199c80f83309fd866b25be9545831fd", "https://www.semanticscholar.org/paper/Fast-Determinantal-Point-Process-Sampling-with-to-Kang/8ba555d9587688bd3225d71ef9d686dad288e1f1"]},
{"id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776", "title": "Attention is All you Need", "authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N. Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "date": "2017", "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.", "references": ["https://www.semanticscholar.org/paper/Deep-Recurrent-Models-with-Fast-Forward-Connections-Zhou-Cao/b60abe57bc195616063be10638c6437358c81d1e", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-in-Linear-Time-Kalchbrenner-Espeholt/98445f4172659ec5e891e031d8202c102135c644", "https://www.semanticscholar.org/paper/A-Deep-Reinforced-Model-for-Abstractive-Paulus-Xiong/032274e57f7d8b456bd255fe76b909b2c1d7458e", "https://www.semanticscholar.org/paper/Can-Active-Memory-Replace-Attention-Kaiser-Bengio/735d547fc75e0772d2a78c46a1cc5fad7da1474c", "https://www.semanticscholar.org/paper/End-To-End-Memory-Networks-Sukhbaatar-Szlam/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "https://www.semanticscholar.org/paper/Structured-Attention-Networks-Kim-Denton/13d9323a8716131911bfda048a40e2cde1a76a46", "https://www.semanticscholar.org/paper/Multi-task-Sequence-to-Sequence-Learning-Luong-Le/d76c07211479e233f7c6a6f32d5346c983c5598f", "https://www.semanticscholar.org/paper/Convolutional-Sequence-to-Sequence-Learning-Gehring-Auli/43428880d75b3a14257c3ee9bda054e61eb869c0", "https://www.semanticscholar.org/paper/Outrageously-Large-Neural-Networks%3A-The-Layer-Shazeer-Mirhoseini/510e26733aaff585d65701b9f1be7ca9d5afc586"]},
{"id": "cc46229a7c47f485e090857cbab6e6bf68c09811", "title": "Understanding Dropout", "authors": ["Pierre Baldi", "Peter Sadowski"], "date": "2013", "abstract": "Dropout is a relatively new algorithm for training neural networks which relies on stochastically \"dropping out\" neurons during training in order to avoid the co-adaptation of feature detectors. We introduce a general formalism for studying dropout on either units or connections, with arbitrary probability values, and use it to analyze the averaging and regularizing properties of dropout in both linear and non-linear networks. For deep neural networks, the averaging properties of dropout are characterized by three recursive equations, including the approximation of expectations by normalized weighted geometric means. We provide estimates and bounds for these approximations and corroborate the results with simulations. Among other results, we also show how dropout performs stochastic gradient descent on a regularized error function.", "references": ["https://www.semanticscholar.org/paper/The-dropout-learning-algorithm-Baldi-Sadowski/327d3df8ea2020882827d6bace1e26c9d24309c2", "https://www.semanticscholar.org/paper/Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava/1366de5bb112746a555e9c0cd00de3ad8628aea8", "https://www.semanticscholar.org/paper/Improving-Neural-Networks-with-Dropout-Srivastava/5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "https://www.semanticscholar.org/paper/Stochastic-Learning-Bottou/7b0db6135b8dd3e2a9efa86163e91c0cd0fdf660", "https://www.semanticscholar.org/paper/A-CONVERGENCE-THEOREM-FOR-NON-NEGATIVE-ALMOST-AND-Robbins-Siegmund/0688fbcbfb08d7b91238bc90589209b31f97290f", "https://www.semanticscholar.org/paper/A-refinement-of-the-arithmetic-mean-geometric-mean-Cartwright-Field/7ab5ceb40c0e267ea6fcdbcaedd327d7b263bb8e", "https://www.semanticscholar.org/paper/On-the-Ky-Fan-inequality-and-related-inequalities-Neuman-S%C3%A1ndor/ba15f09796d53adfbe9e78cf79182e59b6045543", "https://www.semanticscholar.org/paper/Online-Algorithms-and-Stochastic-Approximations-Bottou/fc6b1ff29f2da985cccfa644652bb320d7720d59"]},
{"id": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "title": "Pruning Filters for Efficient ConvNets", "authors": ["Hao Li", "Asim Kadav", "Igor Durdanovic", "Hanan Samet", "Hans Peter Graf"], "date": "2016", "abstract": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks.", "references": ["https://www.semanticscholar.org/paper/Structured-Pruning-of-Deep-Convolutional-Neural-Anwar-Hwang/7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "https://www.semanticscholar.org/paper/Sparse-Convolutional-Neural-Networks-Liu-Wang/d559dd84fc473fca7e91b9075675750823935afa", "https://www.semanticscholar.org/paper/Less-Is-More%3A-Towards-Compact-CNNs-Zhou-Alvarez/3ed94217fbf29b86d5f1baec90dc33adacb40b58", "https://www.semanticscholar.org/paper/Convolutional-neural-networks-with-low-rank-Tai-Xiao/d5b4721c8188269b120d3d06149a04435753e755", "https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Convolutional-neural-networks-at-constrained-time-He-Sun/8ad35df17ae4064dd174690efb04d347428f1117", "https://www.semanticscholar.org/paper/Training-CNNs-with-Low-Rank-Filters-for-Efficient-Ioannou-Robertson/751c8884c1e857e675d85d8594c5f9b608005ed5", "https://www.semanticscholar.org/paper/A-Deep-Neural-Network-Compression-Pipeline%3A-Huffman-Han-Mao/397de65a9a815ec39b3704a79341d687205bc80a", "https://www.semanticscholar.org/paper/Efficient-and-accurate-approximations-of-nonlinear-Zhang-Zou/b64601d509711468f5d085261d463846f36785b2", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff"]},
{"id": "e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724", "title": "Optimal Brain Surgeon and general network pruning", "authors": ["Babak Hassibi", "David G. Stork", "Gregory J. Wolff"], "date": "1993", "abstract": "The use of information from all second-order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and, in some cases, enable rule extraction, is investigated. The method, Optimal Brain Surgeon (OBS), is significantly better than magnitude-based methods and Optimal Brain Damage, which often remove the wrong weights. OBS, permits pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H/sup -1/ from training data and structural information of the set. OBS deletes the correct weights from a trained XOR network in every case.<<ETX>>", "references": ["https://www.semanticscholar.org/paper/Second-Order-Derivatives-for-Network-Pruning%3A-Brain-Hassibi-Stork/a42954d4b9d0ccdf1036e0af46d87a01b94c3516", "https://www.semanticscholar.org/paper/A-Frobenius-approximation-reduction-method-(FARM)-Kung-Hu/5887de8eed53c444b2ef93d8ab9c8cc685cd7ac5", "https://www.semanticscholar.org/paper/Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg/de996c32045df6f7b404dda2a753b6a9becf3c08", "https://www.semanticscholar.org/paper/Interpretation-of-Artificial-Neural-Networks%3A-into-Towell-Shavlik/1b29884885401d12299a01b0eae099f425dd32e1", "https://www.semanticscholar.org/paper/Learning-internal-representations-by-error-Rumelhart-Hinton/111fd833a4ae576cfdbb27d87d2f8fc0640af355", "https://www.semanticscholar.org/paper/Introduction-to-the-Theory-of-Neural-Computation-Hertz-Krogh/57dc98cfb48247b400cc8decb93380e022864905"]},
{"id": "b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "title": "Data-free Parameter Pruning for Deep Neural Networks", "authors": ["Suraj Srinivas", "R. Venkatesh Babu"], "date": "2015", "abstract": "Deep Neural nets (NNs) with millions of parameters are at the heart of many state-of-the-art computer vision systems today. However, recent works have shown that much smaller models can achieve similar levels of performance. In this work, we address the problem of pruning parameters in a trained NN model. Instead of removing individual weights one at a time as done in previous works, we remove one neuron at a time. We show how similar neurons are redundant, and propose a systematic way to remove them. Our experiments in pruning the densely connected layers show that we can remove upto 85\\% of the total parameters in an MNIST-trained network, and about 35\\% for AlexNet without significantly affecting performance. Our method can be applied on top of most networks with a fully connected layer to give a smaller network.", "references": ["https://www.semanticscholar.org/paper/Memory-Bounded-Deep-Convolutional-Networks-Collins-Kohli/2a4117849c88d4728c33b1becaa9fb6ed7030725", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/High-Performance-Neural-Networks-for-Visual-Object-Ciresan-Meier/82b9099ddf092463f497bd48bb112c46ca52c4d1", "https://www.semanticscholar.org/paper/FitNets%3A-Hints-for-Thin-Deep-Nets-Romero-Ballas/cd85a549add0c7c7def36aca29837efd24b24080", "https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108", "https://www.semanticscholar.org/paper/Distilling-the-Knowledge-in-a-Neural-Network-Hinton-Vinyals/0c908739fbff75f03469d13d4a1a07de3414ee19", "https://www.semanticscholar.org/paper/Predicting-Parameters-in-Deep-Learning-Denil-Shakibi/e8650503ab80ad7299f0845b1843abf3a97f313a"]},
{"id": "642d0f49b7826adcf986616f4af77e736229990f", "title": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding", "authors": ["Song Han", "Huizi Mao", "William J. Dally"], "date": "2016", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.", "references": ["https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/Compressing-Deep-Convolutional-Networks-using-Gong-Liu/e7bf9803705f2eb608db1e59e5c7636a3f171916", "https://www.semanticscholar.org/paper/Fixed-point-optimization-of-deep-convolutional-for-Anwar-Hwang/a6373454105df0c5511ca5f6cae4d20c48214272", "https://www.semanticscholar.org/paper/Memory-Bounded-Deep-Convolutional-Networks-Collins-Kohli/2a4117849c88d4728c33b1becaa9fb6ed7030725", "https://www.semanticscholar.org/paper/Compressing-Neural-Networks-with-the-Hashing-Trick-Chen-Wilson/efb5032e6199c80f83309fd866b25be9545831fd", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/Deep-Fried-Convnets-Yang-Moczulski/27a99c21a1324f087b2f144adc119f04137dfd87", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Caffe%3A-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer/6bdb186ec4726e00a8051119636d4df3b94043b5"]},
{"id": "34f25a8704614163c4095b3ee2fc969b60de4698", "title": "Dropout: a simple way to prevent neural networks from overfitting", "authors": ["Nitish Srivastava", "Geoffrey E. Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "date": "2014", "abstract": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.", "references": ["https://www.semanticscholar.org/paper/Improving-Neural-Networks-with-Dropout-Srivastava/5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086", "https://www.semanticscholar.org/paper/Fast-dropout-training-Wang-Manning/ec92efde21707ddf4b81f301cd58e2051c1a2443", "https://www.semanticscholar.org/paper/Bayesian-learning-for-neural-networks-Hinton-Neal/db869fa192a3222ae4f2d766674a378e47013b1b", "https://www.semanticscholar.org/paper/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero/8978cf7574ceb35f4c3096be768c7547b28a35d0", "https://www.semanticscholar.org/paper/Learning-with-Marginalized-Corrupted-Features-Maaten-Chen/3c20df69865df6a627cc45c524869ccc0297048f", "https://www.semanticscholar.org/paper/Simplifying-Neural-Networks-by-Soft-Weight-Sharing-Nowlan-Hinton/de75e4e15e22d4376300e5c968e2db44be29ac9e", "https://www.semanticscholar.org/paper/Dropout-Training-as-Adaptive-Regularization-Wager-Wang/d124a098cdc6f99b9a152fcf8afa9327dac583be", "https://www.semanticscholar.org/paper/Deep-Boltzmann-Machines-Salakhutdinov-Hinton/85021c84383d18a7a4434d76dc8135fc6bdc0aa6", "https://www.semanticscholar.org/paper/Stacked-Denoising-Autoencoders%3A-Learning-Useful-in-Vincent-Larochelle/e2b7f37cd97a7907b1b8a41138721ed06a0b76cd"]},
{"id": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "title": "Learning both Weights and Connections for Efficient Neural Network", "authors": ["Song Han", "Jeff Pool", "John Tran", "William J. Dally"], "date": "2015", "abstract": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy.", "references": ["https://www.semanticscholar.org/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f", "https://www.semanticscholar.org/paper/Data-free-Parameter-Pruning-for-Deep-Neural-Srinivas-Babu/b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/Dropout%3A-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton/34f25a8704614163c4095b3ee2fc969b60de4698", "https://www.semanticscholar.org/paper/Memory-Bounded-Deep-Convolutional-Networks-Collins-Kohli/2a4117849c88d4728c33b1becaa9fb6ed7030725", "https://www.semanticscholar.org/paper/Network-In-Network-Lin-Chen/5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "https://www.semanticscholar.org/paper/How-transferable-are-features-in-deep-neural-Yosinski-Clune/081651b38ff7533550a3adfc1c00da333a8fe86c", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Compressing-Deep-Convolutional-Networks-using-Gong-Liu/e7bf9803705f2eb608db1e59e5c7636a3f171916"]},
{"id": "049fd80f52c0b1fa4d532945d95a24734b62bdf3", "title": "ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression", "authors": ["Jian-Hao Luo", "Jianxin Wu", "Weiyao Lin"], "date": "2017", "abstract": "We propose an efficient and unified framework, namely ThiNet, to simultaneously accelerate and compress CNN models in both training and inference stages. We focus on the filter level pruning, i.e., the whole filter would be discarded if it is less important. Our method does not change the original network structure, thus it can be perfectly supported by any off-the-shelf deep learning libraries. We formally establish filter pruning as an optimization problem, and reveal that we need to prune filters based on statistics information computed from its next layer, not the current layer, which differentiates ThiNet from existing methods. Experimental results demonstrate the effectiveness of this strategy, which has advanced the state-of-the-art. We also show the performance of ThiNet on ILSVRC-12 benchmark. ThiNet achieves 3.31 x FLOPs reduction and 16.63\u00d7 compression on VGG-16, with only 0.52% top-5 accuracy drop. Similar experiments with ResNet-50 reveal that even for a compact network, ThiNet can also reduce more than half of the parameters and FLOPs, at the cost of roughly 1% top-5 accuracy drop. Moreover, the original VGG-16 model can be further pruned into a very small model with only 5.05MB model size, preserving AlexNet level accuracy but showing much stronger generalization ability.", "references": ["https://www.semanticscholar.org/paper/Pruning-Filters-for-Efficient-ConvNets-Li-Kadav/c2a1cb1612ba21e067a5c3ba478a8d73b796b77a", "https://www.semanticscholar.org/paper/Network-Trimming%3A-A-Data-Driven-Neuron-Pruning-Deep-Hu-Peng/60ae4f18cb53efff0174e3fea7064049737e1e67", "https://www.semanticscholar.org/paper/Compressing-Deep-Convolutional-Networks-using-Gong-Liu/e7bf9803705f2eb608db1e59e5c7636a3f171916", "https://www.semanticscholar.org/paper/Deep-Compression%3A-Compressing-Deep-Neural-Network-Han-Mao/642d0f49b7826adcf986616f4af77e736229990f", "https://www.semanticscholar.org/paper/Learning-Structured-Sparsity-in-Deep-Neural-Wen-Wu/7601b995303f953955004db7b9b8b206c0e02ff8", "https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Network-In-Network-Lin-Chen/5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108"]},
{"id": "b9de9599d7241459db9213b5cdd7059696f5ef8d", "title": "Character-Level Language Modeling with Deeper Self-Attention", "authors": ["Rami Al-Rfou", "Dokook Choe", "Noah Constant", "Mandy Guo", "Llion Jones"], "date": "2019", "abstract": "LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.", "references": ["https://www.semanticscholar.org/paper/Language-Modeling-with-Gated-Convolutional-Networks-Dauphin-Fan/88caa4a0253a8b0076176745ebc072864eab66e1", "https://www.semanticscholar.org/paper/Regularizing-and-Optimizing-LSTM-Language-Models-Merity-Keskar/58c6f890a1ae372958b7decf56132fe258152722", "https://www.semanticscholar.org/paper/SUBWORD-LANGUAGE-MODELING-WITH-NEURAL-NETWORKS-Mikolov-Sutskever/1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "https://www.semanticscholar.org/paper/Multiplicative-LSTM-for-sequence-modelling-Krause-Lu/55cf59bfbb25d6363cab87cb747648ebe8a096e5", "https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl%C3%BCter/f9a1b3850dfd837793743565a8af95973d395a4e", "https://www.semanticscholar.org/paper/End-To-End-Memory-Networks-Sukhbaatar-Szlam/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Language-Modeling-J%C3%B3zefowicz-Vinyals/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "https://www.semanticscholar.org/paper/MuFuRU%3A-The-Multi-Function-Recurrent-Unit-Weissenborn-Rockt%C3%A4schel/4db8cd9117254d21c9c828b8ba2aea58e57ee2c4", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Natural-Conneau-Schwenk/84ca430856a92000e90cd728445ca2241c10ddc3", "https://www.semanticscholar.org/paper/Memory-Architectures-in-Recurrent-Neural-Network-Yogatama-Miao/27981998aaef92952eabef2c1490b926f9150c4f"]},
{"id": "0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "title": "Semi-supervised sequence tagging with bidirectional language models", "authors": ["Matthew E. Peters", "Waleed Ammar", "Chandra Bhagavatula", "Russell Power"], "date": "2017", "abstract": "Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.", "references": ["https://www.semanticscholar.org/paper/context2vec%3A-Learning-Generic-Context-Embedding-Melamud-Goldberger/59761abc736397539bdd01ad7f9d91c8607c0457", "https://www.semanticscholar.org/paper/Semi-Supervised-Sequential-Labeling-and-Using-Scale-Suzuki-Isozaki/7ece4e8d31f872d928369ac2cf58a616a7182112", "https://www.semanticscholar.org/paper/Semi-Supervised-Sequence-Modeling-with-Syntactic-Li-McCallum/b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "https://www.semanticscholar.org/paper/Natural-Language-Processing-(Almost)-from-Scratch-Collobert-Weston/bc1022b031dc6c7019696492e8116598097a8c12", "https://www.semanticscholar.org/paper/Skip-Thought-Vectors-Kiros-Zhu/6e795c6e9916174ae12349f5dc3f516570c17ce8", "https://www.semanticscholar.org/paper/A-Joint-Many-Task-Model%3A-Growing-a-Neural-Network-Hashimoto-Xiong/ade0c116120b54b57a91da51235108b75c28375a", "https://www.semanticscholar.org/paper/End-to-end-Sequence-Labeling-via-Bi-directional-Ma-Hovy/8dd6aae51e31a72752c4be5cddbdd76dfdc6cda4", "https://www.semanticscholar.org/paper/Learning-Distributed-Representations-of-Sentences-Hill-Cho/26e743d5bd465f49b9538deaf116c15e61b7951f", "https://www.semanticscholar.org/paper/A-Bidirectional-Recurrent-Neural-Language-Model-for-Peris-Casacuberta/2c821e2ec8ef976d3abb36fb0dc1946f04208512", "https://www.semanticscholar.org/paper/Transfer-Learning-for-Sequence-Tagging-with-Yang-Salakhutdinov/189e6bb7523733c4e524214b9e6ae92d4ed50dac"]},
{"id": "0c47cad9729c38d9db1f75491b1ee4bd883a5d4e", "title": "Semi-Supervised Sequence Modeling with Cross-View Training", "authors": ["Kevin Clark", "Minh-Thang Luong", "Christopher D. Manning", "Quoc V. Le"], "date": "2018", "abstract": "Unsupervised representation learning algorithms such as word2vec and ELMo improve the accuracy of many supervised NLP models, mainly because they can take advantage of large amounts of unlabeled text. However, the supervised models only learn from task-specific labeled data during the main training phase. We therefore propose Cross-View Training (CVT), a semi-supervised learning algorithm that improves the representations of a Bi-LSTM sentence encoder using a mix of labeled and unlabeled data. On labeled examples, standard supervised learning is used. On unlabeled examples, CVT teaches auxiliary prediction modules that see restricted views of the input (e.g., only part of a sentence) to match the predictions of the full model seeing the whole input. Since the auxiliary modules and the full model share intermediate representations, this in turn improves the full model. Moreover, we show that CVT is particularly effective when combined with multi-task learning. We evaluate CVT on five sequence tagging tasks, machine translation, and dependency parsing, achieving state-of-the-art results.", "references": ["https://www.semanticscholar.org/paper/Empower-Sequence-Labeling-with-Task-Aware-Neural-Liu-Shang/7647a06965d868a4f6451bef0818994100a142e8", "https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectional-Peters-Ammar/0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "https://www.semanticscholar.org/paper/Semi-supervised-Sequence-Learning-Dai-Le/4aa9f5150b46320f534de4747a2dd0cd7f3fe292", "https://www.semanticscholar.org/paper/Unsupervised-Pretraining-for-Sequence-to-Sequence-Ramachandran-Liu/85f94d8098322f8130512b4c6c4627548ce4a6cc", "https://www.semanticscholar.org/paper/Learning-General-Purpose-Distributed-Sentence-via-Subramanian-Trischler/afc2850945a871e72c245818f9bc141bd659b453", "https://www.semanticscholar.org/paper/Semi-supervised-Multitask-Learning-for-Sequence-Rei/ac17cfa150d802750b46220084d850cfdb64d1c1", "https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Supervised-Learning-of-Universal-Sentence-from-Data-Conneau-Kiela/ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "https://www.semanticscholar.org/paper/Multi-task-Sequence-to-Sequence-Learning-Luong-Le/d76c07211479e233f7c6a6f32d5346c983c5598f"]},
{"id": "ac11062f1f368d97f4c826c317bf50dcc13fdb59", "title": "Dissecting Contextual Word Embeddings: Architecture and Representation", "authors": ["Matthew E. Peters", "Mark Neumann", "Luke Zettlemoyer", "Wen-tau Yih"], "date": "2018", "abstract": "Contextual word representations derived from pre-trained bidirectional language models (biLMs) have recently been shown to provide significant improvements to the state of the art for a wide range of NLP tasks. However, many questions remain as to how and why these models are so effective. In this paper, we present a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. We show there is a tradeoff between speed and accuracy, but all architectures learn high quality contextual representations that outperform word embeddings for four challenging NLP tasks. Additionally, all architectures learn representations that vary with network depth, from exclusively morphological based at the word embedding layer through local syntax based in the lower contextual layers to longer range semantics such coreference at the upper layers. Together, these results suggest that unsupervised biLMs, independent of architecture, are learning much more about the structure of language than previously appreciated.", "references": ["https://www.semanticscholar.org/paper/Language-Modeling-Teaches-You-More-Syntax-than-Task-Zhang-Bowman/26f7305e4cf293b3daa672f0f75c1b0bac1e873a", "https://www.semanticscholar.org/paper/Deep-contextualized-word-representations-Peters-Neumann/3febb2bed8865945e7fddc99efd791887bb7e14f", "https://www.semanticscholar.org/paper/Deep-RNNs-Encode-Soft-Hierarchical-Syntax-Blevins-Levy/efef34c1caef102ad5cc052642d75beaaf5adcaf", "https://www.semanticscholar.org/paper/Semi-supervised-sequence-tagging-with-bidirectional-Peters-Ammar/0bb4cadc80c0afaf29c57518dc9c06f8fcfa5f38", "https://www.semanticscholar.org/paper/Learned-in-Translation%3A-Contextualized-Word-Vectors-McCann-Bradbury/bc8fa64625d9189f5801837e7b133e7fe3c581f7", "https://www.semanticscholar.org/paper/Sharp-Nearby%2C-Fuzzy-Far-Away%3A-How-Neural-Language-Khandelwal-He/fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "https://www.semanticscholar.org/paper/What-do-Neural-Machine-Translation-Models-Learn-Belinkov-Durrani/82364428995c29b3dcb60c1835548eeff4adcd20", "https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035", "https://www.semanticscholar.org/paper/Character-Aware-Neural-Language-Models-Kim-Jernite/891ce1687e2befddd19f54e4eef1d3f39c8dbaf7", "https://www.semanticscholar.org/paper/Assessing-the-Ability-of-LSTMs-to-Learn-Linzen-Dupoux/3aa52436575cf6768a0a1a476601825f6a62e58f"]},
{"id": "735d547fc75e0772d2a78c46a1cc5fad7da1474c", "title": "Can Active Memory Replace Attention?", "authors": ["Lukasz Kaiser", "Samy Bengio"], "date": "2016", "abstract": "Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.", "references": ["https://www.semanticscholar.org/paper/Encoding-Source-Language-with-Convolutional-Neural-Meng-Lu/eb5eb891061c78f4fcbc9deb3df8bca7fd005acd", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Grid-Long-Short-Term-Memory-Kalchbrenner-Danihelka/5b791cd374c7109693aaddee2c12d659ae4e3ec0", "https://www.semanticscholar.org/paper/Show%2C-Attend-and-Tell%3A-Neural-Image-Caption-with-Xu-Ba/4d8f2d14af5991d4f0d050d22216825cac3157bd", "https://www.semanticscholar.org/paper/One-Shot-Generalization-in-Deep-Generative-Models-Rezende-Mohamed/0811597b0851b7ebe21aadce7cb4daac4664b44f", "https://www.semanticscholar.org/paper/Modeling-Coverage-for-Neural-Machine-Translation-Tu-Lu/33108287fbc8d94160787d7b2c7ef249d3ad6437", "https://www.semanticscholar.org/paper/On-the-Properties-of-Neural-Machine-Translation%3A-Cho-Merrienboer/1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Long-Short-Term-Memory-Hochreiter-Schmidhuber/44d2abe2175df8153f465f6c39b68b76a0d40ab9", "https://www.semanticscholar.org/paper/Deep-Residual-Learning-for-Image-Recognition-He-Zhang/2c03df8b48bf3fa39054345bafabfeff15bfd11d"]},
{"id": "93b8da28d006415866bf48f9a6e06b5242129195", "title": "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding", "authors": ["Alex Wang", "Amanpreet Singh", "Julian Michael", "Felix Hill", "Omer Levy", "Samuel R. Bowman"], "date": "2018", "abstract": "For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.", "references": ["https://www.semanticscholar.org/paper/A-Joint-Many-Task-Model%3A-Growing-a-Neural-Network-Hashimoto-Xiong/ade0c116120b54b57a91da51235108b75c28375a", "https://www.semanticscholar.org/paper/Sluice-networks%3A-Learning-what-to-share-between-Ruder-Bingel/e242ba1a62eb2595d89afbec2657f33d9ab4abe3", "https://www.semanticscholar.org/paper/AllenNLP%3A-A-Deep-Semantic-Natural-Language-Platform-Gardner-Grus/93b4cc549a1bc4bc112189da36c318193d05d806", "https://www.semanticscholar.org/paper/Recurrent-Neural-Network-Based-Sentence-Encoder-for-Chen-Zhu/ceb7dddbd0c51f511c4ba97d328b48fd10d2a7fc", "https://www.semanticscholar.org/paper/One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov/5d833331b0e22ff359db05c62a8bca18c4f04b68", "https://www.semanticscholar.org/paper/Learning-General-Purpose-Distributed-Sentence-via-Subramanian-Trischler/afc2850945a871e72c245818f9bc141bd659b453", "https://www.semanticscholar.org/paper/Towards-Linguistically-Generalizable-NLP-Systems%3A-A-Ettinger-Rao/8472e999f723a9ccaffc6089b7be1865d8a1b863", "https://www.semanticscholar.org/paper/A-large-annotated-corpus-for-learning-natural-Bowman-Angeli/f04df4e20a18358ea2f689b4c129781628ef7fc1", "https://www.semanticscholar.org/paper/Transforming-Question-Answering-Datasets-Into-Demszky-Guu/8f1c9b656157b1d851563fb42129245701d83175", "https://www.semanticscholar.org/paper/Annotation-Artifacts-in-Natural-Language-Inference-Gururangan-Swayamdipta/2997b26ffb8c291ce478bd8a6e47979d5a55c466"]},
{"id": "e7bf9803705f2eb608db1e59e5c7636a3f171916", "title": "Compressing Deep Convolutional Networks using Vector Quantization", "authors": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir D. Bourdev"], "date": "2014", "abstract": "Deep convolutional neural networks (CNN) has become the most promising method for object recognition, repeatedly demonstrating record breaking results for image classification and object detection in recent years. However, a very deep CNN generally involves many layers with millions of parameters, making the storage of the network model to be extremely large. This prohibits the usage of deep CNNs on resource limited hardware, especially cell phones or other embedded devices. In this paper, we tackle this model storage issue by investigating information theoretical vector quantization methods for compressing the parameters of CNNs. In particular, we have found in terms of compressing the most storage demanding dense connected layers, vector quantization methods have a clear gain over existing matrix factorization methods. Simply applying k-means clustering to the weights or conducting product quantization can lead to a very good balance between model size and recognition accuracy. For the 1000-category classification task in the ImageNet challenge, we are able to achieve 16-24 times compression of the network with only 1% loss of classification accuracy using the state-of-the-art CNN.", "references": ["https://www.semanticscholar.org/paper/Deep-Fisher-Networks-for-Large-Scale-Image-Simonyan-Vedaldi/6d77482b5e3478f4616f7467054ad50505207958", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108", "https://www.semanticscholar.org/paper/Multi-scale-Orderless-Pooling-of-Deep-Convolutional-Gong-Wang/a99add9d76d849a8d47b93532703e4ca0f683b92", "https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff/a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/CNN-Features-Off-the-Shelf%3A-An-Astounding-Baseline-Razavian-Azizpour/6270baedeba28001cd1b563a199335720d6e0fe0", "https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d"]},
{"id": "8c1b00128e74f1cd92aede3959690615695d5101", "title": "QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension", "authors": ["Adams Wei Yu", "David Dohan", "Minh-Thang Luong", "Rui Zhao", "Kai Chen", "Mohammad Norouzi", "Quoc V. Le"], "date": "2018", "abstract": "Current end-to-end machine reading and question answering (Q\\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.", "references": ["https://www.semanticscholar.org/paper/Attention-is-All-you-Need-Vaswani-Shazeer/204e3073870fae3d05bcbc2f6a8e263d9b72e776", "https://www.semanticscholar.org/paper/Multi-Perspective-Context-Matching-for-Machine-Wang-Mi/e94697b98b707f557436e025bdc8498fa261d3bc", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/Learning-Recurrent-Span-Representations-for-Lee-Kwiatkowski/97e6ed1f7e5de0034f71c370c01f59c87aaf9a72", "https://www.semanticscholar.org/paper/DiSAN%3A-Directional-Self-Attention-Network-for-Shen-Zhou/adc276e6eae7051a027a4c269fb21dae43cadfed", "https://www.semanticscholar.org/paper/Gated-Self-Matching-Networks-for-Reading-and-Wang-Yang/b798cfd967e1a9ca5e7bc995d33a907bf65d1c7f", "https://www.semanticscholar.org/paper/Globally-Normalized-Reader-Raiman-Miller/de0c30321b22c56d637e7c29cb59180f157272a8", "https://www.semanticscholar.org/paper/Machine-Comprehension-Using-Match-LSTM-and-Answer-Wang-Jiang/ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "https://www.semanticscholar.org/paper/MEMEN%3A-Multi-layer-Embedding-with-Memory-Networks-Pan-Li/12e20e4ea572dbe476fd894c5c9a9930cf250dd2", "https://www.semanticscholar.org/paper/Learning-to-Skim-Text-Yu-Lee/c25a67ad7e8629a9d12b9e2fc356cd73af99a060"]},
{"id": "48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016", "title": "Determinantal Point Processes for Machine Learning", "authors": ["Alex Kulesza", "Ben Taskar"], "date": "2012", "abstract": "Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep and beautiful theory, DPPs are relatively new in machine learning. Determinantal Point Processes for Machine Learning provides a comprehensible introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and shows how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. It presents the general mathematical background to DPPs along with a range of modeling extensions, efficient algorithms, and theoretical results that aim to enable practical modeling and learning.", "references": ["https://www.semanticscholar.org/paper/Near-Optimal-MAP-Inference-for-Determinantal-Point-Gillenwater-Kulesza/15acca25f75076b80b0bd24c5710c70733308c11", "https://www.semanticscholar.org/paper/k-DPPs%3A-Fixed-Size-Determinantal-Point-Processes-Kulesza-Taskar/ec46bcbced500820521e9f65b0f9ffef5a83ae11", "https://www.semanticscholar.org/paper/Learning-Determinantal-Point-Processes-Kulesza-Taskar/c87f5836627a4ea7d928ff1aecf1b7cdebaf1302", "https://www.semanticscholar.org/paper/Structured-Determinantal-Point-Processes-Kulesza-Taskar/6c4ab2b7bf202e621dcb722d2e7cf421415cc3ed", "https://www.semanticscholar.org/paper/Hyperdeterminantal-point-processes-Evans-Gottlieb/733edbe153056edb62f3c3cdec975db85a072906", "https://www.semanticscholar.org/paper/Learning-associative-Markov-networks-Taskar-Chatalbashev/702c2fde33ccb4328be06405c11e208a4b3ee347", "https://www.semanticscholar.org/paper/Structured-Learning-with-Approximate-Inference-Kulesza-Pereira/325ea1f2022ee3886a5810df76dcfbe4010ad439", "https://www.semanticscholar.org/paper/Finding-MAPs-for-Belief-Networks-is-NP-Hard-Shimony/96a5867d0b9b997108633ff3da314edf69b0122c", "https://www.semanticscholar.org/paper/Linear-Programming-Relaxations-and-Belief-An-Study-Yanover-Meltzer/e0ab5b00d4fb0ef319093f94d5024008b6000381", "https://www.semanticscholar.org/paper/Determinantal-Processes-and-Independence-Hough-Krishnapur/b91afd46236a9c9eda9056bf4e70fe9235867571"]},
{"id": "fc6b1ff29f2da985cccfa644652bb320d7720d59", "title": "Online Algorithms and Stochastic Approximations", "authors": ["L\u00e9on Bottou"], "date": "1998", "abstract": "A process for the liquid phase oxidation of hydrocarbons with a molecular oxygen-containing gas in the presence of a dissolved cobalt salt catalyst characterized in that the oxidation is carried out in the substantial absence of chromium in the reaction medium i.e. a concentration of chromium in the liquid phase of not greater than 400 ppm.", "references": []},
{"id": "d559dd84fc473fca7e91b9075675750823935afa", "title": "Sparse Convolutional Neural Networks", "authors": ["Bao-Yuan Liu", "Meitian Wang", "Hassan Foroosh", "Marshall F. Tappen", "Marianna Pensky"], "date": "2015", "abstract": "Deep neural networks have achieved remarkable performance in both image classification and object detection problems, at the cost of a large number of parameters and computational complexity. In this work, we show how to reduce the redundancy in these parameters using a sparse decomposition. Maximum sparsity is obtained by exploiting both inter-channel and intra-channel redundancy, with a fine-tuning step that minimize the recognition loss caused by maximizing sparsity. This procedure zeros out more than 90% of parameters, with a drop of accuracy that is less than 1% on the ILSVRC2012 dataset. We also propose an efficient sparse matrix multiplication algorithm on CPU for Sparse Convolutional Neural Networks (SCNN) models. Our CPU implementation demonstrates much higher efficiency than the off-the-shelf sparse matrix libraries, with a significant speedup realized over the original dense network. In addition, we apply the SCNN model to the object detection problem, in conjunction with a cascade model and sparse fully connected layers, to achieve significant speedups.", "references": ["https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff/a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "https://www.semanticscholar.org/paper/Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue/a9ce496186120df8f9ed3367e76a4947419e992e", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/Spatial-Pyramid-Pooling-in-Deep-Convolutional-for-He-Zhang/cbb19236820a96038d000dc629225d36e0b6294a", "https://www.semanticscholar.org/paper/Caffe%3A-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer/6bdb186ec4726e00a8051119636d4df3b94043b5"]},
{"id": "8ad35df17ae4064dd174690efb04d347428f1117", "title": "Convolutional neural networks at constrained time cost", "authors": ["Kaiming He", "Jian Sun"], "date": "2015", "abstract": "Though recent advanced convolutional neural networks (CNNs) have been improving the image recognition accuracy, the models are getting more complex and time-consuming. For real-world applications in industrial and commercial scenarios, engineers and developers are often faced with the requirement of constrained time budget. In this paper, we investigate the accuracy of CNNs under constrained time cost. Under this constraint, the designs of the network architectures should exhibit as trade-offs among the factors like depth, numbers of filters, filter sizes, etc. With a series of controlled comparisons, we progressively modify a baseline model while preserving its time complexity. This is also helpful for understanding the importance of the factors in network designs. We present an architecture that achieves very competitive accuracy in the ImageNet dataset (11.8% top-5 error, 10-view test), yet is 20% faster than \u201cAlexNet\u201d [14] (16.0% top-5 error, 10-view test).", "references": ["https://www.semanticscholar.org/paper/Speeding-up-Convolutional-Neural-Networks-with-Low-Jaderberg-Vedaldi/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "https://www.semanticscholar.org/paper/Return-of-the-Devil-in-the-Details%3A-Delving-Deep-Chatfield-Simonyan/14d9be7962a4ec5a6e55755f4c7588ea00793652", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108", "https://www.semanticscholar.org/paper/Spatial-Pyramid-Pooling-in-Deep-Convolutional-for-He-Zhang/cbb19236820a96038d000dc629225d36e0b6294a", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327", "https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "https://www.semanticscholar.org/paper/CNN-Features-Off-the-Shelf%3A-An-Astounding-Baseline-Razavian-Azizpour/6270baedeba28001cd1b563a199335720d6e0fe0", "https://www.semanticscholar.org/paper/Some-Improvements-on-Deep-Convolutional-Neural-Howard/d67175d17c450ab0ac9c256103828f9e9a0acb85"]},
{"id": "7d39283a0fce1c96f57eb20046d09bd95ccc56d7", "title": "Structured Pruning of Deep Convolutional Neural Networks", "authors": ["Sajid Anwar", "Kyuyeon Hwang", "Wonyong Sung"], "date": "2017", "abstract": "Real-time application of deep learning algorithms is often hindered by high computational complexity and frequent memory accesses. Network pruning is a promising technique to solve this problem. However, pruning usually results in irregular network connections that not only demand extra representation efforts but also do not fit well on parallel computation. We introduce structured sparsity at various scales for convolutional neural networks: feature map-wise, kernel-wise, and intra-kernel strided sparsity. This structured sparsity is very advantageous for direct computational resource savings on embedded computers, in parallel computing environments, and in hardware-based systems. To decide the importance of network connections and paths, the proposed method uses a particle filtering approach. The importance weight of each particle is assigned by assessing the misclassification rate with a corresponding connectivity pattern. The pruned network is retrained to compensate for the losses due to pruning. While implementing convolutions as matrix products, we particularly show that intra-kernel strided sparsity with a simple constraint can significantly reduce the size of the kernel and feature map tensors. The proposed work shows that when pruning granularities are applied in combination, we can prune the CIFAR-10 network by more than 70% with less than a 1% loss in accuracy.", "references": ["https://www.semanticscholar.org/paper/A-Deep-Neural-Network-Compression-Pipeline%3A-Huffman-Han-Mao/397de65a9a815ec39b3704a79341d687205bc80a", "https://www.semanticscholar.org/paper/Learning-both-Weights-and-Connections-for-Efficient-Han-Pool/1ff9a37d766e3a4f39757f5e1b235a42dacf18ff", "https://www.semanticscholar.org/paper/High-Performance-Convolutional-Neural-Networks-for-Chellapilla-Puri/2cc157afda51873c30b195fff56e917b9c06b853", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff"]},
{"id": "3c20df69865df6a627cc45c524869ccc0297048f", "title": "Learning with Marginalized Corrupted Features", "authors": ["Laurens van der Maaten", "Minmin Chen", "Stephen Tyree", "Kilian Q. Weinberger"], "date": "2013", "abstract": "The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variations in the data distribution. In the case of finite training data, an effective solution is to extend the training set with artificially created examples--which, however, is also computationally costly. We propose to corrupt training examples with noise from known distributions within the exponential family and present a novel learning algorithm, called marginalized corrupted features (MCF), that trains robust predictors by minimizing the expected value of the loss function under the corrupting distribution-- essentially learning with infinitely many (corrupted) training examples. We show empirically on a variety of data sets that MCF classifiers can be trained efficiently, may generalize substantially better to test data, and are more robust to feature deletion at test time.", "references": ["https://www.semanticscholar.org/paper/Learning-to-classify-with-missing-and-corrupted-Dekel-Shamir/c5ee421735abee2669a687dd8cad95376a4b7fee", "https://www.semanticscholar.org/paper/Training-with-Noise-is-Equivalent-to-Tikhonov-Bishop/c3ecd8e19e016d15670c8953b4b9afaa5186b0f3", "https://www.semanticscholar.org/paper/Extracting-and-composing-robust-features-with-Vincent-Larochelle/843959ffdccf31c6694d135fad07425924f785b1", "https://www.semanticscholar.org/paper/Nightmare-at-test-time%3A-robust-learning-by-feature-Globerson-Roweis/6dd9bb6b38e5b84616e207f00a181dbadce06937", "https://www.semanticscholar.org/paper/An-Analysis-of-Single-Layer-Networks-in-Feature-Coates-Ng/be9a17321537d9289875fe475b71f4821457b435", "https://www.semanticscholar.org/paper/Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava/1366de5bb112746a555e9c0cd00de3ad8628aea8", "https://www.semanticscholar.org/paper/Max-margin-Classification-of-Data-with-Absent-Chechik-Heitz/3554953715a4d7af3d4c9201d4080899b84fbad7", "https://www.semanticscholar.org/paper/Marginalized-Denoising-Autoencoders-for-Domain-Chen-Xu/8db26a22942404bd435909a16bb3a50cd67b4318", "https://www.semanticscholar.org/paper/Estimating-a-Kernel-Fisher-Discriminant-in-the-of-Lawrence-Sch%C3%B6lkopf/fcba51774867c77f491581d3625d375a0a8f473b", "https://www.semanticscholar.org/paper/Feature-selection%2C-L1-vs.-L2-regularization%2C-and-Ng/5ac5fbae8a7faf2e9bd49ad01106cec4a2d8f20a"]},
{"id": "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "authors": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman"], "date": "2014", "abstract": "The focus of this paper is speeding up the application of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition [15], showing a possible 2.5\u00d7 speedup with no loss in accuracy, and 4.5\u00d7 speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks.", "references": ["https://www.semanticscholar.org/paper/Exploiting-Linear-Structure-Within-Convolutional-Denton-Zaremba/e5ae8ab688051931b4814f6d32b18391f8d1fa8d", "https://www.semanticscholar.org/paper/Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff/a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "https://www.semanticscholar.org/paper/Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior/fbeaa499e10e98515f7e1c4ad89165e8c0677427", "https://www.semanticscholar.org/paper/Simplifying-ConvNets-for-Fast-Learning-Mamalet-Garcia/4dbc68cf2e14155edb6da0def30661aca8c96c22", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Learning-and-Transferring-Mid-level-Image-Using-Oquab-Bottou/c08f5fa876181fc040d76c75fe2433eee3c9b001", "https://www.semanticscholar.org/paper/Learning-Convolutional-Feature-Hierarchies-for-Kavukcuoglu-Sermanet/8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "https://www.semanticscholar.org/paper/Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse/1e80f755bcbf10479afd2338cec05211fdbd325c", "https://www.semanticscholar.org/paper/Multi-digit-Number-Recognition-from-Street-View-Goodfellow-Bulatov/b3d8dffb73bc93de239998548386c84177caa2ad"]},
{"id": "2c821e2ec8ef976d3abb36fb0dc1946f04208512", "title": "A Bidirectional Recurrent Neural Language Model for Machine Translation", "authors": ["\u00c1lvaro Peris", "Francisco Casacuberta"], "date": "2015", "abstract": "A language model based in continuous representations of words is pre- sented, which has been applied to a statistical machine translation task. This model is implemented by means of a bidirectional recurrent neural network, which is able to take into account both the past and the future context of a word in order to perform predictions. Due to its high temporal cost at training time, for obtaining relevant training data an instance selection algorithm is used, which aims to capture useful information for translating a test set. Obtained results show that the neural model trained with the selected data outperforms the results obtained by an n-gram language model.", "references": ["https://www.semanticscholar.org/paper/Translation-Modeling-with-Bidirectional-Recurrent-Sundermeyer-Alkhouli/d29cf0f457ec2089fd4d776ef9a246de810be689", "https://www.semanticscholar.org/paper/OxLM%3A-A-Neural-Language-Modelling-Framework-for-Paul-Phil/f83d712e1afb83fd45628261ede23fb112ec1666", "https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl%C3%BCter/f9a1b3850dfd837793743565a8af95973d395a4e", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Neural-Network-Based-Bilingual-Language-Model-for-Wang-Hai/3090262c765a95fe9cd975fa12b4ec15391ece6d", "https://www.semanticscholar.org/paper/Statistical-Language-Models-Based-on-Neural-U%C4%8Den%C3%AD-Brn%C4%9B/96364af2d208ea75ca3aeb71892d2f7ce7326b55", "https://www.semanticscholar.org/paper/Efficient-data-selection-for-machine-translation-Mandal-Vergyri/ade34bf617f7733bfa0676f2bd57fa5658d4e54c", "https://www.semanticscholar.org/paper/Fast-and-Robust-Neural-Network-Joint-Models-for-Devlin-Zbib/0894b06cff1cd0903574acaa7fcf071b144ae775", "https://www.semanticscholar.org/paper/Addressing-the-Rare-Word-Problem-in-Neural-Machine-Luong-Sutskever/1956c239b3552e030db1b78951f64781101125ed", "https://www.semanticscholar.org/paper/CSLM-a-modular-open-source-continuous-space-toolkit-Schwenk/d36b19b4c5977dd2a2796a5ad3508a3d8a087809"]},
{"id": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb", "title": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS", "authors": ["Tom\u00e1\u0161 Mikolov", "Ilya Sutskever", "Anoop Deoras", "Hai-Son Le", "Stefan Kombrink", "Jan \u010cernock\u00fd"], "date": "2011", "abstract": "We explore the performance of several types of language mode ls on the word-level and the character-level language modelin g tasks. This includes two recently proposed recurrent neural netwo rk architectures, a feedforward neural network model, a maximum ent ropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from th e data, and show that it combines advantages of both character and wo rdlevel models. Finally, we show that neural network based lan gu ge models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task. By using sub-word un its, the size can be reduced even more.", "references": ["https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5", "https://www.semanticscholar.org/paper/Mandarin-Word-Character-Hybrid-Input-Neural-Network-Kang-Ng/5999fd9b9712fee3184989d043bff899935b4208", "https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "https://www.semanticscholar.org/paper/Strategies-for-training-large-scale-neural-network-Mikolov-Deoras/cb45e9217fe323fbc199d820e7735488fca2a9b3", "https://www.semanticscholar.org/paper/Extensions-of-recurrent-neural-network-language-Mikolov-Kombrink/07ca885cb5cc4328895bfaec9ab752d5801b14cd", "https://www.semanticscholar.org/paper/Empirical-Evaluation-and-Combination-of-Advanced-Mikolov-Deoras/77dfe038a9bdab27c4505444931eaa976e9ec667", "https://www.semanticscholar.org/paper/Generating-Text-with-Recurrent-Neural-Networks-Sutskever-Martens/e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de", "https://www.semanticscholar.org/paper/A-Fast-Re-scoring-Strategy-to-Capture-Long-Distance-Deoras-Mikolov/0591a9a6ec348559dc3d8f88956e301200aaf4f8", "https://www.semanticscholar.org/paper/Learning-Sub-Word-Units-for-Open-Vocabulary-Speech-Parada-Dredze/1596e3cb20ba3b9aefe440e30660c2a9f035f683", "https://www.semanticscholar.org/paper/Hybrid-Language-Models-Using-Mixed-Types-of-Units-Shaik-Mousa/472bd5b90c289b715340708536ade437d20b237e"]},
{"id": "081651b38ff7533550a3adfc1c00da333a8fe86c", "title": "How transferable are features in deep neural networks?", "authors": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "date": "2014", "abstract": "Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.", "references": ["https://www.semanticscholar.org/paper/DeCAF%3A-A-Deep-Convolutional-Activation-Feature-for-Donahue-Jia/b8de958fead0d8a9619b55c7299df3257c624a96", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Deep-Learning-of-Representations-for-Unsupervised-Bengio/3e58d9800aa31e5db89d99dcd33e5786b7837bfd", "https://www.semanticscholar.org/paper/Visualizing-and-Understanding-Convolutional-Zeiler-Fergus/1a2a770d23b4a171fa81de62a78a3deb0588f238", "https://www.semanticscholar.org/paper/ICA-with-Reconstruction-Cost-for-Efficient-Feature-Le-Karpenko/51e93552fe55be91a5711ff2aabc04b742503e68", "https://www.semanticscholar.org/paper/OverFeat%3A-Integrated-Recognition%2C-Localization-and-Sermanet-Eigen/1109b663453e78a59e4f66446d71720ac58cec25", "https://www.semanticscholar.org/paper/Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse/1e80f755bcbf10479afd2338cec05211fdbd325c", "https://www.semanticscholar.org/paper/Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava/1366de5bb112746a555e9c0cd00de3ad8628aea8", "https://www.semanticscholar.org/paper/What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu/1f88427d7aa8225e47f946ac41a0667d7b69ac52"]},
{"id": "bc1022b031dc6c7019696492e8116598097a8c12", "title": "Natural Language Processing (Almost) from Scratch", "authors": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "date": "2011", "abstract": "We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.", "references": ["https://www.semanticscholar.org/paper/Distributional-Representations-for-Handling-in-Huang-Yates/c8c007a5d86caa2fc77f186bae974b9af39428f1", "https://www.semanticscholar.org/paper/Early-results-for-Named-Entity-Recognition-with-and-McCallum-Li/8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "https://www.semanticscholar.org/paper/Semi-Supervised-Learning-for-Natural-Language-Liang/31b4c03d721dc10b87c178277c1d369f91db8f0e", "https://www.semanticscholar.org/paper/Semi-Supervised-Sequential-Labeling-and-Using-Scale-Suzuki-Isozaki/7ece4e8d31f872d928369ac2cf58a616a7182112", "https://www.semanticscholar.org/paper/Guided-Learning-for-Bidirectional-Sequence-Shen-Satta/6503a3d9fb204c2a08ecfcfe6ba5b815fc65a030", "https://www.semanticscholar.org/paper/Feature-Rich-Part-of-Speech-Tagging-with-a-Cyclic-Toutanova-Klein/eb42a490cf4f186d3383c92963817d100afd81e2", "https://www.semanticscholar.org/paper/Joint-Parsing-and-Semantic-Role-Labeling-Sutton-McCallum/48b4524a3b1207157b1b2f87885c434c96fc7a19", "https://www.semanticscholar.org/paper/Word-Representations%3A-A-Simple-and-General-Method-Turian-Ratinov/dac72f2c509aee67524d3321f77e97e8eff51de6", "https://www.semanticscholar.org/paper/Shallow-Semantic-Parsing-using-Support-Vector-Pradhan-Ward/a16e484824b2580e092c985aa659e8680aeda5ee", "https://www.semanticscholar.org/paper/Simple-Semi-supervised-Dependency-Parsing-Koo-Carreras/790ecefeaf2b471b439743a772ccce026131bef5"]},
{"id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "title": "Network In Network", "authors": ["Min Lin", "Qiang Chen", "Shuicheng Yan"], "date": "2014", "abstract": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.", "references": ["https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Learnable-Pooling-Regions-for-Image-Classification-Malinowski-Fritz/f9def788d4ae040edb8bde18b8aeea635444a4d1", "https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086", "https://www.semanticscholar.org/paper/Discriminative-Transfer-Learning-with-Tree-based-Srivastava-Salakhutdinov/822f3b9a392a9abccdaa7ef5ae4183d2d4d3d6db", "https://www.semanticscholar.org/paper/Stochastic-Pooling-for-Regularization-of-Deep-Zeiler-Fergus/0abb49fe138e8fb7332c26b148a48d0db39724fc", "https://www.semanticscholar.org/paper/Improving-Neural-Networks-with-Dropout-Srivastava/5d5d4f49d6443c8529a6f5ebef5c499d47a869da", "https://www.semanticscholar.org/paper/Knowledge-Matters%3A-Importance-of-Prior-Information-G%C3%BCl%C3%A7ehre-Bengio/523b12db4004b89284387f978c2af8ae0e79d54b", "https://www.semanticscholar.org/paper/Multi-digit-Number-Recognition-from-Street-View-Goodfellow-Bulatov/b3d8dffb73bc93de239998548386c84177caa2ad", "https://www.semanticscholar.org/paper/Regularization-of-Neural-Networks-using-DropConnect-Wan-Zeiler/38f35dd624cd1cf827416e31ac5e0e0454028eca"]},
{"id": "fd5794fc63d5f19bf83cf7baa36e0aa62cbf6299", "title": "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context", "authors": ["Urvashi Khandelwal", "He He", "Peng Qi", "Dan Jurafsky"], "date": "2018", "abstract": "We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.", "references": ["https://www.semanticscholar.org/paper/Contextual-LSTM-(CLSTM)-models-for-Large-scale-NLP-Ghosh-Vinyals/6067628004373e61b962bd4b470308882e57448b", "https://www.semanticscholar.org/paper/Larger-Context-Language-Modelling-with-Recurrent-Wang-Cho/722e01d5ba05083f7a091f3188cfdfcf183a325d", "https://www.semanticscholar.org/paper/Regularizing-and-Optimizing-LSTM-Language-Models-Merity-Keskar/58c6f890a1ae372958b7decf56132fe258152722", "https://www.semanticscholar.org/paper/Visualizing-and-Understanding-Neural-Models-in-NLP-Li-Chen/fafa541419b3756968fe5b3156c6f0257cb29c23", "https://www.semanticscholar.org/paper/Fine-grained-Analysis-of-Sentence-Embeddings-Using-Adi-Kermany/e44da7d8c71edcc6e575fa7faadd5e75785a7901", "https://www.semanticscholar.org/paper/Pointer-Sentinel-Mixture-Models-Merity-Xiong/efbd381493bb9636f489b965a2034d529cd56bcd", "https://www.semanticscholar.org/paper/Assessing-the-Ability-of-LSTMs-to-Learn-Linzen-Dupoux/3aa52436575cf6768a0a1a476601825f6a62e58f", "https://www.semanticscholar.org/paper/Unbounded-cache-model-for-online-language-modeling-Grave-Ciss%C3%A9/2ed4ebe1878fd8e421f24e1aac76fbdc89e9d381", "https://www.semanticscholar.org/paper/Language-Modeling-with-Gated-Convolutional-Networks-Dauphin-Fan/88caa4a0253a8b0076176745ebc072864eab66e1", "https://www.semanticscholar.org/paper/N-gram-Language-Modeling-using-Recurrent-Neural-Chelba-Norouzi/a23744c15f86025a789126a7fc7a6ba54d797782"]},
{"id": "3aa52436575cf6768a0a1a476601825f6a62e58f", "title": "Assessing the Ability of LSTMs to Learn Syntax-Sensitive Dependencies", "authors": ["Tal Linzen", "Emmanuel Dupoux", "Yoav Goldberg"], "date": "2016", "abstract": "The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture\u2019s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.", "references": ["https://www.semanticscholar.org/paper/Visualizing-and-Understanding-Neural-Models-in-NLP-Li-Chen/fafa541419b3756968fe5b3156c6f0257cb29c23", "https://www.semanticscholar.org/paper/Language-acquisition-in-the-absence-of-explicit-how-Rohde-Plaut/47133d54d4a5f1fb3c46bdbf3a7a5e270d930e2f", "https://www.semanticscholar.org/paper/Visualizing-and-Understanding-Recurrent-Networks-Karpathy-Johnson/40be3888daa5c2e5af4d36ae22f690bcc8caf600", "https://www.semanticscholar.org/paper/The-Acquisition-of-Anaphora-by-Simple-Recurrent-Frank-Mathis/5d1d86c1990a21cdb71aea8f1939eddc36dbbe9e", "https://www.semanticscholar.org/paper/Statistical-Representation-of-Grammaticality-the-of-Clark-Giorgolo/e442a3ca917b8b491375c9662843f7fd8c729598", "https://www.semanticscholar.org/paper/One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov/5d833331b0e22ff359db05c62a8bca18c4f04b68", "https://www.semanticscholar.org/paper/Representation-of-Linguistic-Form-and-Function-in-K%C3%A1d%C3%A1r-Chrupa%C5%82a/9462eee3e5eff15df5e97c38e24072c65e581cee", "https://www.semanticscholar.org/paper/Grammar-as-a-Foreign-Language-Vinyals-Kaiser/47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "https://www.semanticscholar.org/paper/LSTM-recurrent-networks-learn-simple-context-free-Gers-Schmidhuber/f828b401c86e0f8fddd8e77774e332dfd226cb05", "https://www.semanticscholar.org/paper/LSTM-Neural-Networks-for-Language-Modeling-Sundermeyer-Schl%C3%BCter/f9a1b3850dfd837793743565a8af95973d395a4e"]},
{"id": "bc8fa64625d9189f5801837e7b133e7fe3c581f7", "title": "Learned in Translation: Contextualized Word Vectors", "authors": ["Bryan McCann", "James Bradbury", "Caiming Xiong", "Richard Socher"], "date": "2017", "abstract": "Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.", "references": ["https://www.semanticscholar.org/paper/Towards-Universal-Paraphrastic-Sentence-Embeddings-Wieting-Bansal/395044a2e3f5624b2471fb28826e7dbb1009356e", "https://www.semanticscholar.org/paper/Supervised-Learning-of-Universal-Sentence-from-Data-Conneau-Kiela/ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "https://www.semanticscholar.org/paper/Ask-Me-Anything%3A-Dynamic-Memory-Networks-for-Kumar-Irsoy/452059171226626718eb677358836328f884298e", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Recursive-Deep-Models-for-Semantic-Compositionality-Socher-Perelygin/687bac2d3320083eb4530bf18bb8f8f721477600", "https://www.semanticscholar.org/paper/Glove%3A-Global-Vectors-for-Word-Representation-Pennington-Socher/f37e1b62a767a307c046404ca96bc140b3e68cb5", "https://www.semanticscholar.org/paper/Learning-Word-Vectors-for-Sentiment-Analysis-Maas-Daly/649d03490ef72c5274e3bccd03d7a299d2f8da91", "https://www.semanticscholar.org/paper/Encoding-Syntactic-Knowledge-in-Neural-Networks-for-Huang-Qian/9f46d7793995677d15a3fa10c3cb2605a44610e8", "https://www.semanticscholar.org/paper/Reading-and-Thinking%3A-Re-read-LSTM-Unit-for-Textual-Sha-Chang/4c7e85ff37dd8b99d8f443eabd3b163ff8b71538", "https://www.semanticscholar.org/paper/The-representational-geometry-of-word-meanings-by-Hill-Cho/032e9974cedb31f5c6e354626760e54e5ebf1e3c"]},
{"id": "e242ba1a62eb2595d89afbec2657f33d9ab4abe3", "title": "Sluice networks: Learning what to share between loosely related tasks", "authors": ["Sebastian Ruder", "Joachim Bingel", "Isabelle Augenstein", "Anders S\u00f8gaard"], "date": "2017", "abstract": "Multi-task learning is motivated by the observation that humans bring to bear what they know about related problems when solving new ones. Similarly, deep neural networks can profit from related tasks by sharing parameters with other networks. However, humans do not consciously decide to transfer knowledge between tasks. In Natural Language Processing (NLP), it is hard to predict if sharing will lead to improvements, particularly if tasks are only loosely related. To overcome this, we introduce Sluice Networks, a general framework for multi-task learning where trainable parameters control the amount of sharing. Our framework generalizes previous proposals in enabling sharing of all combinations of subspaces, layers, and skip connections. We perform experiments on three task pairs, and across seven different domains, using data from OntoNotes 5.0, and achieve up to 15% average error reductions over common approaches to multi-task learning. We show that a) label entropy is predictive of gains in sluice networks, confirming findings for hard parameter sharing and b) while sluice networks easily fit noise, they are robust across domains in practice.", "references": ["https://www.semanticscholar.org/paper/A-Joint-Many-Task-Model%3A-Growing-a-Neural-Network-Hashimoto-Xiong/ade0c116120b54b57a91da51235108b75c28375a", "https://www.semanticscholar.org/paper/Identifying-beneficial-task-relations-for-learning-S%C3%B8gaard-Bingel/1b02204b210f822dabf8d68b7e3ea7ac14ee1268", "https://www.semanticscholar.org/paper/Deep-multi-task-learning-with-low-level-tasks-at-S%C3%B8gaard-Goldberg/03ad06583c9721855ccd82c3d969a01360218d86", "https://www.semanticscholar.org/paper/Multi-task-Multi-domain-Representation-Learning-for-Peng-Dredze/58a644686a9c44708aff98f019602fa9553e88ff", "https://www.semanticscholar.org/paper/When-is-multitask-learning-effective-Semantic-under-Plank-Alonso/9405d0388f90ba1432ef13c21309d8363860e22e", "https://www.semanticscholar.org/paper/Cross-Stitch-Networks-for-Multi-task-Learning-Misra-Shrivastava/f14325ec3041a73118bc4d819204cbbca07d5a71", "https://www.semanticscholar.org/paper/A-unified-architecture-for-natural-language-deep-Collobert-Weston/57458bc1cffe5caa45a885af986d70f723f406b4", "https://www.semanticscholar.org/paper/A-Dirty-Model-for-Multi-task-Learning-Jalali-Ravikumar/895217d527de919dfdfbfeae5362bf5adba984ce", "https://www.semanticscholar.org/paper/Domain-Separation-Networks-Bousmalis-Trigeorgis/01cb4071a0a43aeef63e5d568ad5afe1fb8b2411", "https://www.semanticscholar.org/paper/Exploiting-Task-Relatedness-for-Mulitple-Task-Ben-David-Borbely/3a4551508f84a3f5447d3490b2db95b4d87a7969"]},
{"id": "93b4cc549a1bc4bc112189da36c318193d05d806", "title": "AllenNLP: A Deep Semantic Natural Language Processing Platform", "authors": ["Matt Gardner", "Joel Grus", "Mark Neumann", "Oyvind Tafjord", "Pradeep Dasigi", "Nelson F. Liu", "Matthew E. Peters", "Michael Schmitz", "Luke Zettlemoyer"], "date": "2018", "abstract": "This paper describes AllenNLP, a platform for research on deep learning methods in natural language understanding. AllenNLP is designed to support researchers who want to build novel language understanding models quickly and easily. It is built on top of PyTorch, allowing for dynamic computation graphs, and provides (1) a flexible data API that handles intelligent batching and padding, (2) high-level abstractions for common operations in working with text, and (3) a modular and extensible experiment framework that makes doing good science easy. It also includes reference implementations of high quality approaches for both core semantic problems (e.g. semantic role labeling (Palmer et al., 2005)) and language understanding applications (e.g. machine comprehension (Rajpurkar et al., 2016)). AllenNLP is an ongoing open-source effort maintained by engineers and researchers at the Allen Institute for Artificial Intelligence.", "references": ["https://www.semanticscholar.org/paper/End-to-end-learning-of-semantic-role-labeling-using-Zhou-Xu/c34e41312b47f60986458759d5cc546c2b53f748", "https://www.semanticscholar.org/paper/A-large-annotated-corpus-for-learning-natural-Bowman-Angeli/f04df4e20a18358ea2f689b4c129781628ef7fc1", "https://www.semanticscholar.org/paper/Steven-Bird%2C-Ewan-Klein-and-Edward-Loper%3A-Natural-Wagner/cfdd423c8672a7b178ea85d56079328df4eea647", "https://www.semanticscholar.org/paper/Deep-Semantic-Role-Labeling%3A-What-Works-and-What's-He-Lee/a4dd3beea286a20c4e4f66436875932d597190bc", "https://www.semanticscholar.org/paper/Neural-Semantic-Parsing-with-Type-Constraints-for-Krishnamurthy-Dasigi/8c6f58ed0ebf379858c0bbe02c53ee51b3eb398a", "https://www.semanticscholar.org/paper/ParlAI%3A-A-Dialog-Research-Software-Platform-Miller-Feng/5b5cc77898a71a1386734584ceef4070263b8d03", "https://www.semanticscholar.org/paper/A-Decomposable-Attention-Model-for-Natural-Language-Parikh-T%C3%A4ckstr%C3%B6m/2cd8e8f510c89c7c18268e8ad51c061e459ad321", "https://www.semanticscholar.org/paper/Deep-contextualized-word-representations-Peters-Neumann/3febb2bed8865945e7fddc99efd791887bb7e14f", "https://www.semanticscholar.org/paper/Bidirectional-Attention-Flow-for-Machine-Seo-Kembhavi/3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "https://www.semanticscholar.org/paper/End-to-end-Neural-Coreference-Resolution-Lee-He/8ae1af4a424f5e464d46903bc3d18fe1cf1434ff"]},
{"id": "2cc157afda51873c30b195fff56e917b9c06b853", "title": "High Performance Convolutional Neural Networks for Document Processing", "authors": ["Kumar Chellapilla", "Sidd Puri", "Patrice Y. Simard"], "date": "2006", "abstract": "Convolutional neural networks (CNNs) are well known for producing state-of-the-art recognizers for document processing [1]. However, they can be difficult to implement and are usually slower than traditional multi-layer perceptrons (MLPs). We present three novel approaches to speeding up CNNs: a) unrolling convolution, b) using BLAS (basic linear algebra subroutines), and c) using GPUs (graphic processing units). Unrolled convolution converts the processing in each convolutional layer (both forward-propagation and back-propagation) into a matrix-matrix product. The matrix-matrix product representation of CNNs makes their implementation as easy as MLPs. BLAS is used to efficiently compute matrix products on the CPU. We also present a pixel shader based GPU implementation of CNNs. Results on character recognition problems indicate that unrolled convolution with BLAS produces a dramatic 2.4X\u22123.0X speedup. The GPU implementation is even faster and produces a 3.1X\u22124.1X speedup.", "references": ["https://www.semanticscholar.org/paper/Best-practices-for-convolutional-neural-networks-to-Simard-Steinkraus/5562a56da3a96dae82add7de705e2bd841eb00fc", "https://www.semanticscholar.org/paper/Gradient-based-learning-applied-to-document-LeCun-Bottou/162d958ff885f1462aeda91cd72582323fd6a1f4", "https://www.semanticscholar.org/paper/Multi-Digit-Recognition-Using-a-Space-Displacement-Matan-Burges/464e8d981df7f326c3af6e9d7bd627f83e438816", "https://www.semanticscholar.org/paper/Linear-Operators-for-GPU-Implementation-of-Kruger-Westermann/c681ebe49f96954bee87e131214a2921ac7c0f8e"]},
{"id": "325ea1f2022ee3886a5810df76dcfbe4010ad439", "title": "Structured Learning with Approximate Inference", "authors": ["Alex Kulesza", "Fernando C Pereira"], "date": "2007", "abstract": "In many structured prediction problems, the highest-scoring labeling is hard to compute exactly, leading to the use of approximate inference methods. However, when inference is used in a learning algorithm, a good approximation of the score may not be sufficient. We show in particular that learning can fail even with an approximate inference method with rigorous approximation guarantees. There are two reasons for this. First, approximate methods can effectively reduce the expressivity of an underlying model by making it impossible to choose parameters that reliably give good predictions. Second, approximations can respond to parameter changes in such a way that standard learning algorithms are misled. In contrast, we give two positive results in the form of learning bounds for the use of LP-relaxed inference in structured perceptron and empirical risk minimization settings. We argue that without understanding combinations of inference and learning, such as these, that are appropriately compatible, learning performance under approximate inference cannot be guaranteed.", "references": ["https://www.semanticscholar.org/paper/Learning-as-search-optimization%3A-approximate-large-Daum%C3%A9-Marcu/a5c48c673b0d3152010e3374cac189314a13df10", "https://www.semanticscholar.org/paper/Piecewise-Training-for-Undirected-Models-Sutton-McCallum/beb0766d6233836ac1203b224930dc037e2b1dff", "https://www.semanticscholar.org/paper/Estimating-the-%22Wrong%22-Graphical-Model%3A-Benefits-in-Wainwright/f3d831a9447ae7a8f9aa5c5beef8a28dbfacf352", "https://www.semanticscholar.org/paper/Learning-and-Inference-over-Constrained-Output-Punyakanok-Roth/64141d2e7a3cab0be864bbe2d5c7e0212fb17d27", "https://www.semanticscholar.org/paper/The-Computational-Complexity-of-Probabilistic-Using-Cooper/ed5324bb3a19f0dcc2e90e482c06373b934fc28c", "https://www.semanticscholar.org/paper/Learning-associative-Markov-networks-Taskar-Chatalbashev/702c2fde33ccb4328be06405c11e208a4b3ee347", "https://www.semanticscholar.org/paper/Loopy-Belief-Propagation-for-Approximate-Inference%3A-Murphy-Weiss/19908640236767427ebf0524dc3a4bb09d65145e", "https://www.semanticscholar.org/paper/A-Linear-Programming-Formulation-for-Global-in-Roth-Yih/5aa70188f70d349580aed96c10a68f57dace2d33", "https://www.semanticscholar.org/paper/PAC-Bayesian-Stochastic-Model-Selection-McAllester/1af5c57357bbe22364ce106c23ea7b016c316f96", "https://www.semanticscholar.org/paper/MAP-estimation-via-agreement-on-trees%3A-and-linear-Wainwright-Jaakkola/b28024225b22741035cf87203a3639c917959404"]},
{"id": "cbb19236820a96038d000dc629225d36e0b6294a", "title": "Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition", "authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "date": "2015", "abstract": "Existing deep convolutional neural networks (CNNs) require a fixed-size (e.g., 224<inline-formula><tex-math>$\\times$ </tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq1-2389824.gif\"/></alternatives></inline-formula>224) input image. This requirement is \u201cartificial\u201d and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, \u201cspatial pyramid pooling\u201d, to eliminate the above requirement. The new network structure, called SPP-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, SPP-net should in general improve all CNN-based image classification methods. On the ImageNet 2012 dataset, we demonstrate that SPP-net boosts the accuracy of a variety of CNN architectures despite their different designs. On the Pascal VOC 2007 and Caltech101 datasets, SPP-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of SPP-net is also significant in object detection. Using SPP-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 <inline-formula><tex-math>$\\times$</tex-math><alternatives><inline-graphic xlink:type=\"simple\" xlink:href=\"he-ieq2-2389824.gif\"/> </alternatives></inline-formula> faster than the R-CNN method, while achieving better or comparable accuracy on Pascal VOC 2007. In ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2014, our methods rank #2 in object detection and #3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.", "references": ["https://www.semanticscholar.org/paper/Spatial-Pyramid-Pooling-in-Deep-Convolutional-for-He-Zhang/0d0cf9c009409c8945e53fe76b54e9249e3c4e5d", "https://www.semanticscholar.org/paper/Multi-scale-Orderless-Pooling-of-Deep-Convolutional-Gong-Wang/a99add9d76d849a8d47b93532703e4ca0f683b92", "https://www.semanticscholar.org/paper/Network-In-Network-Lin-Chen/5e83ab70d0cbc003471e87ec306d27d9c80ecb16", "https://www.semanticscholar.org/paper/Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman/eb42cf88027de515750f230b23b1a057dc782108", "https://www.semanticscholar.org/paper/Return-of-the-Devil-in-the-Details%3A-Delving-Deep-Chatfield-Simonyan/14d9be7962a4ec5a6e55755f4c7588ea00793652", "https://www.semanticscholar.org/paper/Learning-and-Transferring-Mid-level-Image-Using-Oquab-Bottou/c08f5fa876181fc040d76c75fe2433eee3c9b001", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Going-deeper-with-convolutions-Szegedy-Liu/e15cf50aa89fee8535703b9f9512fca5bfc43327", "https://www.semanticscholar.org/paper/DeCAF%3A-A-Deep-Convolutional-Activation-Feature-for-Donahue-Jia/b8de958fead0d8a9619b55c7299df3257c624a96"]},
{"id": "d67175d17c450ab0ac9c256103828f9e9a0acb85", "title": "Some Improvements on Deep Convolutional Neural Network Based Image Classification", "authors": ["Andrew G. Howard"], "date": "2014", "abstract": "Abstract: We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55% using no external data which is over a 20% relative improvement on the previous year's winner.", "references": ["https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/ImageNet-Classification-with-Deep-Convolutional-Krizhevsky-Sutskever/abd1c342495432171beb7ca8fd9551ef13cbd0ff", "https://www.semanticscholar.org/paper/Visualizing-and-Understanding-Convolutional-Zeiler-Fergus/1a2a770d23b4a171fa81de62a78a3deb0588f238", "https://www.semanticscholar.org/paper/Multi-column-deep-neural-networks-for-image-Ciresan-Meier/398c296d0cc7f9d180f84969f8937e6d3a413796", "https://www.semanticscholar.org/paper/Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava/1366de5bb112746a555e9c0cd00de3ad8628aea8", "https://www.semanticscholar.org/paper/Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser/a8e8f3c8d4418c8d62e306538c9c1292635e9d27", "https://www.semanticscholar.org/paper/ImageNet%3A-A-large-scale-hierarchical-image-database-Deng-Dong/d2c733e34d48784a37d717fe43d9e93277a8c53e", "https://www.semanticscholar.org/paper/Stacked-generalization-Wolpert/7e1291583873fb890e7922ec0dfefd4846df46c9", "https://www.semanticscholar.org/paper/Lessons-from-the-Netflix-prize-challenge-Bell-Koren/bd4318cd5129cf0d6268876888359f87b410d719", "https://www.semanticscholar.org/paper/f-ADoefaa-Doetsch/4dc21a168070dc87266accd9ce2c06ee6115a285"]},
{"id": "fcba51774867c77f491581d3625d375a0a8f473b", "title": "Estimating a Kernel Fisher Discriminant in the Presence of Label Noise", "authors": ["Neil D. Lawrence", "Bernhard Sch\u00f6lkopf"], "date": "2001", "abstract": "Data noise is present in many machine learning problems domains, some of these are well studied but others have received less attention. In this paper we propose an algorithm for constructing a kernel Fisher discriminant (KFD) from training examples with noisy labels. The approach allows to associate with each example a probability of the label being flipped. We utilise an expectation maximization (EM) algorithm for updating the probabilities. The E-step uses class conditional probabilities estimated as a by-product of the KFD algorithm. The M-step updates the flip probabilities and determines the parameters of the discriminant. We demonstrate the feasibility of the approach on two real-world data-sets.", "references": ["https://www.semanticscholar.org/paper/The-Kernel-Gibbs-Sampler-Graepel-Herbrich/fa11394b216de02514eb9a4854b28791ba842c70", "https://www.semanticscholar.org/paper/Nonlinear-Discriminant-Analysis-Using-Kernel-Roth-Steinhage/20700ddb035ffc2d75d6fe3d7307bd5da9125b39", "https://www.semanticscholar.org/paper/Fisher-discriminant-analysis-with-kernels-Mika-R%C3%A4tsch/3e43d731d638f769f12f8ab413d14a77a761856c", "https://www.semanticscholar.org/paper/Generalized-Discriminant-Analysis-Using-a-Kernel-Baudat-Anouar/994e91efb53a4a6b04a562ec10751cd0bbcdeac5", "https://www.semanticscholar.org/paper/Learning-DNF-Via-Probabilistic-Evidence-Combination-Norton-Hirsh/fd01781254105eda92bb41cbd4ce062618559ece", "https://www.semanticscholar.org/paper/Comparing-support-vector-machines-with-Gaussian-to-Sch%C3%B6lkopf-Sung/c4a422669ec9b6a60b05d2d2595314008a5fb419", "https://www.semanticscholar.org/paper/A-Framework-for-Multiple-Instance-Learning-Maron-Lozano-Perez/4d8340eae2c98ab5e0a3b1a7e071a7ddb9106cff", "https://www.semanticscholar.org/paper/Sparse-Kernel-Principal-Component-Analysis-Tipping/9f121c98a53e1889d4d04538343de3b23d38d724", "https://www.semanticscholar.org/paper/Active-Learning-with-Statistical-Models-Cohn-Ghahramani/1150f9289c6151506e3f7cf0e6ebbcfd49f1dace", "https://www.semanticscholar.org/paper/Integrated-Segmentation-and-Recognition-of-Numerals-Keeler-Rumelhart/847d6ece37d22430a0d9e061b5dc1d1b8c679055"]},
{"id": "8b25a44f617c1ed3ed52c6655b0d456ff1c565bd", "title": "Learning Convolutional Feature Hierarchies for Visual Recognition", "authors": ["Koray Kavukcuoglu", "Pierre Sermanet", "Y-Lan Boureau", "Karol Gregor", "Micha\u00ebl Mathieu", "Yann LeCun"], "date": "2010", "abstract": "We propose an unsupervised method for learning multi-stage hierarchies of sparse convolutional features. While sparse coding has become an increasingly popular method for learning visual features, it is most often trained at the patch level. Applying the resulting filters convolutionally results in highly redundant codes because overlapping patches are encoded in isolation. By training convolutionally over large image windows, our method reduces the redudancy between feature vectors at neighboring locations and improves the efficiency of the overall representation. In addition to a linear decoder that reconstructs the image from sparse features, our method trains an efficient feed-forward encoder that predicts quasi-sparse features from the input. While patch-based training rarely produces anything but oriented edge detectors, we show that convolutional training produces highly diverse filters, including center-surround filters, corner detectors, cross detectors, and oriented grating detectors. We show that using these filters in multistage convolutional network architecture improves performance on a number of visual recognition and detection tasks.", "references": ["https://www.semanticscholar.org/paper/Efficient-Learning-of-Sparse-Representations-with-Ranzato-Poultney/932c2a02d462abd75af018125413b1ceaa1ee3f4", "https://www.semanticscholar.org/paper/Learning-invariant-features-through-topographic-Kavukcuoglu-Ranzato/54a9c2553138932426faebcaa67a63a84a56b55d", "https://www.semanticscholar.org/paper/Learning-mid-level-features-for-recognition-Boureau-Bach/498efaa51f5eda731dc6199c3547b9465717fa68", "https://www.semanticscholar.org/paper/Learning-Fast-Approximations-of-Sparse-Coding-Gregor-LeCun/e8f811399746c059bf4d4c3d43334045e0222209", "https://www.semanticscholar.org/paper/Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse/1e80f755bcbf10479afd2338cec05211fdbd325c", "https://www.semanticscholar.org/paper/What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu/1f88427d7aa8225e47f946ac41a0667d7b69ac52", "https://www.semanticscholar.org/paper/Object-recognition-with-features-inspired-by-visual-Serre-Wolf/040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "https://www.semanticscholar.org/paper/Deconvolutional-networks-Zeiler-Krishnan/83dfe3980b875c4e5fe6f2cb1df131cc46d175c8", "https://www.semanticscholar.org/paper/Histograms-of-oriented-gradients-for-human-Dalal-Triggs/cec734d7097ab6b1e60d95228ffd64248eb89d66", "https://www.semanticscholar.org/paper/Integral-Channel-Features-Doll%C3%A1r-Tu/fd375345cbd203aa9c88e1aa3c2e4e1835548b10"]},
{"id": "4dbc68cf2e14155edb6da0def30661aca8c96c22", "title": "Simplifying ConvNets for Fast Learning", "authors": ["Franck Mamalet", "Christophe Garcia"], "date": "2012", "abstract": "In this paper, we propose different strategies for simplifying filters, used as feature extractors, to be learnt in convolutional neural networks (ConvNets) in order to modify the hypothesis space, and to speed-up learning and processing times. We study two kinds of filters that are known to be computationally efficient in feed-forward processing: fused convolution/sub-sampling filters, and separable filters. We compare the complexity of the back-propagation algorithm on ConvNets based on these different kinds of filters. We show that using these filters allows to reach the same level of recognition performance as with classical ConvNets for handwritten digit recognition, up to 3.3 times faster.", "references": ["https://www.semanticscholar.org/paper/Hybrid-convolutional-neural-networks-Mr%C3%A1zov%C3%A1-Kukacka/80e6390f0eacbcc33d410e9703941aef2471b10c", "https://www.semanticscholar.org/paper/Convolutional-face-finder%3A-a-neural-architecture-Garcia-Delakis/68a859142ef42196e6a56305b8c6ac4cb2c9326e", "https://www.semanticscholar.org/paper/High-Performance-Convolutional-Neural-Networks-for-Chellapilla-Puri/2cc157afda51873c30b195fff56e917b9c06b853", "https://www.semanticscholar.org/paper/Handwritten-Digit-Recognition-with-a-Committee-of-Ciresan-Meier/c52df1a43255359ed45ccfedbf6c28b54379a542", "https://www.semanticscholar.org/paper/text-Detection-with-Convolutional-Neural-Networks-Delakis-Garcia/fd790b061082571e20be7892ce4a97e156497c9f", "https://www.semanticscholar.org/paper/Embedded-facial-image-processing-with-Convolutional-Mamalet-Roux/28729bafdb929fa157d7a1a0ba721783e49803ae", "https://www.semanticscholar.org/paper/Real-Time-Video-Convolutional-Face-Finder-on-Mamalet-Roux/9cec1a6d7a1ca2e4750320cfacfc9e385a3df0cf", "https://www.semanticscholar.org/paper/Gradient-based-learning-applied-to-document-LeCun-Bottou/162d958ff885f1462aeda91cd72582323fd6a1f4", "https://www.semanticscholar.org/paper/Automatic-Scene-Text-Recognition-using-a-Neural-Saidane/82bac0ae7d60f5bef57deb837de404b4472ee0a0", "https://www.semanticscholar.org/paper/What-is-the-best-multi-stage-architecture-for-Jarrett-Kavukcuoglu/1f88427d7aa8225e47f946ac41a0667d7b69ac52"]},
{"id": "1596e3cb20ba3b9aefe440e30660c2a9f035f683", "title": "Learning Sub-Word Units for Open Vocabulary Speech Recognition", "authors": ["Carolina Parada", "Mark Dredze", "Abhinav Sethy", "Ariya Rastrow"], "date": "2011", "abstract": "Large vocabulary speech recognition systems fail to recognize words beyond their vocabulary, many of which are information rich terms, like named entities or foreign words. Hybrid word/sub-word systems solve this problem by adding sub-word units to large vocabulary word based systems; new words can then be represented by combinations of sub-word units. Previous work heuristically created the sub-word lexicon from phonetic representations of text using simple statistics to select common phone sequences. We propose a probabilistic model to learn the subword lexicon optimized for a given task. We consider the task of out of vocabulary (OOV) word detection, which relies on output from a hybrid model. A hybrid model with our learned sub-word lexicon reduces error by 6.3% and 7.6% (absolute) at a 5% false alarm rate on an English Broadcast News and MIT Lectures task respectively.", "references": ["https://www.semanticscholar.org/paper/Modelling-out-of-vocabulary-words-for-robust-speech-Bazzi-Glass/ec5f929b57cf12b4d624ab125f337c14ad642ab1", "https://www.semanticscholar.org/paper/Towards-using-hybrid-word-and-fragment-units-for-Rastrow-Sethy/96ba5f76c1bb49bc1a8c294382a30be8dbf6c3d2", "https://www.semanticscholar.org/paper/Detection-of-OOV-words-using-generalized-word-and-a-Schaaf/9b470d0b7a3d0417fb44983071ce38628d406fa3", "https://www.semanticscholar.org/paper/Learning-units-for-domain-independent-out-of-word-Bazzi-Glass/bf7d940ea625da995e6f577d5249379f82e1c004", "https://www.semanticscholar.org/paper/Open-vocabulary-speech-recognition-with-flat-hybrid-Bisani-Ney/325e90957eb2797e6a0e9f85331e19bc59b077a4", "https://www.semanticscholar.org/paper/Linguistically-motivated-sub-word-modeling-with-to-Choueiter/981a87782cd6f4b016b031f5e097c49d5f740d04", "https://www.semanticscholar.org/paper/Vocabulary-independent-spoken-term-detection-Mamou-Ramabhadran/5774a894c6fec8e6412eba12521f778606bce2a5", "https://www.semanticscholar.org/paper/Using-word-confidence-measure-for-OOV-words-in-a-Sun-Zhang/27c63939daf043f810942296bd98f1cac3510846", "https://www.semanticscholar.org/paper/OOV-detection-by-joint-word%2Fphone-lattice-alignment-Lin-Bilmes/c0787db6b91bf8d7c2b175eccaf6ab27be031c81", "https://www.semanticscholar.org/paper/Effect-of-pronounciations-on-OOV-queries-in-spoken-Can-Cooper/8e1dfee72a4d26ea95f8f56ee3d9429fdc114c28"]},
{"id": "822f3b9a392a9abccdaa7ef5ae4183d2d4d3d6db", "title": "Discriminative Transfer Learning with Tree-based Priors", "authors": ["Nitish Srivastava", "Ruslan Salakhutdinov"], "date": "2013", "abstract": "High capacity classifiers, such as deep neural networks, often struggle on classes that have very few training examples. We propose a method for improving classification performance for such classes by discovering similar classes and transferring knowledge among them. Our method learns to organize the classes into a tree hierarchy. This tree structure imposes a prior over the classifier's parameters. We show that the performance of deep neural networks can be improved by applying these priors to the weights in the last layer. Our method combines the strength of discriminatively trained deep neural networks, which typically require large amounts of training data, with tree-based priors, making deep neural networks work well on infrequent classes as well. We also propose an algorithm for learning the underlying tree structure. Starting from an initial pre-specified tree, this algorithm modifies the tree to make it more pertinent to the task being solved, for example, removing semantic relationships in favour of visual ones for an image classification task. Our method achieves state-of-the-art classification results on the CIFAR-100 image data set and the MIR Flickr image-text data set.", "references": ["https://www.semanticscholar.org/paper/Convolutional-deep-belief-networks-for-scalable-of-Lee-Grosse/1e80f755bcbf10479afd2338cec05211fdbd325c", "https://www.semanticscholar.org/paper/Learning-to-Learn-with-Compound-HD-Models-Salakhutdinov-Tenenbaum/e24a5e843d2ea999393b9f278f4b5c80f8a651d1", "https://www.semanticscholar.org/paper/Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky/5d90f06bb70a0a3dced62413346235c02b1aa086", "https://www.semanticscholar.org/paper/Multi-Task-Learning-for-Classification-with-Process-Xue-Liao/89c808af926ecb20870b2521fbaa7dcbb85be106", "https://www.semanticscholar.org/paper/Multimodal-learning-with-deep-Boltzmann-machines-Srivastava-Salakhutdinov/5726c7b40fcc454b77d989656c085520bf6c15fa", "https://www.semanticscholar.org/paper/Hierarchical-Regularization-Cascade-for-Joint-Zweig-Weinshall/bff05119cd30c2c61323861a5e2a28094388427f", "https://www.semanticscholar.org/paper/Multimodal-semi-supervised-learning-for-image-Guillaumin-Verbeek/e0f49caabbf79ffda35432219bb0ec9b41753dff", "https://www.semanticscholar.org/paper/One-shot-learning-of-object-categories-Fei-Fei-Fergus/812355cec91fa30bb50e9e992a3549af39e4f6eb", "https://www.semanticscholar.org/paper/Scaling-learning-algorithms-towards-AI-Bengio-LeCun/6fdb77260fc83dff91c44fea0f31a2cb8ed13d04", "https://www.semanticscholar.org/paper/Tree-Guided-Group-Lasso-for-Multi-Task-Regression-Kim-Xing/4f5e2d78128805249cff3075dc7e8e526f0e4fb1"]},
{"id": "dac72f2c509aee67524d3321f77e97e8eff51de6", "title": "Word Representations: A Simple and General Method for Semi-Supervised Learning", "authors": ["Joseph P. Turian", "Lev-Arie Ratinov", "Yoshua Bengio"], "date": "2010", "abstract": "If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features. We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih & Hinton, 2009) embeddings of words on both NER and chunking. We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines. We find further improvements by combining different word representations. You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here: http://metaoptimize.com/projects/wordreprs/", "references": ["https://www.semanticscholar.org/paper/A-Preliminary-Evaluation-of-Word-Representations-Turian/1fb11eca39c0edab06c046d6e09d9e1331e5b7bf", "https://www.semanticscholar.org/paper/Semi-Supervised-Sequential-Labeling-and-Using-Scale-Suzuki-Isozaki/7ece4e8d31f872d928369ac2cf58a616a7182112", "https://www.semanticscholar.org/paper/Distributional-Representations-for-Handling-in-Huang-Yates/c8c007a5d86caa2fc77f186bae974b9af39428f1", "https://www.semanticscholar.org/paper/Semi-supervised-Semantic-Role-Labeling-Using-the-Deschacht-Moens/ae29b936d437a93ad259ee008ba56fe82ab4db61", "https://www.semanticscholar.org/paper/Simple-Semi-supervised-Dependency-Parsing-Koo-Carreras/790ecefeaf2b471b439743a772ccce026131bef5", "https://www.semanticscholar.org/paper/A-High-Performance-Semi-Supervised-Learning-Method-Ando-Zhang/b4299baa815ca5a815a70fba94a9f6f2b42fff19", "https://www.semanticscholar.org/paper/Semi-Supervised-Sequence-Modeling-with-Syntactic-Li-McCallum/b89926ec5f0046f3a5671d8e68c918ab9cac76fd", "https://www.semanticscholar.org/paper/A-unified-architecture-for-natural-language-deep-Collobert-Weston/57458bc1cffe5caa45a885af986d70f723f406b4", "https://www.semanticscholar.org/paper/Semi-Supervised-Learning-for-Natural-Language-Liang/31b4c03d721dc10b87c178277c1d369f91db8f0e", "https://www.semanticscholar.org/paper/Improving-generative-statistical-parsing-with-word-Candito-Crabb%C3%A9/9474fe3f566863ab7410e74b66ac848eac7cb4e2"]},
{"id": "2ed4ebe1878fd8e421f24e1aac76fbdc89e9d381", "title": "Unbounded cache model for online language modeling with open vocabulary", "authors": ["Edouard Grave", "Moustapha Ciss\u00e9", "Armand Joulin"], "date": "2017", "abstract": "Recently, continuous cache models were proposed as extensions to recurrent neural network language models, to adapt their predictions to local changes in the data distribution. These models only capture the local context, of up to a few thousands tokens. In this paper, we propose an extension of continuous cache models, which can scale to larger contexts. In particular, we use a large scale non-parametric memory component that stores all the hidden activations seen in the past. We leverage recent advances in approximate nearest neighbor search and quantization algorithms to store millions of representations while searching them efficiently. We conduct extensive experiments showing that our approach significantly improves the perplexity of pre-trained language models on new distributions, and can scale efficiently to much larger contexts than previously proposed local cache models.", "references": ["https://www.semanticscholar.org/paper/Improving-Neural-Language-Models-with-a-Continuous-Grave-Joulin/2d7782c225e0fc123d6e227f2cb253e58279ac73", "https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Language-Modeling-J%C3%B3zefowicz-Vinyals/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "https://www.semanticscholar.org/paper/Pointer-Sentinel-Mixture-Models-Merity-Xiong/efbd381493bb9636f489b965a2034d529cd56bcd", "https://www.semanticscholar.org/paper/Context-dependent-recurrent-neural-network-language-Mikolov-Zweig/d1275b2a2ab53013310e759e5c6878b96df643d4", "https://www.semanticscholar.org/paper/Modeling-long-distance-dependence-in-language%3A-Iyer-Ostendorf/6bf29693c91924958f6a6427a1bcbe96b244aa66", "https://www.semanticscholar.org/paper/One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov/5d833331b0e22ff359db05c62a8bca18c4f04b68", "https://www.semanticscholar.org/paper/Efficient-softmax-approximation-for-GPUs-Grave-Joulin/9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "https://www.semanticscholar.org/paper/A-bit-of-progress-in-language-modeling-Goodman/09c76da2361d46689825c4efc37ad862347ca577", "https://www.semanticscholar.org/paper/Towards-better-integration-of-semantic-predictors-Coccaro-Jurafsky/b888cae7e6e288b108f9d119fc23b84b4d447029", "https://www.semanticscholar.org/paper/KenLM%3A-Faster-and-Smaller-Language-Model-Queries-Heafield/883d1d06d857a85a0e64bb19f0b17d56f2cc9d7b"]},
{"id": "649d03490ef72c5274e3bccd03d7a299d2f8da91", "title": "Learning Word Vectors for Sentiment Analysis", "authors": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "date": "2011", "abstract": "Unsupervised vector-based approaches to semantics can model rich lexical meanings, but they largely fail to capture sentiment information that is central to many word meanings and important for a wide range of NLP tasks. We present a model that uses a mix of unsupervised and supervised techniques to learn word vectors capturing semantic term--document information as well as rich sentiment content. The proposed model can leverage both continuous and multi-dimensional sentiment information as well as non-sentiment annotations. We instantiate the model to utilize the document-level sentiment polarity annotations present in many online documents (e.g. star ratings). We evaluate the model using small, widely used sentiment and subjectivity corpora and find it out-performs several previously introduced methods for sentiment classification. We also introduce a large dataset of movie reviews to serve as a more robust benchmark for work in this area.", "references": ["https://www.semanticscholar.org/paper/Holistic-Sentiment-Analysis-Across-Languages%3A-Boyd-Graber-Resnik/746c085477ec83bee8ca28cb4c1482439905ee15", "https://www.semanticscholar.org/paper/Mining-WordNet-for-a-Fuzzy-Sentiment%3A-Sentiment-Tag-Andreevskaia-Bergler/d2d8b52f59945b4a3ef9d20ab44e108319eead6f", "https://www.semanticscholar.org/paper/Sentiment-Analysis-with-Global-Topics-and-Local-Li-Huang/75de299d2aeccfbadeced0e8fdbd54030f7cd927", "https://www.semanticscholar.org/paper/Delta-TFIDF%3A-An-Improved-Feature-Space-for-Analysis-Martineau-Finin/5ef2edfe51d3d768b1d89ad7e74e4ce8e55d1d49", "https://www.semanticscholar.org/paper/A-Sentimental-Education%3A-Sentiment-Analysis-Using-Pang-Lee/167e1359943b96b9e92ee73db1df69a1f65d731d", "https://www.semanticscholar.org/paper/A-Study-of-Information-Retrieval-Weighting-Schemes-Paltoglou-Thelwall/4ac9bf53e4c5fae5ff6feacf96f2e2c474fcc6ca", "https://www.semanticscholar.org/paper/Using-appraisal-groups-for-sentiment-analysis-Whitelaw-Garg/0470303953de1d19765423f719d8314e3cb91278", "https://www.semanticscholar.org/paper/Thumbs-up-Sentiment-Classification-using-Machine-Pang-Lee/12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "https://www.semanticscholar.org/paper/Word-Representations%3A-A-Simple-and-General-Method-Turian-Ratinov/dac72f2c509aee67524d3321f77e97e8eff51de6", "https://www.semanticscholar.org/paper/Sentiment-Classification-of-Movie-Reviews-Using-Kennedy-Inkpen/e14609a3a6c6f8ef3269d3e0728f88da57826698"]},
{"id": "5d1d86c1990a21cdb71aea8f1939eddc36dbbe9e", "title": "The Acquisition of Anaphora by Simple Recurrent Networks", "authors": ["Robert Frank", "Donald Mathis", "William Badecker"], "date": "2013", "abstract": "This article applies Simple Recurrent Networks (SRNs; Elman 1991, 1993) to the task of assigning an interpretation to reflexive and pronominal anaphora. This task demands more refined sensitivity to syntactic structure than has been previously explored. Measured quantitatively, SRNs perform quite well. However, the way in which they achieve such performance diverges in key respects from the target grammar: (i) linear N-V-reflexive/pronoun sequences affect the SRN's interpretations, even without a relevant structural relation, yielding errors unlike those made by humans; (ii) the SRN's representations distinguish sentence types, inhibiting structural generalization; (iii) the SRN's knowledge of the conditions on anaphoric dependencies fails to generalize to novel lexical items. These results have important consequences not only for the viability of SRNs as models of language learning but also for the systematicity of generalization in neural networks (Hadley 1994; Marcus 1998).", "references": ["https://www.semanticscholar.org/paper/The-processing-role-of-structural-constraints-on-of-Badecker-Straub/330ba0ced08b4551a2619a1d2d694a415927651a", "https://www.semanticscholar.org/paper/Learnability-and-the-Statistical-Structure-of-of-Lewis-Elman/ab4c1da6b9ecfa12d31b3767abb758726be92583", "https://www.semanticscholar.org/paper/Structure-dependence-in-grammar-formation-Crain-Nakayama/e1a06b2022bcd4ad1dfe5ee8f19c8f521d91f30e", "https://www.semanticscholar.org/paper/Proximity-in-agreement-errors-Hemforth-Konieczny/4e5f2e84e77e36fd5c8e963637d65ff404ea6f22", "https://www.semanticscholar.org/paper/The-emergence-of-grammaticality-in-connectionist-Allen-Seidenberg/724b26153983e669253226c974860bf00a0312b4", "https://www.semanticscholar.org/paper/Language-acquisition-in-the-absence-of-explicit-how-Rohde-Plaut/47133d54d4a5f1fb3c46bdbf3a7a5e270d930e2f", "https://www.semanticscholar.org/paper/A-connectionist-model-of-sentence-comprehension-and-Rohde-Plaut/0af7d0b414827af2865c03ab52b2bee8b94fe646", "https://www.semanticscholar.org/paper/Subject%E2%80%93Verb-Agreement-Processes-in-Comprehension%E2%98%86-Nicol-Forster/6655ceb8b516ab401ef1ffe5d4f317c5637a11ed", "https://www.semanticscholar.org/paper/Phonology-and-syntax-in-specific-language-Evidence-Joanisse-Seidenberg/33f7fc7e244318a90a7f0c7b6fca5ee06a60c1e4", "https://www.semanticscholar.org/paper/Distributed-Representations%2C-Simple-Recurrent-And-Elman/d3dba8be4a83cca022aa8c85d65094d5f7412e05"]},
{"id": "1109b663453e78a59e4f66446d71720ac58cec25", "title": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks", "authors": ["Pierre Sermanet", "David Eigen", "Xiang Zhang", "Micha\u00ebl Mathieu", "Rob Fergus", "Yann LeCun"], "date": "2014", "abstract": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.", "references": ["https://www.semanticscholar.org/paper/Learning-Hierarchical-Features-for-Scene-Labeling-Farabet-Couprie/48adff169c044c674e7cbcc033c81d77c7ac9b43", "https://www.semanticscholar.org/paper/Object-Recognition-by-Sequential-Figure-Ground-Carreira-Li/140d2acd4cdbc30b102dac34f4c68f279ace6a26", "https://www.semanticscholar.org/paper/Traffic-sign-recognition-with-multi-scale-Networks-Sermanet-LeCun/9ab0de951cc9cdf16887b1f841f8da6affc9c0de", "https://www.semanticscholar.org/paper/text-Detection-with-Convolutional-Neural-Networks-Delakis-Garcia/fd790b061082571e20be7892ce4a97e156497c9f", "https://www.semanticscholar.org/paper/Transforming-Autoencoders-Hinton-Krizhevsky/79ef1a3843a2dc01bde67c3a9a17c6deb352e285", "https://www.semanticscholar.org/paper/Convolutional-face-finder%3A-a-neural-architecture-Garcia-Delakis/68a859142ef42196e6a56305b8c6ac4cb2c9326e", "https://www.semanticscholar.org/paper/Pedestrian-Detection-with-Unsupervised-Multi-stage-Sermanet-Kavukcuoglu/a1306ce652f556fbb9e794d91084a29294298e6d", "https://www.semanticscholar.org/paper/Supervised-Learning-of-Image-Restoration-with-Jain-Murray/f566b1f24e63151ddae652826638af054973a27f", "https://www.semanticscholar.org/paper/Selective-Search-for-Object-Recognition-Uijlings-Sande/38b6540ddd5beebffd05047c78183f7575559fb2", "https://www.semanticscholar.org/paper/Multi-column-deep-neural-networks-for-image-Ciresan-Meier/398c296d0cc7f9d180f84969f8937e6d3a413796"]},
{"id": "4c7e85ff37dd8b99d8f443eabd3b163ff8b71538", "title": "Reading and Thinking: Re-read LSTM Unit for Textual Entailment Recognition", "authors": ["Lei Sha", "Baobao Chang", "Zhifang Sui", "Sujian Li"], "date": "2016", "abstract": "Recognizing Textual Entailment (RTE) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI1) corpus has made it possible to develop and evaluate deep neural network methods for the RTE task. Previous neural network based methods usually try to encode the two sentences and send them together into multi-layer perceptron, or use LSTM-RNN to link two sentence together while using attention mechanic to enhance the model\u2019s ability. In this paper, we propose to use the intensive reading mechanic, which means to re-read the sentence (read the sentence again) according to the memory of the other sentence for a better understanding of the sentence pair. The re-read process can be applied alternatively between the two sentences. Experiments show that we achieve results better than current state-of-art equivalents.", "references": ["https://www.semanticscholar.org/paper/Learning-Natural-Language-Inference-using-LSTM-and-Liu-Sun/f93a0a3e8a3e6001b4482430254595cf737697fa", "https://www.semanticscholar.org/paper/Learning-Natural-Language-Inference-with-LSTM-Wang-Jiang/596c882de006e4bb4a93f1fa08a5dd467bee060a", "https://www.semanticscholar.org/paper/ABCNN%3A-Attention-Based-Convolutional-Neural-Network-Yin-Sch%C3%BCtze/7f3ae283243e15e05f188a05779ccfae9a3567f4", "https://www.semanticscholar.org/paper/Reasoning-about-Entailment-with-Neural-Attention-Rockt%C3%A4schel-Grefenstette/2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "https://www.semanticscholar.org/paper/Recognizing-Textual-Entailment-Using-Probabilistic-Sha-Li/78821ed420ead11c47f72dbbac7f67e17ecf3a58", "https://www.semanticscholar.org/paper/A-Fast-Unified-Model-for-Parsing-and-Sentence-Bowman-Gauthier/36c097a225a95735271960e2b63a2cb9e98bff83", "https://www.semanticscholar.org/paper/Using-Dependency-Based-Features-to-Take-the-out-of-Wan-Dras/c6afe8a8aa13de8e3f2710ef07b22ce86a005419", "https://www.semanticscholar.org/paper/A-large-annotated-corpus-for-learning-natural-Bowman-Angeli/f04df4e20a18358ea2f689b4c129781628ef7fc1", "https://www.semanticscholar.org/paper/Long-Short-Term-Memory-Networks-for-Machine-Reading-Cheng-Dong/13fe71da009484f240c46f14d9330e932f8de210", "https://www.semanticscholar.org/paper/Modelling-Interaction-of-Sentence-Pair-with-Liu-Qiu/10d89efc96beb45676149c8a3237a86a72a2116e"]},
{"id": "3a7b63b50c64f4ec3358477790e84cbd6be2a0b4", "title": "Bidirectional Attention Flow for Machine Comprehension", "authors": ["Minjoon Seo", "Aniruddha Kembhavi", "Ali Farhadi", "Hannaneh Hajishirzi"], "date": "2017", "abstract": "Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test.", "references": ["https://www.semanticscholar.org/paper/Gated-Attention-Readers-for-Text-Comprehension-Dhingra-Liu/0f2ea810c16275dc74e880296e20dbd83b1bae1c", "https://www.semanticscholar.org/paper/Attention-over-Attention-Neural-Networks-for-Cui-Chen/c6e5df6322659276da6133f9b734a389d7a255e8", "https://www.semanticscholar.org/paper/Text-Understanding-with-the-Attention-Sum-Reader-Kadlec-Schmid/f2e50e2ee4021f199877c8920f1f984481c723aa", "https://www.semanticscholar.org/paper/Iterative-Alternating-Neural-Attention-for-Machine-Sordoni-Bachman/f314339651cb25e4234e0b96fe8bd87206847993", "https://www.semanticscholar.org/paper/Machine-Comprehension-Using-Match-LSTM-and-Answer-Wang-Jiang/ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "https://www.semanticscholar.org/paper/Dynamic-Memory-Networks-for-Visual-and-Textual-Xiong-Merity/f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "https://www.semanticscholar.org/paper/Stacked-Attention-Networks-for-Image-Question-Yang-He/2c1890864c1c2b750f48316dc8b650ba4772adc5", "https://www.semanticscholar.org/paper/Dynamic-Coattention-Networks-For-Question-Answering-Xiong-Zhong/e978d832a4d86571e1b52aa1685dc32ccb250f50", "https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5", "https://www.semanticscholar.org/paper/ReasoNet%3A-Learning-to-Stop-Reading-in-Machine-Shen-Huang/c636a2dd242908fe2e598a1077c0c57bfdea8633"]},
{"id": "64141d2e7a3cab0be864bbe2d5c7e0212fb17d27", "title": "Learning and Inference over Constrained Output", "authors": ["Vasin Punyakanok", "Dan Roth", "Wen-tau Yih", "Dav Zimak"], "date": "2005", "abstract": "We study learning structured output in a discriminative framework where values of the output variables are estimated by local classifiers. In this framework, complex dependencies among the output variables are captured by constraints and dictate which global labels can be inferred. We compare two strategies, learning independent classifiers and inference based training, by observing their behaviors in different conditions. Experiments and theoretical justification lead to the conclusion that using inference based learning is superior when the local classifiers are difficult to learn but may require many examples before any discernible difference can be observed.", "references": ["https://www.semanticscholar.org/paper/Online-Learning-via-Global-Feedback-for-Phrase-Carreras-Villodre/b25eedcecf98c3f6684416a6e79b3ffbfe523818", "https://www.semanticscholar.org/paper/Constraint-Classification%3A-A-New-Approach-to-Har-Peled-Roth/ec7e5aecf2794d12ee89f643fed8be21d5d1d8fc", "https://www.semanticscholar.org/paper/Semantic-Role-Labeling-Via-Integer-Linear-Inference-Punyakanok-Roth/480519b1488d3457a714342d4ffe7a3e363daf70", "https://www.semanticscholar.org/paper/Maximum-Entropy-Markov-Models-for-Information-and-McCallum-Freitag/bece46ed303f8eaef2affae2cba4e0aef51fe636", "https://www.semanticscholar.org/paper/Building-a-Large-Annotated-Corpus-of-English%3A-The-Marcus-Santorini/0b44fcbeea9415d400c5f5789d6b892b6f98daff", "https://www.semanticscholar.org/paper/Conditional-Random-Fields%3A-Probabilistic-Models-for-Lafferty-McCallum/f4ba954b0412773d047dc41231c733de0c1f4926"]},
{"id": "2cd8e8f510c89c7c18268e8ad51c061e459ad321", "title": "A Decomposable Attention Model for Natural Language Inference", "authors": ["Ankur P. Parikh", "Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Jakob Uszkoreit"], "date": "2016", "abstract": "We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements.", "references": ["https://www.semanticscholar.org/paper/A-large-annotated-corpus-for-learning-natural-Bowman-Angeli/f04df4e20a18358ea2f689b4c129781628ef7fc1", "https://www.semanticscholar.org/paper/Learning-Natural-Language-Inference-with-LSTM-Wang-Jiang/596c882de006e4bb4a93f1fa08a5dd467bee060a", "https://www.semanticscholar.org/paper/Natural-Language-Inference-by-Tree-Based-and-Mou-Men/ea407573bfcd39f9a478fe33cf6ce0ee1780a5f0", "https://www.semanticscholar.org/paper/Order-Embeddings-of-Images-and-Language-Vendrov-Kiros/46b8cbcdff87b842c2c1d4a003c831f845096ba7", "https://www.semanticscholar.org/paper/Reasoning-about-Entailment-with-Neural-Attention-Rockt%C3%A4schel-Grefenstette/2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "https://www.semanticscholar.org/paper/A-Fast-Unified-Model-for-Parsing-and-Sentence-Bowman-Gauthier/36c097a225a95735271960e2b63a2cb9e98bff83", "https://www.semanticscholar.org/paper/Paraphrase-Driven-Learning-for-Open-Question-Fader-Zettlemoyer/c0be2ac2f45681f1852fc1d298af5dceb85834f4", "https://www.semanticscholar.org/paper/Recognising-Textual-Entailment-with-Logical-Bos-Markert/ea2563467c1c472a346d165b7f97c86317d63ca4", "https://www.semanticscholar.org/paper/A-Phrase-Based-Alignment-Model-for-Natural-Language-MacCartney-Galley/581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "https://www.semanticscholar.org/paper/ABCNN%3A-Attention-Based-Convolutional-Neural-Network-Yin-Sch%C3%BCtze/7f3ae283243e15e05f188a05779ccfae9a3567f4"]},
{"id": "a5c48c673b0d3152010e3374cac189314a13df10", "title": "Learning as search optimization: approximate large margin methods for structured prediction", "authors": ["Hal Daum\u00e9", "Daniel Marcu"], "date": "2005", "abstract": "Mappings to structured output spaces (strings, trees, partitions, etc.) are typically learned using extensions of classification algorithms to simple graphical structures (eg., linear chains) in which search and parameter estimation can be performed exactly. Unfortunately, in many complex problems, it is rare that exact search or parameter estimation is tractable. Instead of learning exact models and searching via heuristic means, we embrace this difficulty and treat the structured output problem in terms of approximate search. We present a framework for learning as search optimization, and two parameter updates with convergence the-orems and bounds. Empirical evidence shows that our integrated approach to learning and decoding can outperform exact models at smaller computational cost.", "references": []},
{"id": "e8f811399746c059bf4d4c3d43334045e0222209", "title": "Learning Fast Approximations of Sparse Coding", "authors": ["Karol Gregor", "Yann LeCun"], "date": "2010", "abstract": "In Sparse Coding (SC), input vectors are reconstructed using a sparse linear combination of basis vectors. SC has become a popular method for extracting features from data. For a given input, SC minimizes a quadratic reconstruction error with an L1 penalty term on the code. The process is often too slow for applications such as real-time pattern recognition. We proposed two versions of a very fast algorithm that produces approximate estimates of the sparse code that can be used to compute good visual features, or to initialize exact iterative algorithms. The main idea is to train a non-linear, feed-forward predictor with a specific architecture and a fixed depth to produce the best possible approximation of the sparse code. A version of the method, which can be seen as a trainable version of Li and Osher's coordinate descent method, is shown to produce approximate solutions with 10 times less computation than Li and Os-her's for the same approximation error. Unlike previous proposals for sparse code predictors, the system allows a kind of approximate \"explaining away\" to take place during inference. The resulting predictor is differentiable and can be included into globally-trained recognition systems.", "references": ["https://www.semanticscholar.org/paper/Efficient-sparse-coding-algorithms-Lee-Battle/e64a9960734215e2b1866ea3cb723ffa5585ac14", "https://www.semanticscholar.org/paper/Online-dictionary-learning-for-sparse-coding-Mairal-Bach/cf80cc34528273d8fbe17783efe802a6509e1562", "https://www.semanticscholar.org/paper/Fast-Inference-in-Sparse-Coding-Algorithms-with-to-Kavukcuoglu-Ranzato/c63ef05c5f9c424b5cfeeed90dbe35eedf6cb8ec", "https://www.semanticscholar.org/paper/Optimally-sparse-representation-in-general-via-%E2%84%931-Donoho-Elad/17e7cca7e795d8ba1fa9d2c88bf2675c2d6ddfe8", "https://www.semanticscholar.org/paper/Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field/8012c4a1e2ca663f1a04e80cbb19631a00cbab27", "https://www.semanticscholar.org/paper/Sparse-Coding-via-Thresholding-and-Local-in-Neural-Rozell-Johnson/20b003b91679fd88b63e9bfab6f05f1e7ecabe2c", "https://www.semanticscholar.org/paper/Linear-spatial-pyramid-matching-using-sparse-coding-Yang-Yu/db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "https://www.semanticscholar.org/paper/Sparse-Representation-for-Color-Image-Restoration-Mairal-Elad/92281d5002178003bd7060fc66677a3471cdaa4b", "https://www.semanticscholar.org/paper/Nonlinear-Learning-using-Local-Coordinate-Coding-Yu-Zhang/a5e23ef59eaf3fd897c460c28d23a982c72e8f65", "https://www.semanticscholar.org/paper/Atomic-Decomposition-by-Basis-Pursuit-Chen-Donoho/9af121fbed84c3484ab86df8f17f1f198ed790a0"]},
{"id": "7e1291583873fb890e7922ec0dfefd4846df46c9", "title": "Stacked generalization", "authors": ["David H. Wolpert"], "date": "1992", "abstract": "This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory.", "references": ["https://www.semanticscholar.org/paper/On-the-ability-of-neural-networks-to-perform-by-Anshelevich-Amirikian/278d50721acb7abd1d3c374713705b9934f19291", "https://www.semanticscholar.org/paper/Hierarchical-training-of-neural-networks-and-of-Deppisch-Bauer/0341b7b5195b4a5a3636f242a2ab4eb6f480ac47", "https://www.semanticscholar.org/paper/Nonlinear-prediction-of-chaotic-time-series-Casdagli/be946457d3f880d9ec836aee3d0d231ffa3bcc9a"]},
{"id": "040c23e5a409fbdedd5032263dfcb1a4d7dfd200", "title": "Object recognition with features inspired by visual cortex", "authors": ["Thomas Serre", "Lior Wolf", "Tomaso A. Poggio"], "date": "2005", "abstract": "We introduce a novel set of features for robust object recognition. Each element of this set is a complex feature obtained by combining position- and scale-tolerant edge-detectors over neighboring positions and multiple orientations. Our system's architecture is motivated by a quantitative model of visual cortex. We show that our approach exhibits excellent recognition performance and outperforms several state-of-the-art systems on a variety of image datasets including many different object categories. We also demonstrate that our system is able to learn from very few examples. The performance of the approach constitutes a suggestive plausibility proof for a class of feedforward models of object recognition in cortex.", "references": ["https://www.semanticscholar.org/paper/A-New-Biologically-Motivated-Framework-for-Robust-Serre-Wolf/830eb22c97caae687fbd22bae103c554a1d496a5", "https://www.semanticscholar.org/paper/On-the-Role-of-Object-Specific-Features-for-Real-in-Serre-Riesenhuber/7aa7c0df2c2e154d7bddb873168ebc8446472425", "https://www.semanticscholar.org/paper/Unsupervised-Learning-of-Models-for-Recognition-Weber-Welling/1cf1527807ebb16020b04d4166e7ba8d27652302", "https://www.semanticscholar.org/paper/An-integrated-network-for-invariant-visual-and-Amit-Mascaro/56f4906d8fcc4097299d9644ee54fa4534b8e594", "https://www.semanticscholar.org/paper/Object-class-recognition-by-unsupervised-learning-Fergus-Perona/62837ab473124ea43cb8d7c6a4b4ee0f6f14e8c5", "https://www.semanticscholar.org/paper/Object-recognition-from-local-scale-invariant-Lowe/f9f836d28f52ad260213d32224a6d227f8e8849a", "https://www.semanticscholar.org/paper/Hierarchical-models-of-object-recognition-in-cortex-Riesenhuber-Poggio/85abadb689897997f1e37baa7b5fc6f7d497518b", "https://www.semanticscholar.org/paper/Visual-features-of-intermediate-complexity-and-use-Ullman-Vidal-Naquet/d52be22dc0033293d335b6dc5cf3e3588c1fc0bc", "https://www.semanticscholar.org/paper/SEEMORE%3A-Combining-Color%2C-Shape%2C-and-Texture-in-a-Mel/33b98b5dc150680fc02a14c0cf629168dd0af08b", "https://www.semanticscholar.org/paper/Learning-Optimized-Features-for-Hierarchical-Models-Wersing-K%C3%B6rner/d44f85fcc2eaa17e3c08313ad3ce70f8accf46fb"]},
{"id": "c0787db6b91bf8d7c2b175eccaf6ab27be031c81", "title": "OOV detection by joint word/phone lattice alignment", "authors": ["Hui Lin", "Jeff A. Bilmes", "Dimitra Vergyri", "Katrin Kirchhoff"], "date": "2007", "abstract": "We propose a new method for detecting out-of-vocabulary (OOV) words for large vocabulary continuous speech recognition (LVCSR) systems. Our method is based on performing a joint alignment between independently generated word and phone lattices, where the word-lattice is aligned via a recognition lexicon. Based on a similarity measure between phones, we can locate highly mis-aligned regions of time, and then specify those regions as candidate OOVs. This novel approach is implemented using the framework of graphical models (GMs), which enable fast flexible integration of different scores from word lattices, phone lattices, and the similarity measures. We evaluate our method on switchboard data using RT-04 as test set. Experimental results show that our approach provides a promising and scalable new way to detect OOV for LVCSR.", "references": ["https://www.semanticscholar.org/paper/Hybrid-language-models-for-out-of-vocabulary-word-Yazgan-Sara%C3%A7lar/9bb497d05ac4e2a12ac3e3abb08cef477c286532", "https://www.semanticscholar.org/paper/Detection-of-OOV-words-using-generalized-word-and-a-Schaaf/9b470d0b7a3d0417fb44983071ce38628d406fa3", "https://www.semanticscholar.org/paper/Modeling-out-of-vocabulary-words-for-robust-speech-Bazzi-Glass/26208cdcf49948cddb6438fc1c70c26b4a8b4926", "https://www.semanticscholar.org/paper/Two-pass-strategy-for-handling-OOVs-in-a-large-task-Scharenborg-Seneff/9d403e0cad5cb80f3f6bd42bfb0a53c80a159085", "https://www.semanticscholar.org/paper/Using-word-confidence-measure-for-OOV-words-in-a-Sun-Zhang/27c63939daf043f810942296bd98f1cac3510846", "https://www.semanticscholar.org/paper/GRAPHICAL-MODEL-REPRESENTATIONS-OF-WORD-LATTICES-Ji-Bilmes/7afbd4786634bca104632265c37f1662dda883b5", "https://www.semanticscholar.org/paper/Recognition-Confidence-Measures%3A-Detection-of-and-Young/b21ed328b414d3b72e5b688200bc90e942126948", "https://www.semanticscholar.org/paper/Open-vocabulary-speech-recognition-with-flat-hybrid-Bisani-Ney/325e90957eb2797e6a0e9f85331e19bc59b077a4", "https://www.semanticscholar.org/paper/Confidence-measures-for-large-vocabulary-continuous-Wessel-Schl%C3%BCter/d3a51176b45a6b40b15d096a5cee605cddbe5644", "https://www.semanticscholar.org/paper/Recent-innovations-in-speech-to-text-transcription-Stolcke-Chen/4e3ba28fb3493afd2c3db4bd8be6d8d41cf3647a"]},
{"id": "89c808af926ecb20870b2521fbaa7dcbb85be106", "title": "Multi-Task Learning for Classification with Dirichlet Process Priors", "authors": ["Ya Xue", "Xuejun Liao", "Lawrence Carin", "Balaji Krishnapuram"], "date": "2007", "abstract": "Consider the problem of learning logistic-regression models for multiple classification tasks, where the training data set for each task is not drawn from the same statistical distribution. In such a multi-task learning (MTL) scenario, it is necessary to identify groups of similar tasks that should be learned jointly. Relying on a Dirichlet process (DP) based statistical model to learn the extent of similarity between classification tasks, we develop computationally efficient algorithms for two different forms of the MTL problem. First, we consider a symmetric multi-task learning (SMTL) situation in which classifiers for multiple tasks are learned jointly using a variational Bayesian (VB) algorithm. Second, we consider an asymmetric multi-task learning (AMTL) formulation in which the posterior density function from the SMTL model parameters (from previous tasks) is used as a prior for a new task: this approach has the significant advantage of not requiring storage and use of all previous data from prior tasks. The AMTL formulation is solved with a simple Markov Chain Monte Carlo (MCMC) construction. Experimental results on two real life MTL problems indicate that the proposed algorithms: (a) automatically identify subgroups of related tasks whose training data appear to be drawn from similar distributions; and (b) are more accurate than simpler approaches such as single-task learning, pooling of data across all tasks, and simplified approximations to DP.", "references": ["https://www.semanticscholar.org/paper/Variational-inference-for-Dirichlet-process-Jordan-Jordan/e996344199af190bdd47ee31a652e76811a5487b", "https://www.semanticscholar.org/paper/Learning-Multiple-Related-Tasks-using-Latent-Zhang-Ghahramani/12ccc43692cf774c66100e95ee2992afd7382ab5", "https://www.semanticscholar.org/paper/Learning-to-learn-with-the-informative-vector-Lawrence-Platt/73e1c4a1152a75ec7310adfb4b8daea16d627bc7", "https://www.semanticscholar.org/paper/Discovering-Structure-in-Multiple-Learning-Tasks%3A-Thrun-O'Sullivan/2f42a55da3a222184eee20c67d374a9134b77bdc", "https://www.semanticscholar.org/paper/A-CONSTRUCTIVE-DEFINITION-OF-DIRICHLET-PRIORS-Sethuramant/e1ba33f56f75c40aed15538073e377c23f4013da", "https://www.semanticscholar.org/paper/A-nonparametric-hierarchical-bayesian-framework-for-Yu-Tresp/5fd2b7cb05b30c59bbc13a5f2b7ec68555fba261", "https://www.semanticscholar.org/paper/A-Model-of-Inductive-Bias-Learning-Baxter/727e1e16ede6eaad241bad11c525da07b154c688", "https://www.semanticscholar.org/paper/Learning-Generative-Visual-Models-from-Few-Training-Fei-Fei-Fergus/ed9db7b20e019cdb1c7db8b7921221ee2d9f36e2", "https://www.semanticscholar.org/paper/Gibbs-Sampling-Methods-for-Stick-Breaking-Priors-Ishwaran-James/45db16309fbf7e9fc8907047e7e1a9933c4e1b85", "https://www.semanticscholar.org/paper/Employing-Em-in-Pool-based-Active-Learning-for-Text-Nigamyknigam/6f17768a9fe231a2fd38708be90f98db3890c986"]},
{"id": "4f5e2d78128805249cff3075dc7e8e526f0e4fb1", "title": "Tree-Guided Group Lasso for Multi-Task Regression with Structured Sparsity", "authors": ["Seyoung Kim", "Eric P. Xing"], "date": "2010", "abstract": "We consider the problem of learning a sparse multi-task regression, where the structure in the outputs can be represented as a tree with leaf nodes as outputs and internal nodes as clusters of the outputs at multiple granularity. Our goal is to recover the common set of relevant inputs for each output cluster. Assuming that the tree structure is available as prior knowledge, we formulate this problem as a new multi-task regularized regression called tree-guided group lasso. Our structured regularization is based on a group-lasso penalty, where groups are defined with respect to the tree structure. We describe a systematic weighting scheme for the groups in the penalty such that each output variable is penalized in a balanced manner even if the groups overlap. We present an efficient optimization method that can handle a large-scale problem. Using simulated and yeast datasets, we demonstrate that our method shows a superior performance in terms of both prediction errors and recovery of true sparsity patterns compared to other methods for multi-task learning.", "references": ["https://www.semanticscholar.org/paper/Consistency-of-the-group-Lasso-and-multiple-kernel-Bach/5214b65ed56efffd97493a59114a772dfb54caf1", "https://www.semanticscholar.org/paper/Grouped-and-Hierarchical-Model-Selection-through-Zhao-Rocha/b01c2d38e27614b657d77bc6512bc132b04baf5b", "https://www.semanticscholar.org/paper/Joint-covariate-selection-and-joint-subspace-for-Obozinski-Taskar/3f4f6a261f0536c3c9b4e023f34186897a7a4887", "https://www.semanticscholar.org/paper/High-Dimensional-Graphical-Model-Selection-Using-Wainwright-Ravikumar/a49a75990571ce0983bd045af8d2127bb6e3e9a8", "https://www.semanticscholar.org/paper/Y.%3A-SimpleMKL-Rakotomamonjy-Bach/bc39bdaf3145a2329ed4bebabfe4169a6b0ab51a", "https://www.semanticscholar.org/paper/Group-lasso-with-overlap-and-graph-lasso-Jacob-Obozinski/02c388d43f619146ef64babb4c848190e83add1b", "https://www.semanticscholar.org/paper/Structured-Variable-Selection-with-Norms-Jenatton-Audibert/910f8df95db7849036910e5773dcc9b09e70f03f", "https://www.semanticscholar.org/paper/Regression-Shrinkage-and-Selection-via-the-Lasso-Tibshirani/b365b8e45b7d81f081de44ac8f9eadf9144f3ca5", "https://www.semanticscholar.org/paper/Regularization-and-variable-selection-via-the-net-Zou-Hastie/c9f5723859a665c3e54b8d9a1a7eecb8b5084af6", "https://www.semanticscholar.org/paper/Model-selection-and-estimation-in-regression-with-Yuan-Lin/d98ef875e2cbde3e2cc8fad521e3cbfe1bddbd69"]},
{"id": "8f1c9b656157b1d851563fb42129245701d83175", "title": "Transforming Question Answering Datasets Into Natural Language Inference Datasets", "authors": ["Dorottya Demszky", "Kelvin Guu", "Percy Liang"], "date": "2018", "abstract": "Existing datasets for natural language inference (NLI) have propelled research on language understanding. We propose a new method for automatically deriving NLI datasets from the growing abundance of large-scale question answering datasets. Our approach hinges on learning a sentence transformation model which converts question-answer pairs into their declarative forms. Despite being primarily trained on a single QA dataset, we show that it can be successfully applied to a variety of other QA resources. Using this system, we automatically derive a new freely available dataset of over 500k NLI examples (QA-NLI), and show that it exhibits a wide range of inference phenomena rarely seen in previous NLI datasets.", "references": ["https://www.semanticscholar.org/paper/Towards-a-Unified-Natural-Language-Inference-to-Poliak-Haldar/c40665520563fb872b051bd27d3b43e042869ba4", "https://www.semanticscholar.org/paper/Generating-Natural-Language-Inference-Chains-Kolesnyk-Rockt%C3%A4schel/2dec6a802cbac1f640980b5106d88ae72c45ece4", "https://www.semanticscholar.org/paper/A-large-annotated-corpus-for-learning-natural-Bowman-Angeli/f04df4e20a18358ea2f689b4c129781628ef7fc1", "https://www.semanticscholar.org/paper/Annotation-Artifacts-in-Natural-Language-Inference-Gururangan-Swayamdipta/2997b26ffb8c291ce478bd8a6e47979d5a55c466", "https://www.semanticscholar.org/paper/Supervised-Learning-of-Universal-Sentence-from-Data-Conneau-Kiela/ee7b883e35d754ae4f71c21bb71f9f03e4ffbb2c", "https://www.semanticscholar.org/paper/SQuAD%3A-100%2C-000%2B-Questions-for-Machine-of-Text-Rajpurkar-Zhang/05dd7254b632376973f3a1b4d39485da17814df5", "https://www.semanticscholar.org/paper/A-Broad-Coverage-Challenge-Corpus-for-Sentence-Williams-Nangia/5ded2b8c64491b4a67f6d39ce473d4b9347a672e", "https://www.semanticscholar.org/paper/Question-Answer-Driven-Semantic-Role-Labeling%3A-to-He-Lewis/7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "https://www.semanticscholar.org/paper/Crowdsourcing-Question-Answer-Meaning-Michael-Stanovsky/1a210410493fbc052f0b7a54e7bc89cee20e8d28", "https://www.semanticscholar.org/paper/NewsQA%3A-A-Machine-Comprehension-Dataset-Trischler-Wang/3eda43078ae1f4741f09be08c4ecab6229046a5c"]},
{"id": "d1275b2a2ab53013310e759e5c6878b96df643d4", "title": "Context dependent recurrent neural network language model", "authors": ["Tomas Mikolov", "Geoffrey Zweig"], "date": "2012", "abstract": "Recurrent neural network language models (RNNLMs) have recently demonstrated state-of-the-art performance across a variety of tasks. In this paper, we improve their performance by providing a contextual real-valued input vector in association with each word. This vector is used to convey contextual information about the sentence being modeled. By performing Latent Dirichlet Allocation using a block of preceding text, we achieve a topic-conditioned RNNLM. This approach has the key advantage of avoiding the data fragmentation associated with building multiple topic models on different data subsets. We report perplexity results on the Penn Treebank data, where we achieve a new state-of-the-art. We further apply the model to the Wall Street Journal speech recognition task, where we observe improvements in word-error-rate.", "references": ["https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5", "https://www.semanticscholar.org/paper/Structured-Output-Layer-neural-network-language-Le-Oparin/3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "https://www.semanticscholar.org/paper/Cache-neural-network-language-models-based-on-for-a-Zamora-Mart%C3%ADnez-Boquera/e6f4381f25bdb5466f3ed7c2d7c66bd4349f746d", "https://www.semanticscholar.org/paper/Measuring-the-Influence-of-Long-Range-Dependencies-Le-Allauzen/0ce317d84086b8885ddbc7923ec00bedb64ab6dc", "https://www.semanticscholar.org/paper/Continuous-space-language-models-Schwenk/0fcc184b3b90405ec3ceafd6a4007c749df7c363", "https://www.semanticscholar.org/paper/Extensions-of-recurrent-neural-network-language-Mikolov-Kombrink/07ca885cb5cc4328895bfaec9ab752d5801b14cd", "https://www.semanticscholar.org/paper/Exact-training-of-a-neural-syntactic-language-model-Emami-Jelinek/9319ca5a532462f9f3515ac3d317668aa9650d5b", "https://www.semanticscholar.org/paper/Statistical-Language-Models-Based-on-Neural-U%C4%8Den%C3%AD-Brn%C4%9B/96364af2d208ea75ca3aeb71892d2f7ce7326b55", "https://www.semanticscholar.org/paper/Shrinking-Exponential-Language-Models-Chen/a1a4d2f3621ce7c56d7c396bb59f4eaf0e2b14fa", "https://www.semanticscholar.org/paper/Improving-arabic-broadcast-transcription-using-Chu-Mangu/3e7840a81072bcd1f36dae230b613f5f9af27eda"]},
{"id": "9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "title": "Efficient softmax approximation for GPUs", "authors": ["Edouard Grave", "Armand Joulin", "Moustapha Ciss\u00e9", "David Grangier", "Herv\u00e9 J\u00e9gou"], "date": "2017", "abstract": "We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computational complexity. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.", "references": ["https://www.semanticscholar.org/paper/BlackOut%3A-Speeding-up-Recurrent-Neural-Network-With-Ji-Vishwanathan/12a5b7190b981bf478b4c9c04d3c0d41f13b9023", "https://www.semanticscholar.org/paper/Strategies-for-Training-Large-Vocabulary-Neural-Chen-Grangier/759956bb98689dbcc891528636d8994e54318f85", "https://www.semanticscholar.org/paper/Efficient-Exact-Gradient-Update-for-training-Deep-Vincent-Br%C3%A9bisson/757f517f1952addc1716ea56f912f2e4a2803f7a", "https://www.semanticscholar.org/paper/Large%2C-Pruned-or-Continuous-Space-Language-Models-a-Schwenk-Rousseau/e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Language-Modeling-J%C3%B3zefowicz-Vinyals/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0", "https://www.semanticscholar.org/paper/Adaptive-Importance-Sampling-to-Accelerate-Training-Bengio-Senecal/699d5ab38deee78b1fd17cc8ad233c74196d16e9", "https://www.semanticscholar.org/paper/A-fast-and-simple-algorithm-for-training-neural-Mnih-Teh/5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "https://www.semanticscholar.org/paper/Strategies-for-training-large-scale-neural-network-Mikolov-Deoras/cb45e9217fe323fbc199d820e7735488fca2a9b3"]},
{"id": "4dc21a168070dc87266accd9ce2c06ee6115a285", "title": "? ? ? ? f ? ? ? ? ?", "authors": ["A ADoefaa", "Hans Peter Doetsch", "Draweng Table", "Hans Peter Doetsch"], "date": "2003", "abstract": "OF THE DISCLOSURE A gas Spring for a drawing table which has a cylinder, a piston in the cylinder, and a piston rod projecting from the piston through one end wall of the cylinder, the other end wall being imperforate. Gas under pressure fills the cylinder and may be released from the chamber adjacent the piston rod if the latter is almost fully expelled from the cylinder against a compression spring through a chan nel having orifices in the piston and in the piston rod. Additional gas is admitted to the cylinder of a modified Spring when the piston rod is almost fully retracted into the cylinder and the piston strikes a normally closed valve Separating the cylinder cavity from a storage chamber. SLSSSMSSSLSSSSTSSSTSS The present invention relates to a weight-compensating and equilibrium-maintaining device, and it is the principal object of this invention to provide a very simple and inexpensive device in the form of a so-called gas spring for compensating the weight or maintaining the equi librium of any device or apparatus which is adjustable to different levels or inclinations and for permitting such adjustments to be carried out with the least possible physi cal effort. Among the numerous types of devices and appa ratus to which the present invention may be applied may be mentioned especially: drawing tables, X-ray apparatus, hair driers, tilting doors and windows, covers for large freezer chests, and so forth. In connection with such devices or apparatus it is con ventionai to balance their weight or to maintain their equi librium by the provision of counterweights or coil springs. The employment of counterweights has the disadvantage that they require considerable space and also considerably increase the weight of the entire apparatus. The use of coil springs, on the other hand, has the disadvantage that the operations of compressing or expanding such springs require a considerable force and either require or result in considerable changes inforce which have to be compen sated by special mechanical means such as levers, cams, or other force-transmitting means which considerably in crease the cost of the respective apparatus. Another device which has previously been employed for the above-mentioned purposes is a so-called gas spring which consists of a pneumatic cylinder and piston unit in which the cylinder is filled with a pressure gas, for exam ple, compressed air. Although very successful when spe cially designed for a specific apparatus, these gas springs have the disadvantage that each of them has a very par ticular spring characteristic and that therefore a large number of different gas springs have to be produced and be held available for compensating the different forces of different devices or apparatus and even for compen sating differences in force which might be due to inac curacies of manufacture of an individual apparatus of a series thereof. When such gas springs are to be installed, for example, on a drawing table, it is evident that dif ferent gas springs would be required either for merely balancing the weight of the drawing board itself or for 0.", "references": []},
{"id": "2d7782c225e0fc123d6e227f2cb253e58279ac73", "title": "Improving Neural Language Models with a Continuous Cache", "authors": ["Edouard Grave", "Armand Joulin", "Nicolas Usunier"], "date": "2017", "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.", "references": ["https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Language-Modeling-J%C3%B3zefowicz-Vinyals/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "https://www.semanticscholar.org/paper/Pointer-Sentinel-Mixture-Models-Merity-Xiong/efbd381493bb9636f489b965a2034d529cd56bcd", "https://www.semanticscholar.org/paper/End-To-End-Memory-Networks-Sukhbaatar-Szlam/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "https://www.semanticscholar.org/paper/Empirical-Evaluation-and-Combination-of-Advanced-Mikolov-Deoras/77dfe038a9bdab27c4505444931eaa976e9ec667", "https://www.semanticscholar.org/paper/Context-dependent-recurrent-neural-network-language-Mikolov-Zweig/d1275b2a2ab53013310e759e5c6878b96df643d4", "https://www.semanticscholar.org/paper/Efficient-softmax-approximation-for-GPUs-Grave-Joulin/9ec499af9b85f30bdbdd6cdfbb07d484808c526a", "https://www.semanticscholar.org/paper/Inferring-Algorithmic-Patterns-with-Stack-Augmented-Joulin-Mikolov/d38e8631bba0720becdaf7b89f79d9f9dca45d82", "https://www.semanticscholar.org/paper/Learning-Longer-Memory-in-Recurrent-Neural-Networks-Mikolov-Joulin/9665247ea3421929f9b6ad721f139f11edb1dbb8", "https://www.semanticscholar.org/paper/A-Cache-Based-Natural-Language-Model-for-Speech-Kuhn-Mori/be1fed9544830df1137e72b1d2396c40d3e18365", "https://www.semanticscholar.org/paper/Learning-to-Transduce-with-Unbounded-Memory-Grefenstette-Hermann/e837b79de602c69395498c1fbbe39bbb4e6f75ad"]},
{"id": "5ef2edfe51d3d768b1d89ad7e74e4ce8e55d1d49", "title": "Delta TFIDF: An Improved Feature Space for Sentiment Analysis", "authors": ["Justin Martineau", "Timothy W. Finin"], "date": "2009", "abstract": "Mining opinions and sentiment from social networking sites is a popular application for social media systems. Common approaches use a machine learning system with a bag of words feature set. We present Delta TFIDF, an intuitive general purpose technique to efficiently weight word scores before classification. Delta TFIDF is easy to compute, implement, and understand. We use Support Vector Machines to show that Delta TFIDF significantly improves accuracy for sentiment analysis problems using three well known data sets.", "references": ["https://www.semanticscholar.org/paper/Thumbs-up-Sentiment-Classification-using-Machine-Pang-Lee/12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "https://www.semanticscholar.org/paper/A-Sentimental-Education%3A-Sentiment-Analysis-Using-Pang-Lee/167e1359943b96b9e92ee73db1df69a1f65d731d", "https://www.semanticscholar.org/paper/Using-appraisal-groups-for-sentiment-analysis-Whitelaw-Garg/0470303953de1d19765423f719d8314e3cb91278", "https://www.semanticscholar.org/paper/Using-%22Annotator-Rationales%22-to-Improve-Machine-for-Zaidan-Eisner/99fe982dce046869f60e4552c7f91c3627304780", "https://www.semanticscholar.org/paper/Text-Categorization-with-Support-Vector-Machines.-Leopold-Kindermann/88151be6a62a5843d7ff0b131452f8b3c6491cb2", "https://www.semanticscholar.org/paper/Text-Categorization-with-Support-Vector-Machines%3A-Joachims/40212e9474c3ddf3d8c6ffd13dd3211ec9406c49", "https://www.semanticscholar.org/paper/Automatically-Assessing-Review-Helpfulness-Kim-Pantel/4492611e06d61d127d8b51d70d1be654a109209d", "https://www.semanticscholar.org/paper/Making-large-scale-support-vector-machine-learning-Joachims/bb99668d4df98a3f6ff0b9fa3402e09008f22e2c", "https://www.semanticscholar.org/paper/Get-out-the-vote%3A-Determining-support-or-opposition-Thomas-Pang/0c954bc14181c7b140e6727738ebb7a596754de9", "https://www.semanticscholar.org/paper/Text-categorization-with-support-vector-machines-Edda-Kindermann/79788370cea1ca0b046f5c16dbddc851a63ad208"]},
{"id": "78821ed420ead11c47f72dbbac7f67e17ecf3a58", "title": "Recognizing Textual Entailment Using Probabilistic Inference", "authors": ["Lei Sha", "Sujian Li", "Baobao Chang", "Zhifang Sui", "Tingsong Jiang"], "date": "2015", "abstract": "Recognizing Text Entailment (RTE) plays an important role in NLP applications including question answering, information retrieval, etc. In recent work, some research explore \u201cdeep\u201d expressions such as discourse commitments or strict logic for representing the text. However, these expressions suffer from the limitation of inference inconvenience or translation loss. To overcome the limitations, in this paper, we propose to use the predicate-argument structures to represent the discourse commitments extracted from text. At the same time, with the help of the YAGO knowledge, we borrow the distant supervision technique to mine the implicit facts from the text. We also construct a probabilistic network for all the facts and conduct inference to judge the confidence of each fact for RTE. The experimental results show that our proposed method achieves a competitive result compared to the previous work.", "references": ["https://www.semanticscholar.org/paper/Learning-to-recognize-features-of-valid-textual-MacCartney-Grenager/757a192157d7587433db93a899d32c4c5e832489", "https://www.semanticscholar.org/paper/Robust-Textual-Inference-via-Graph-Matching-Haghighi-Ng/131eb9c57d842e55be86f311c6d47f735b24e443", "https://www.semanticscholar.org/paper/Statistical-Relational-Learning-to-Recognise-Rios-Specia/b40505c8cd8a49639fa009ca26d3a1060b64bc90", "https://www.semanticscholar.org/paper/Using-Discourse-Commitments-to-Recognize-Textual-Hickl/57c2a6907a80f2a466870d5ab4abf7ff1f484dbd", "https://www.semanticscholar.org/paper/An-Inference-Model-for-Semantic-Entailment-in-Braz-Girju/41e07d21451df21dacda2fea6f90b53bf4b89b27", "https://www.semanticscholar.org/paper/Syntactic-Contributions-in-the-Entailment-Task%3A-an-Vanderwende-Menezes/ef282b9935dda4fa45f4f32cbc1793aec599eb0d", "https://www.semanticscholar.org/paper/A-Logic-based-Approach-for-Recognizing-Textual-by-Wotzlaw-Coote/cf6508043c418891c1e7299debccfc1527e4ca2a", "https://www.semanticscholar.org/paper/Recognizing-Textual-Entailment-Using-Lexical-Jijkoun-Rijke/318e9eb50bb879f01efb7b4ed0482c8e458b6e8c", "https://www.semanticscholar.org/paper/Distant-supervision-for-relation-extraction-without-Mintz-Bills/d84b57362e2010f6f65357267df7e0157af30684", "https://www.semanticscholar.org/paper/DIRT-%40SBT%40discovery-of-inference-rules-from-text-Lin-Pantel/f3f9a848dca0a80ef64987a9dd511ee6b7e19cd1"]},
{"id": "f314339651cb25e4234e0b96fe8bd87206847993", "title": "Iterative Alternating Neural Attention for Machine Reading", "authors": ["Alessandro Sordoni", "Philip Bachman", "Yoshua Bengio"], "date": "2016", "abstract": "We propose a novel neural attention architecture to tackle machine comprehension tasks, such as answering Cloze-style queries with respect to a document. Unlike previous models, we do not collapse the query into a single vector, instead we deploy an iterative alternating attention mechanism that allows a fine-grained exploration of both the query and the document. Our model outperforms state-of-the-art baselines in standard machine comprehension benchmarks such as CNN news articles and the Children\u2019s Book Test (CBT) dataset.", "references": ["https://www.semanticscholar.org/paper/Text-Understanding-with-the-Attention-Sum-Reader-Kadlec-Schmid/f2e50e2ee4021f199877c8920f1f984481c723aa", "https://www.semanticscholar.org/paper/Teaching-Machines-to-Read-and-Comprehend-Hermann-Kocisk%C3%BD/d1505c6123c102e53eb19dff312cb25cea840b72", "https://www.semanticscholar.org/paper/Show%2C-Attend-and-Tell%3A-Neural-Image-Caption-with-Xu-Ba/4d8f2d14af5991d4f0d050d22216825cac3157bd", "https://www.semanticscholar.org/paper/A-Thorough-Examination-of-the-CNN%2FDaily-Mail-Task-Chen-Bolton/b1e20420982a4f923c08652941666b189b11b7fe", "https://www.semanticscholar.org/paper/Effective-Approaches-to-Attention-based-Neural-Luong-Pham/93499a7c7f699b6630a86fad964536f9423bb6d0", "https://www.semanticscholar.org/paper/End-To-End-Memory-Networks-Sukhbaatar-Szlam/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "https://www.semanticscholar.org/paper/Ask-Me-Anything%3A-Dynamic-Memory-Networks-for-Kumar-Irsoy/452059171226626718eb677358836328f884298e", "https://www.semanticscholar.org/paper/The-Goldilocks-Principle%3A-Reading-Children's-Books-Hill-Bordes/35b91b365ceb016fb3e022577cec96fb9b445dc5", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Pointer-Networks-Vinyals-Fortunato/9653d5c2c7844347343d073bbedd96e05d52f69b"]},
{"id": "05dd7254b632376973f3a1b4d39485da17814df5", "title": "SQuAD: 100, 000+ Questions for Machine Comprehension of Text", "authors": ["Pranav Rajpurkar", "Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "date": "2016", "abstract": "We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. \nThe dataset is freely available at this https URL", "references": ["https://www.semanticscholar.org/paper/Machine-Comprehension-Using-Match-LSTM-and-Answer-Wang-Jiang/ff1861b71eaedba46cb679bbe2c585dbe18f9b19", "https://www.semanticscholar.org/paper/Learning-Answer-Entailing-Structures-for-Machine-Sachan-Dubey/f26e088bc4659a9b7fce28b6604d26de779bcf93", "https://www.semanticscholar.org/paper/MCTest%3A-A-Challenge-Dataset-for-the-Open-Domain-of-Richardson-Burges/564257469fa44cdb57e4272f85253efb9acfd69d", "https://www.semanticscholar.org/paper/WikiQA%3A-A-Challenge-Dataset-for-Open-Domain-Yang-Yih/f53e2ae46470b89cd1ce6e3bf1d60d9c59722ce1", "https://www.semanticscholar.org/paper/Towards-AI-Complete-Question-Answering%3A-A-Set-of-Weston-Bordes/abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "https://www.semanticscholar.org/paper/A-Thorough-Examination-of-the-CNN%2FDaily-Mail-Task-Chen-Bolton/b1e20420982a4f923c08652941666b189b11b7fe", "https://www.semanticscholar.org/paper/Modeling-Biological-Processes-for-Reading-Berant-Srikumar/6396ab37641d36be4c26420e58adeb8665914c3b", "https://www.semanticscholar.org/paper/Deep-Read%3A-A-Reading-Comprehension-System-Hirschman-Light/5ddde06f993d633a5f9750bc1c3301eb7f13e82f", "https://www.semanticscholar.org/paper/Learning-surface-text-patterns-for-a-Question-Ravichandran-Hovy/fbd0e0ad4e06902b10b6a157b9db92df577720f1", "https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Answering-Questions-Ng-Teo/27ab2cbb3b26a431d5ede5be34e9ee9a464d144c"]},
{"id": "0341b7b5195b4a5a3636f242a2ab4eb6f480ac47", "title": "Hierarchical training of neural networks and prediction of chaotic time series", "authors": ["Josef Deppisch", "Hans-Ulrich Bauer", "Theo Geisel"], "date": "1991", "abstract": "Abstract We present a new procedure for hierarchical training of multilayer perceptrons to outputs of high precision. It achieves a dramatic increase in accuracy, e.g. by three orders of magnitude, and can reduce training time considerably. The method is applied to the prediction of chaotic systems where we obtain the optimum error evolution for iterated predictions as well as a substantial reduction of the absolute prediction error.", "references": []},
{"id": "46b8cbcdff87b842c2c1d4a003c831f845096ba7", "title": "Order-Embeddings of Images and Language", "authors": ["Ivan Vendrov", "Ryan Kiros", "Sanja Fidler", "Raquel Urtasun"], "date": "2015", "abstract": "Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.", "references": ["https://www.semanticscholar.org/paper/Framing-Image-Description-as-a-Ranking-Task%3A-Data%2C-Hodosh-Young/9814df8bd00ba999c4d1e305a7e9bca579dc7c75", "https://www.semanticscholar.org/paper/Grounded-Compositional-Semantics-for-Finding-and-Socher-Karpathy/0ca7d208ff8d81377e0eaa9723820aeae7a7322d", "https://www.semanticscholar.org/paper/Word-Representations-via-Gaussian-Embedding-Vilnis-McCallum/233bc1bbdf5c4c08da204f545b1eaf15876ea786", "https://www.semanticscholar.org/paper/From-image-descriptions-to-visual-denotations%3A-New-Young-Lai/44040913380206991b1991daf1192942e038fe31", "https://www.semanticscholar.org/paper/Unifying-Visual-Semantic-Embeddings-with-Multimodal-Kiros-Salakhutdinov/2e36ea91a3c8fbff92be2989325531b4002e2afc", "https://www.semanticscholar.org/paper/Deep-Visual-Semantic-Alignments-for-Generating-Karpathy-Fei-Fei/55e022fb7581bb9e1fce678d21fb25ffbb3fbb88", "https://www.semanticscholar.org/paper/Show-and-tell%3A-A-neural-image-caption-generator-Vinyals-Toshev/d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0", "https://www.semanticscholar.org/paper/Visual-Semantic-Search%3A-Retrieving-Videos-via-Lin-Fidler/7afd833f484c8032e7fdc5f53188d2ebb0fb9934", "https://www.semanticscholar.org/paper/Flickr30k-Entities%3A-Collecting-Region-to-Phrase-for-Plummer-Wang/0612745dbd292fc0a548a16d39cd73e127faedde", "https://www.semanticscholar.org/paper/A-large-annotated-corpus-for-learning-natural-Bowman-Angeli/f04df4e20a18358ea2f689b4c129781628ef7fc1"]},
{"id": "f96898d15a1bf1fa8925b1280d0e07a7a8e72194", "title": "Dynamic Memory Networks for Visual and Textual Question Answering", "authors": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "date": "2016", "abstract": "Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \\babi-10k text question-answering dataset without supporting fact supervision.", "references": ["https://www.semanticscholar.org/paper/Ask%2C-Attend-and-Answer%3A-Exploring-Question-Guided-Xu-Saenko/1cf6bc0866226c1f8e282463adc8b75d92fba9bb", "https://www.semanticscholar.org/paper/Stacked-Attention-Networks-for-Image-Question-Yang-He/2c1890864c1c2b750f48316dc8b650ba4772adc5", "https://www.semanticscholar.org/paper/Ask-Me-Anything%3A-Dynamic-Memory-Networks-for-Kumar-Irsoy/452059171226626718eb677358836328f884298e", "https://www.semanticscholar.org/paper/Memory-Networks-Weston-Chopra/71ae756c75ac89e2d731c9c79649562b5768ff39", "https://www.semanticscholar.org/paper/Learning-to-Compose-Neural-Networks-for-Question-Andreas-Rohrbach/75ddc7ee15be14013a3462c01b38b0548486fbcb", "https://www.semanticscholar.org/paper/Ask-Me-Anything%3A-Free-Form-Visual-Question-Based-on-Wu-Wang/20dbdf02497aa84510970d0f5e8b599073bca1bc", "https://www.semanticscholar.org/paper/Ask-Your-Neurons%3A-A-Neural-Based-Approach-to-about-Malinowski-Rohrbach/bd7bd1d2945a58cdcc1797ba9698b8810fe68f60", "https://www.semanticscholar.org/paper/End-To-End-Memory-Networks-Sukhbaatar-Szlam/4f10b9f47c5bb6b54dd4f5ca8d9fa2c0bbd7ec5e", "https://www.semanticscholar.org/paper/Learning-to-Answer-Questions-from-Image-Using-Ma-Lu/98bd5dd1740f585bf25320ba504e2c1ae57f2e5f", "https://www.semanticscholar.org/paper/A-Neural-Network-for-Factoid-Question-Answering-Iyyer-Boyd-Graber/af44f5db5b4396e1670cda07eff5ad84145ba843"]},
{"id": "56f4906d8fcc4097299d9644ee54fa4534b8e594", "title": "An integrated network for invariant visual detection and recognition", "authors": ["Yali Amit", "Massimo Mascaro"], "date": "2003", "abstract": "We describe an architecture for invariant visual detection and recognition. Learning is performed in a single central module. The architecture makes use of a replica module consisting of copies of retinotopic layers of local features, with a particular design of inputs and outputs, that allows them to be primed either to attend to a particular location, or to attend to a particular object representation. In the former case the data at a selected location can be classified in the central module. In the latter case all instances of the selected object are detected in the field of view. The architecture is used to explain a number of psychophysical and physiological observations: object based attention, the different response time slopes of target detection among distractors, and observed attentional modulation of neuronal responses. We hypothesize that the organization of visual cortex in columns of neurons responding to the same feature at the same location may provide the copying architecture needed for translation invariance.", "references": ["https://www.semanticscholar.org/paper/A-neural-basis-for-visual-search-in-inferior-cortex-Chelazzi-Miller/86876c68d05af50330d07ee10790ddcf6578c7d9", "https://www.semanticscholar.org/paper/A-Neural-Network-Architecture-for-Visual-Selection-Amit/62ccf3020ac41088b39c3982d119752b0ba6b261", "https://www.semanticscholar.org/paper/Visual-categorization-shapes-feature-selectivity-in-Sigala-Logothetis/0bbd52aaddd00bc863c93d7b106db658c5c29402", "https://www.semanticscholar.org/paper/Coding-visual-images-of-objects-in-the-cortex-of-Tanaka-Saito/5f257b61f63468af774cc833cc6982768c315b37", "https://www.semanticscholar.org/paper/A-neurobiological-model-of-visual-attention-and-on-Olshausen-Anderson/700bbcd3518ca8cb3dac50a89fc69cad3dc1a579", "https://www.semanticscholar.org/paper/Columns-for-visual-features-of-objects-in-monkey-Fujita-Tanaka/083630744dbf60994867cbd776bfe601b4d0dbe6", "https://www.semanticscholar.org/paper/fMRI-evidence-for-objects-as-the-units-of-selection-O'Craven-Downing/af4ae6e58b658ea5ca51780aabd2c31af7f9b860", "https://www.semanticscholar.org/paper/A-Computational-Model-for-Visual-Selection-Amit-Geman/61b2d0383b186c4d634c5f51421cab67c16d90a1", "https://www.semanticscholar.org/paper/Neural-mechanisms-of-selective-visual-attention.-Desimone-Duncan/5d2ef51c912df93d31314d6827a98bc474374105", "https://www.semanticscholar.org/paper/Selective-attention-gates-visual-processing-in-the-Moran-Desimone/fe1f6a674cd1b7bb612b145bbb62eed61fbc9833"]},
{"id": "26208cdcf49948cddb6438fc1c70c26b4a8b4926", "title": "Modeling out-of-vocabulary words for robust speech recognition", "authors": ["Issam Bazzi", "James R. Glass"], "date": "2000", "abstract": "In this paper we present an approach for modeling and recognizing out-of-vocabulary (OOV) words in a single stage recognizer. A word-based recognizer is augmented with an extra OOV word model, which enables the OOV word to be predicted by a word-based language model. The OOV model itself is phone-based, so that an OOV word can be realized as an arbitrary sequence of phones. A phone bigram is used to provide phonotactic constraints within the OOV model. A recognizer with this configuration can recognize words in the original vocabulary as well as any potential new words of arbitrary pronunciation. In our preliminary investigation of this framework, we have evaluated the recognizer on a weather information domain with one test set containing only in-vocabulary (IV) data, and another containing OOV words. On the IV test set, the recognizer had an OOV insertion rate of only 1.3%, and degraded the baseline WER from 10.4% to 10.7%. On the OOV test set, the recognizer was able to detect nearly half of the OOV words (47% detection rate).", "references": []},
{"id": "db34d106ea2d3403a3d9ccf22cc80d079772d7c8", "title": "Linear spatial pyramid matching using sparse coding for image classification", "authors": ["Jianchao Yang", "Kai Ping Yu", "Yihong Gong", "Thomas S. Huang"], "date": "2009", "abstract": "Recently SVMs using spatial pyramid matching (SPM) kernel have been highly successful in image classification. Despite its popularity, these nonlinear SVMs have a complexity O(n2 ~ n3) in training and O(n) in testing, where n is the training size, implying that it is nontrivial to scaleup the algorithms to handle more than thousands of training images. In this paper we develop an extension of the SPM method, by generalizing vector quantization to sparse coding followed by multi-scale spatial max pooling, and propose a linear SPM kernel based on SIFT sparse codes. This new approach remarkably reduces the complexity of SVMs to O(n) in training and a constant in testing. In a number of image categorization experiments, we find that, in terms of classification accuracy, the suggested linear SPM based on sparse coding of SIFT descriptors always significantly outperforms the linear SPM kernel on histograms, and is even better than the nonlinear SPM kernels, leading to state-of-the-art performance on several benchmarks by using a single type of descriptors.", "references": []},
{"id": "2f42a55da3a222184eee20c67d374a9134b77bdc", "title": "Discovering Structure in Multiple Learning Tasks: The TC Algorithm", "authors": ["Sebastian Thrun", "Joseph O'Sullivan"], "date": "1996", "abstract": "Recently, there has been an increased interest in \u201clifelong\u201d machine learning methods, that transfer knowledge across multiple learning tasks. Such methods have repeatedly been found to outperform conventional, single-task learning algorithms when the learning tasks are appropriately related. To increase robustness of such approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading. This paper describes the task-clustering (TC) algorithm. TC clusters learning tasks into classes of mutually related tasks. When facing a new learning task, TC first determines the most related task cluster, then exploits information selectively from this task cluster only. An empirical study carried out in a mobile robot domain shows that TC outperforms its non-selective counterpart in situations where only a small number of tasks is relevant.", "references": ["https://www.semanticscholar.org/paper/Clustering-Learning-Tasks-and-the-Selective-of-Thrun-O'Sullivan/c92689ab1e100998231e8c8e35da21817f646f47", "https://www.semanticscholar.org/paper/Multitask-Learning%3A-A-Knowledge-Based-Source-of-Caruana/9464d15f4f8d578f93332db4aa1c9c182fd51735", "https://www.semanticscholar.org/paper/Explanation-based-neural-network-learning-a-Thrun/75e50717070e82cdf3945265a75def6960b55a9d", "https://www.semanticscholar.org/paper/Recursive-automatic-algorithm-selection-for-Brodley/76c08d739988979653a89c9fc072339d9918e0e4", "https://www.semanticscholar.org/paper/Is-Learning-The-n-th-Thing-Any-Easier-Than-Learning-Thrun/371c9dc680e916f79d9c78fcf6c894a2dd299095", "https://www.semanticscholar.org/paper/Transferring-previously-learned-back-propagation-to-Pratt/623487c9a81d5483df2d89d1d9941138fe2fb67e", "https://www.semanticscholar.org/paper/Layered-Concept-Learning-and-Dynamically-Variable-Rendell-Sheshu/d24ffc3e48cab4b9271981ccf82bbd1838361daf", "https://www.semanticscholar.org/paper/Learning-One-More-Thing-Thrun-Mitchell/2b65b99e772727dadc1b5e50a9d83367a892ec28", "https://www.semanticscholar.org/paper/Learning-internal-representations-Baxter/a24508e65e599b5b20c33af96dbe7017d5caca37", "https://www.semanticscholar.org/paper/A-Method-for-Learning-From-Hints-Abu-Mostafa/b79a72a341c93f7e69b4d485dc75c6161331f0d2"]},
{"id": "e996344199af190bdd47ee31a652e76811a5487b", "title": "Variational inference for Dirichlet process mixtures", "authors": ["Michael I. Jordan", "Michael I. Jordan"], "date": "2006", "abstract": "Dirichlet process (DP) mixture models are the cornerstone of non- parametric Bayesian statistics, and the development of Monte-Carlo Markov chain (MCMC) sampling methods for DP mixtures has enabled the application of non- parametric Bayesian methods to a variety of practical data analysis problems. However, MCMC sampling can be prohibitively slow, and it is important to ex- plore alternatives. One class of alternatives is provided by variational methods, a class of deterministic algorithms that convert inference problems into optimization problems (Opper and Saad 2001; Wainwright and Jordan 2003). Thus far, varia- tional methods have mainly been explored in the parametric setting, in particular within the formalism of the exponential family (Attias 2000; Ghahramani and Beal 2001; Blei et al. 2003). In this paper, we present a variational inference algorithm for DP mixtures. We present experiments that compare the algorithm to Gibbs sampling algorithms for DP mixtures of Gaussians and present an application to a large-scale image analysis problem.", "references": ["https://www.semanticscholar.org/paper/Gibbs-Sampling-Methods-for-Stick-Breaking-Priors-Ishwaran-James/45db16309fbf7e9fc8907047e7e1a9933c4e1b85", "https://www.semanticscholar.org/paper/Bayesian-Density-Estimation-and-Inference-Using-Escobar-West/df25adb36860c1ad9edaac04b8855a2f19e79c5b", "https://www.semanticscholar.org/paper/Mixtures-of-Dirichlet-Processes-with-Applications-Antoniak/263f103fd2bfbbd6aeb392c6519d3f590e647c0a", "https://www.semanticscholar.org/paper/Propagation-Algorithms-for-Variational-Bayesian-Ghahramani-Beal/e5f22d558526017f130c75ca35fe0a737c01aaee", "https://www.semanticscholar.org/paper/A-generalized-mean-field-algorithm-for-variational-Xing-Jordan/434bc6e9b1aed420f16e279467a551b0df70f20f", "https://www.semanticscholar.org/paper/A-CONSTRUCTIVE-DEFINITION-OF-DIRICHLET-PRIORS-Sethuramant/e1ba33f56f75c40aed15538073e377c23f4013da", "https://www.semanticscholar.org/paper/Estimating-normal-means-with-a-conjugate-style-MacEachern/f4e973abdf190b862e6ec4ff1791d1ed8a628705", "https://www.semanticscholar.org/paper/A-Variational-Baysian-Framework-for-Graphical-Attias/8c8ee0bc40d8158bba48df860622649a12cabd49", "https://www.semanticscholar.org/paper/A-Bayesian-Analysis-of-Some-Nonparametric-Problems-Ferguson/b8593122c6b1dd2432125a4eeb1a908c280b1d1d", "https://www.semanticscholar.org/paper/Ferguson-distributions-via-Polya-urn-schemes-Blackwell-MacQueen/1b32d466d2c756e1dbcef9ff94e1be895e84700d"]},
{"id": "7daf69424feafdce1c896ff19f9a08a5b31ad5d8", "title": "Question-Answer Driven Semantic Role Labeling: Using Natural Language to Annotate Natural Language", "authors": ["Luheng He", "Mike Lewis", "Luke Zettlemoyer"], "date": "2015", "abstract": "This paper introduces the task of questionanswer driven semantic role labeling (QA-SRL), where question-answer pairs are used to represent predicate-argument structure. For example, the verb \u201cintroduce\u201d in the previous sentence would be labeled with the questions \u201cWhat is introduced?\u201d, and \u201cWhat introduces something?\u201d, each paired with the phrase from the sentence that gives the correct answer. Posing the problem this way allows the questions themselves to define the set of possible roles, without the need for predefined frame or thematic role ontologies. It also allows for scalable data collection by annotators with very little training and no linguistic expertise. We gather data in two domains, newswire text and Wikipedia articles, and introduce simple classifierbased models for predicting which questions to ask and what their answers should be. Our results show that non-expert annotators can produce high quality QA-SRL data, and also establish baseline performance levels for future work on this task.", "references": ["https://www.semanticscholar.org/paper/Developing-a-large-semantically-annotated-corpus-Basile-Bos/1b18c2803d59e9c7d36618fcd44d15a9f4e16379", "https://www.semanticscholar.org/paper/The-Proposition-Bank%3A-An-Annotated-Corpus-of-Roles-Palmer-Kingsbury/99d2dcdcf4cf05facaa101a48c7e31d140b4736d", "https://www.semanticscholar.org/paper/Dependency-based-Semantic-Role-Labeling-of-PropBank-Johansson-Nugues/2791e0d36ba23763195ac984453d61dbaff555da", "https://www.semanticscholar.org/paper/Semantic-Role-Labeling-Santos-Milidi%C3%BA/c4100faa2cc35bc72e61dcbb173f1fee5e8e8840", "https://www.semanticscholar.org/paper/Recognizing-Implied-Predicate-Argument-in-Textual-Stern-Dagan/b29cdc426784534ad5c83ccf646fde67c4695607", "https://www.semanticscholar.org/paper/A-Bayesian-Approach-to-Unsupervised-Semantic-Role-Titov-Klementiev/1de2f2d45e505eab0bc520ee554353c6d509f32a", "https://www.semanticscholar.org/paper/Calibrating-Features-for-Semantic-Role-Labeling-Xue-Palmer/80c6abf049ff5e280c57d4d28d23f3acdab80ddb", "https://www.semanticscholar.org/paper/Joint-A*-CCG-Parsing-and-Semantic-Role-Labelling-Lewis-He/5894c9fbe9d14be08a48e34d0467da4213b6399c", "https://www.semanticscholar.org/paper/The-CoNLL-2009-Shared-Task%3A-Syntactic-and-Semantic-Hajic-Ciaramita/c7d3f610b528226f1c862c4f9cd6b37623f7390f", "https://www.semanticscholar.org/paper/Semantic-Role-Features-for-Machine-Translation-Liu-Gildea/b1d7f596fc34fd6cc6bbfc22a083bca8d2d38f14"]},
{"id": "757f517f1952addc1716ea56f912f2e4a2803f7a", "title": "Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets", "authors": ["Pascal Vincent", "Alexandre de Br\u00e9bisson", "Xavier Bouthillier"], "date": "2015", "abstract": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D x d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d^2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of D/4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "references": ["https://www.semanticscholar.org/paper/Large-Scale-Learning-of-Embeddings-with-Sampling-Dauphin-Glorot/aa1762a629b31d254450e37ce8baa235d729d82b", "https://www.semanticscholar.org/paper/Deep-Networks-With-Large-Output-Spaces-Vijayanarasimhan-Shlens/8250ecbaef057bdb5390ef4e4be798f1523a23f6", "https://www.semanticscholar.org/paper/Learning-word-embeddings-efficiently-with-Mnih-Kavukcuoglu/53ca064b9f1b92951c1997e90b776e95b0880e52", "https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a", "https://www.semanticscholar.org/paper/Riemannian-metrics-for-neural-networks-Ollivier/b83c32b75ded6fd45062b7cdaf8794a85ea36264", "https://www.semanticscholar.org/paper/On-Using-Very-Large-Target-Vocabulary-for-Neural-Jean-Cho/1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "https://www.semanticscholar.org/paper/Distributed-Representations-of-Words-and-Phrases-Mikolov-Sutskever/87f40e6f3022adbc1f1905e3e506abad05a9964f", "https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769", "https://www.semanticscholar.org/paper/Efficient-Estimation-of-Word-Representations-in-Mikolov-Chen/330da625c15427c6e42ccfa3b747fb29e5835bf0", "https://www.semanticscholar.org/paper/One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov/5d833331b0e22ff359db05c62a8bca18c4f04b68"]},
{"id": "e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "title": "Large, Pruned or Continuous Space Language Models on a GPU for Statistical Machine Translation", "authors": ["Holger Schwenk", "Anthony Rousseau", "Mohammed Attik"], "date": "2012", "abstract": "Language models play an important role in large vocabulary speech recognition and statistical machine translation systems. The dominant approach since several decades are back-off language models. Some years ago, there was a clear tendency to build huge language models trained on hundreds of billions of words. Lately, this tendency has changed and recent works concentrate on data selection. Continuous space methods are a very competitive approach, but they have a high computational complexity and are not yet in widespread use. This paper presents an experimental comparison of all these approaches on a large statistical machine translation task. We also describe an open-source implementation to train and use continuous space language models (CSLM) for such large tasks. We describe an efficient implementation of the CSLM using graphical processing units from Nvidia. By these means, we are able to train an CSLM on more than 500 million words in 20 hours. This CSLM provides an improvement of up to 1.8 BLEU points with respect to the best back-off language model that we were able to build.", "references": ["https://www.semanticscholar.org/paper/Efficient-Handling-of-N-gram-Language-Models-for-Federico-Cettolo/f76b28565a0c677f14f802dca10d8d0db09a6cc3", "https://www.semanticscholar.org/paper/Training-Continuous-Space-Language-Models%3A-Some-Le-Allauzen/47e3d8a1f8e92923e739ca34bea17004a40514e9", "https://www.semanticscholar.org/paper/Continuous-Space-Language-Models-for-Statistical-Schwenk/d4a258df43cc14e46988de9a4a7b2f0ea817529b", "https://www.semanticscholar.org/paper/Continuous-space-language-models-Schwenk/0fcc184b3b90405ec3ceafd6a4007c749df7c363", "https://www.semanticscholar.org/paper/Continuous-Space-Language-Models-for-Statistical-Schwenk/b617561fb384186b2647fd3e52b86442816a0e18", "https://www.semanticscholar.org/paper/Efficient-training-of-large-neural-networks-for-Schwenk/d6fb7546a29320eadad868af66835059db93d99f", "https://www.semanticscholar.org/paper/Connectionist-language-modeling-for-large-speech-Schwenk-Gauvain/e41498c05d4c68e4750fb84a380317a112d97b01", "https://www.semanticscholar.org/paper/Large-Language-Models-in-Machine-Translation-Brants-Popat/ba786c46373892554b98df42df7af6f5da343c9d", "https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a", "https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb"]},
{"id": "2dec6a802cbac1f640980b5106d88ae72c45ece4", "title": "Generating Natural Language Inference Chains", "authors": ["Vladyslav Kolesnyk", "Tim Rockt\u00e4schel", "Sebastian Riedel"], "date": "2016", "abstract": "The ability to reason with natural language is a fundamental prerequisite for many NLP tasks such as information extraction, machine translation and question answering. To quantify this ability, systems are commonly tested whether they can recognize textual entailment, i.e., whether one sentence can be inferred from another one. However, in most NLP applications only single source sentences instead of sentence pairs are available. Hence, we propose a new task that measures how well a model can generate an entailed sentence from a source sentence. We take entailment-pairs of the Stanford Natural Language Inference corpus and train an LSTM with attention. On a manually annotated test set we found that 82% of generated sentences are correct, an improvement of 10.3% over an LSTM baseline. A qualitative analysis shows that this model is not only capable of shortening input sentences, but also inferring new statements via paraphrasing and phrase entailment. We then apply this model recursively to input-output pairs, thereby generating natural language inference chains that can be used to automatically construct an entailment graph from source sentences. Finally, by swapping source and target sentences we can also train a model that given an input sentence invents additional information to generate a new sentence.", "references": ["https://www.semanticscholar.org/paper/A-large-annotated-corpus-for-learning-natural-Bowman-Angeli/f04df4e20a18358ea2f689b4c129781628ef7fc1", "https://www.semanticscholar.org/paper/Learning-Natural-Language-Inference-with-LSTM-Wang-Jiang/596c882de006e4bb4a93f1fa08a5dd467bee060a", "https://www.semanticscholar.org/paper/Reasoning-about-Entailment-with-Neural-Attention-Rockt%C3%A4schel-Grefenstette/2846e83d405cbe3bf2f0f3b5f635dd8b3c680c45", "https://www.semanticscholar.org/paper/Towards-AI-Complete-Question-Answering%3A-A-Set-of-Weston-Bordes/abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "https://www.semanticscholar.org/paper/A-Fast-Unified-Model-for-Parsing-and-Sentence-Bowman-Gauthier/36c097a225a95735271960e2b63a2cb9e98bff83", "https://www.semanticscholar.org/paper/Grammar-as-a-Foreign-Language-Vinyals-Kaiser/47570e7f63e296f224a0e7f9a0d08b0de3cbaf40", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/A-Neural-Attention-Model-for-Abstractive-Sentence-Rush-Chopra/5082a1a13daea5c7026706738f8528391a1e6d59", "https://www.semanticscholar.org/paper/Teaching-Machines-to-Read-and-Comprehend-Hermann-Kocisk%C3%BD/d1505c6123c102e53eb19dff312cb25cea840b72"]},
{"id": "0c954bc14181c7b140e6727738ebb7a596754de9", "title": "Get out the vote: Determining support or opposition from Congressional floor-debate transcripts", "authors": ["Matt Thomas", "Bo Pang", "Lillian Lee"], "date": "2006", "abstract": "We investigate whether one can determine from the transcripts of U.S. Congressional floor debates whether the speeches represent support of or opposition to proposed legislation. To address this problem, we exploit the fact that these speeches occur as part of a discussion; this allows us to use sources of information regarding relationships between discourse segments, such as whether a given utterance indicates agreement with the opinion expressed by another. We find that the incorporation of such information yields substantial improvements over classifying speeches in isolation.", "references": ["https://www.semanticscholar.org/paper/Identifying-Agreement-and-Disagreement-in-Speech%3A-Galley-McKeown/18d079a6d72e3f0b0c9214f597b6b178265b05ee", "https://www.semanticscholar.org/paper/Detection-Of-Agreement-vs.-Disagreement-In-Training-Hillard-Ostendorf/0d833ccbc824e81891e55baa2da6318e9651956a", "https://www.semanticscholar.org/paper/A-Preliminary-Investigation-into-Sentiment-Analysis-Mullen-Malouf/b479d6a38a8e8f4cf86427a756b3ad0f04bb04ab", "https://www.semanticscholar.org/paper/Extracting-Policy-Positions-from-Political-Texts-as-Laver-Benoit/7d9cc63dfbd34acf271e3a2c922ea1c07fb2f482", "https://www.semanticscholar.org/paper/Summarizing-Scientific-Articles%3A-Experiments-with-Teufel-Moens/54eafbf7621337724c6591cc13e604986243293a", "https://www.semanticscholar.org/paper/Multidimensional-text-analysis-for-eRulemaking-Kwon-Shulman/64030e7c4a39e30248a0d4728af3e009fb72768e", "https://www.semanticscholar.org/paper/A-Sentimental-Education%3A-Sentiment-Analysis-Using-Pang-Lee/167e1359943b96b9e92ee73db1df69a1f65d731d", "https://www.semanticscholar.org/paper/Automated-classification-of-congressional-Purpura-Hillard/7c06832d8f13e4faf149fff4c1368ac4fab1b6d8", "https://www.semanticscholar.org/paper/Cultural-orientation%3A-Classifying-subjective-by-Efron/328f15d4d55bd0df3556fae639c1e01d74fa69b9", "https://www.semanticscholar.org/paper/Dialogue-Act-Modeling-for-Automatic-Tagging-and-of-Stolcke-Ries/22d45dadde6b5837eff11dc031045754bc5901c3"]},
{"id": "fbd0e0ad4e06902b10b6a157b9db92df577720f1", "title": "Learning surface text patterns for a Question Answering System", "authors": ["Deepak Ravichandran", "Eduard H. Hovy"], "date": "2002", "abstract": "In this paper we explore the power of surface text patterns for open-domain question answering systems. In order to obtain an optimal set of patterns, we have developed a method for learning such patterns automatically. A tagged corpus is built from the Internet in a bootstrapping process by providing a few hand-crafted examples of each question type to Altavista. Patterns are then automatically extracted from the returned documents and standardized. We calculate the precision of each pattern, and the average precision for each question type. These patterns are then applied to find answers to new questions. Using the TREC-10 question set, we report results for two cases: answers determined from the TREC-10 corpus and from the web.", "references": ["https://www.semanticscholar.org/paper/A-question%2Fanswer-typology-with-surface-text-Hovy-Hermjakob/612a793c8750dafcb20a451adba28d33fbd1e457", "https://www.semanticscholar.org/paper/Patterns-of-Potential-Answer-Expressions-as-Clues-Soubbotin/7cb56a8518eeb15604f908cdad18660f11b55417", "https://www.semanticscholar.org/paper/Use-of-WordNet-Hypernyms-for-Answering-What-Is-Prager-Chu-Carroll/485d80aa77dcc120a0f56b699f6df29c40d9a865", "https://www.semanticscholar.org/paper/A-Question-Answering-System-Supported-by-Extraction-Srihari-Li/13d7795f4dc74070ddfe95187ce5e50d852702c5", "https://www.semanticscholar.org/paper/The-Effectiveness-of-Dictionary-and-Web-Based-Lin/040ec82087483db3e2c584c1a3901f8fc392c0a3", "https://www.semanticscholar.org/paper/SiteQ%3A-Engineering-High-Performance-QA-System-Using-Lee-Seo/c447d259ef71ef567f96faf406683a5d86a37084", "https://www.semanticscholar.org/paper/Automatically-Generating-Extraction-Patterns-from-Riloff/acec622ca4fb7e01a56116522d35ded149969d0a", "https://www.semanticscholar.org/paper/The-Use-of-External-Knowledge-of-Factoid-QA-Hovy-Hermjakob/03a1cf16053deadf60801ee7808d31d2ee0c45ea", "https://www.semanticscholar.org/paper/TREC-10-Experiments-at-CAS-ICT%3A-Filtering%2C-Web-and-Wang-Xu/48529c2eb44b348c7f5ba7c34716df8f7da6933c", "https://www.semanticscholar.org/paper/FALCON%3A-Boosting-Knowledge-for-Answer-Engines-Harabagiu-Moldovan/2f6c079bc68b93e7d0934a0b88c2105a2e39e040"]},
{"id": "41e07d21451df21dacda2fea6f90b53bf4b89b27", "title": "An Inference Model for Semantic Entailment in Natural Language", "authors": ["Rodrigo de Salvo Braz", "Roxana Girju", "Vasin Punyakanok", "Dan Roth", "Mark Sammons"], "date": "2005", "abstract": "Semantic entailment is the problem of determining if the meaning of a given sentence entails that of another. This is a fundamental problem in natural language understanding that provides a broad framework for studying language variability and has a large number of applications. This paper presents a principled approach to this problem that builds on inducing representations of text snippets into a hierarchical knowledge representation along with a sound optimization-based inferential mechanism that makes use of it to decide semantic entailment. A preliminary evaluation on the PASCAL text collection is presented.", "references": ["https://www.semanticscholar.org/paper/Knowledge-Representation-for-Semantic-Entailment-Braz-Girju/cf7924b1a151d512fe94b0ed482b74213f0a45fc", "https://www.semanticscholar.org/paper/Learning-to-Parse-Natural-Language-Database-Queries-Tang/9277028170918023039bb88f31e625162be038bc", "https://www.semanticscholar.org/paper/The-Necessity-of-Syntactic-Parsing-for-Semantic-Punyakanok-Roth/31274eabb84407e3bc2c1d14b804cb7bcc068111", "https://www.semanticscholar.org/paper/COGEX%3A-A-Logic-Prover-for-Question-Answering-Moldovan-Clark/aee98350f29a407596d782b09bb37e0f94a7a76f", "https://www.semanticscholar.org/paper/Problems-in-Logical-Form-Moore/a677f2d52d1c0c3e762f0942c7ddac886462d24a", "https://www.semanticscholar.org/paper/Towards-light-semantic-processing-for-question-Durme-Huang/2f04022e2bd8768cdddaeca76ad11f23e084be0c", "https://www.semanticscholar.org/paper/DIRT-%E2%80%93-Discovery-of-Inference-Rules-from-Text-Lin-Pantel/511c439c59f9bbfeb3be135d85ee75bef5594ad2", "https://www.semanticscholar.org/paper/Adding-Semantic-Annotation-to-the-Penn-TreeBank-Fleming-Kingsbury/621566b223a73c0a7d8cf918297ac02c5e58af38", "https://www.semanticscholar.org/paper/From-English-to-Logic%3A-Context-Free-Computation-of-Schubert-Pelletier/a19ea03ab4645dcce3e87719e4b7ffaf565bf526", "https://www.semanticscholar.org/paper/Head-Driven-Statistical-Models-for-Natural-Language-Collins/3fc44ff7f37ec5585310666c183c65e0a0bb2446"]},
{"id": "d1505c6123c102e53eb19dff312cb25cea840b72", "title": "Teaching Machines to Read and Comprehend", "authors": ["Karl Moritz Hermann", "Tom\u00e1s Kocisk\u00fd", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "date": "2015", "abstract": "Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.", "references": ["https://www.semanticscholar.org/paper/Weakly-Supervised-Memory-Networks-Sukhbaatar-Szlam/a583af2696030bcf5f556edc74573fbee902be0b", "https://www.semanticscholar.org/paper/Machine-Reading-at-the-University-of-Washington-Poon-Christensen/380169dfdf019dd77f3316ab14fefab337113652", "https://www.semanticscholar.org/paper/Natural-Language-Processing-(Almost)-from-Scratch-Collobert-Weston/bc1022b031dc6c7019696492e8116598097a8c12", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/MCTest%3A-A-Challenge-Dataset-for-the-Open-Domain-of-Richardson-Burges/564257469fa44cdb57e4272f85253efb9acfd69d", "https://www.semanticscholar.org/paper/A-Convolutional-Neural-Network-for-Modelling-Kalchbrenner-Grefenstette/27725a2d2a8cee9bf9fffc6c2167017103aba0fa", "https://www.semanticscholar.org/paper/A-Rule-based-Question-Answering-System-for-Reading-Riloff-Thelen/445406b0d88ae965fa587cf5c167374ff1bbc09a", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Supervised-Sequence-Labelling-with-Recurrent-Neural-Graves/a97b5db17acc731ef67321832dbbaf5766153135", "https://www.semanticscholar.org/paper/Automatic-Generation-of-Story-Highlights-Woodsend-Lapata/7453a974d355883f342aaa6e29eb86a13edcedb9"]},
{"id": "f26e088bc4659a9b7fce28b6604d26de779bcf93", "title": "Learning Answer-Entailing Structures for Machine Comprehension", "authors": ["Mrinmaya Sachan", "Kumar Avinava Dubey", "Eric P. Xing", "Matthew Richardson"], "date": "2015", "abstract": "Understanding open-domain text is one of the primary challenges in NLP. Machine comprehension evaluates the system\u2019s ability to understand text through a series of question-answering tasks on short pieces of text such that the correct answer can be found only in the given text. For this task, we posit that there is a hidden (latent) structure that explains the relation between the question, correct answer, and text. We call this the answer-entailing structure; given the structure, the correctness of the answer is evident. Since the structure is latent, it must be inferred. We present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs), and uses what it learns to answer machine comprehension questions on novel texts. We extend this framework to incorporate multi-task learning on the different subtasks that are required to perform machine comprehension. Evaluation on a publicly available dataset shows that our framework outperforms various IR and neuralnetwork baselines, achieving an overall accuracy of 67.8% (vs. 59.9%, the best previously-published result.)", "references": ["https://www.semanticscholar.org/paper/MCTest%3A-A-Challenge-Dataset-for-the-Open-Domain-of-Richardson-Burges/564257469fa44cdb57e4272f85253efb9acfd69d", "https://www.semanticscholar.org/paper/Towards-AI-Complete-Question-Answering%3A-A-Set-of-Weston-Bordes/abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "https://www.semanticscholar.org/paper/A-Neural-Network-for-Factoid-Question-Answering-Iyyer-Boyd-Graber/af44f5db5b4396e1670cda07eff5ad84145ba843", "https://www.semanticscholar.org/paper/Semantic-Parsing-on-Freebase-from-Question-Answer-Berant-Chou/b29447ba499507a259ae9d8f685d60cc1597d7d3", "https://www.semanticscholar.org/paper/Question-Answering-Using-Enhanced-Lexical-Semantic-Yih-Chang/f1aa6df7f18f9cb7d6b6c5c190aeade47b450656", "https://www.semanticscholar.org/paper/Learning-Question-Classifiers-Li-Roth/2c8ac3e1f0edeed1fbd76813e61efdc384c319c7", "https://www.semanticscholar.org/paper/A-Phrase-Based-Alignment-Model-for-Natural-Language-MacCartney-Galley/581f4e8d74b9ffa3d1e4fbbac8d8742de79cb6c2", "https://www.semanticscholar.org/paper/Information-Extraction-over-Structured-Data%3A-with-Yao-Durme/319e572fcddff77513eed8a25effbc7d9ff8ef85", "https://www.semanticscholar.org/paper/Factoid-Question-Answering-over-Unstructured-and-Cucerzan-Agichtein/008d80bea26056ced48daac9834abe8ddf95d651", "https://www.semanticscholar.org/paper/Relation-Alignment-for-Textual-Entailment-Sammons-Vydiswaran/5380c0946aff2aa100c91ccbbe7ac034441e24a3"]},
{"id": "083630744dbf60994867cbd776bfe601b4d0dbe6", "title": "Columns for visual features of objects in monkey inferotemporal cortex", "authors": ["Ichiro Fujita", "Keiji Tanaka", "Minami Ito", "Kang Cheng"], "date": "1992", "abstract": "AT early stages of the mammalian visual cortex, neurons with similar stimulus selectivities are vertically arrayed through the thickness of the cortical sheet and clustered in patches or bands across the surface. This organization, referred to as a 'column', has been found with respect to one-dimensional stimulus parameters such as orientation of stimulus contours1, eye dominance of visual inputs1, and direction of stimulus motion2. It is unclear, however, whether information with extremely high dimensions, such as visual shape, is organized in a similar columnar fashion or in a different manner in the brain. Here we report that the anterior inferotemporal area of the monkey cortex, the final station of the visual cortical stream crucial for object recognition3\u20138, consists of columns, each containing cells responsive to similar visual features of objects.", "references": []},
{"id": "71ae756c75ac89e2d731c9c79649562b5768ff39", "title": "Memory Networks", "authors": ["Jason Weston", "Sumit Chopra", "Antoine Bordes"], "date": "2015", "abstract": "Abstract: We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.", "references": ["https://www.semanticscholar.org/paper/Towards-Understanding-Situated-Natural-Language-Bordes-Usunier/2b776119a1347e1455dc498ff5078b3a94029ed9", "https://www.semanticscholar.org/paper/Open-Question-Answering-with-Weakly-Supervised-Bordes-Weston/a584211768d49f80192f13b8ed2fda9c058dec34", "https://www.semanticscholar.org/paper/Paraphrase-Driven-Learning-for-Open-Question-Fader-Zettlemoyer/c0be2ac2f45681f1852fc1d298af5dceb85834f4", "https://www.semanticscholar.org/paper/MCTest%3A-A-Challenge-Dataset-for-the-Open-Domain-of-Richardson-Burges/564257469fa44cdb57e4272f85253efb9acfd69d", "https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/The-Fellowship-of-the-Ring%3A-Being-the-first-part-of-Tolkien/97cc2fab13bf20d130de19dfffe6670aac2076b5"]},
{"id": "86876c68d05af50330d07ee10790ddcf6578c7d9", "title": "A neural basis for visual search in inferior temporal cortex", "authors": ["Leonardo Chelazzi", "Earl K. Miller", "John Duncan", "Robert Desimone"], "date": "1993", "abstract": "WE often search for a face in a crowd or for a particular object in a cluttered environment. In this type of visual search, memory interacts with attention: the mediating neural mechanisms should include a stored representation of the object and a means for selecting that object from among others in the scene1\u20134. Here we test whether neurons in inferior temporal cortex, an area known to be important for high-level visual processing, might provide these components. Monkeys were presented with a complex picture (the cue) to hold in memory during a delay period. The cue initiated activity that persisted through the delay among the neurons that were tuned to its features. The monkeys were then given 2\u20135 choice pictures and were required to make an eye movement to the one (the target) that matched the cue. About 90\u2013120 milliseconds before the onset of the eye movement to the target, responses to non-targets were suppressed and the neuronal response was dominated by the target. The results suggest that inferior temporal cortex is involved in selecting the objects to which we attend and foveate.", "references": []},
{"id": "61b2d0383b186c4d634c5f51421cab67c16d90a1", "title": "A Computational Model for Visual Selection", "authors": ["Yali Amit", "Donald Geman"], "date": "1999", "abstract": "We propose a computational model for detecting and localizing instances from an object class in static gray-level images. We divide detection into visual selection and final classification, concentrating on the former: drastically reducing the number of candidate regions that require further, usually more intensive, processing, but with a minimum of computation and missed detections. Bottom-up processing is based on local groupings of edge fragments constrained by loose geometrical relationships. They have no a priori semantic or geometric interpretation. The role of training is to select special groupings that are moderately likely at certain places on the object but rare in the background. We show that the statistics in both populations are stable. The candidate regions are those that contain global arrangements of several local groupings. Whereas our model was not conceived to explain brain functions, it does cohere with evidence about the functions of neurons in V1 and V2, such as responses to coarse or incomplete patterns (e.g., illusory contours) and to scale and translation invariance in IT. Finally, the algorithm is applied to face and symbol detection.", "references": ["https://www.semanticscholar.org/paper/A-Neural-Network-Architecture-for-Visual-Selection-Amit/62ccf3020ac41088b39c3982d119752b0ba6b261", "https://www.semanticscholar.org/paper/Perceptual-Organization-and-Visual-Recognition-Lowe/27ce5a120a86632dd56f869ee65656b7d7312a3a", "https://www.semanticscholar.org/paper/Human-image-understanding%3A-recent-research-and-a-Biederman/9ee1e8a9cc139282b1786d8c45c94399f150dd5d", "https://www.semanticscholar.org/paper/Inferior-temporal-mechanisms-for-invariant-object-Lueschow-Miller/4c63fa480d1232a128ef91fa4cc262ecce15e501", "https://www.semanticscholar.org/paper/Shape-Quantization-and-Recognition-with-Randomized-Amit-Geman/de5e95325e139fd0a46df1dd28aabecd0273b772", "https://www.semanticscholar.org/paper/High-Level-Vision%3A-Object-Recognition-and-Visual-Ullman/b95bb08703c92d3429865cdc4b8a0910e4ed7059", "https://www.semanticscholar.org/paper/Macaque-VI-neurons-can-signal-%E2%80%98illusory%E2%80%99-contours-Grosof-Shapley/925548196e6545d3cc64bbc4ed99578775f5fa14", "https://www.semanticscholar.org/paper/Psychophysical-support-for-a-two-dimensional-view-B%C3%BClthoff-Edelman/ca97fc4dadcc564e5081743b52a4d77031d1c177", "https://www.semanticscholar.org/paper/Columns-for-visual-features-of-objects-in-monkey-Fujita-Tanaka/083630744dbf60994867cbd776bfe601b4d0dbe6", "https://www.semanticscholar.org/paper/Efficient-Focusing-and-Face-Detection-Amit-Geman/b56ab69d6d2ed840f3850df3c60ccdf1d5f284d9"]},
{"id": "76c08d739988979653a89c9fc072339d9918e0e4", "title": "Recursive automatic algorithm selection for inductive learning", "authors": ["Carla E. Brodley"], "date": "1995", "abstract": "The results of empirical comparisons of existing learning algorithms illustrate that each algorithm has a selective superiority; each is best for some but not all tasks. Selective superiority arises because each learning algorithm searches within a restricted generalization space, defined by its representation language, and employs a search bias for selecting a generalization in that space. Given a data set, it is often not clear beforehand which algorithm will yield the best performance. The problem is complicated further because for some learning tasks, different subtasks are learned best using different algorithms. In such cases, the ability to form a hybrid classifier that combines different representation languages will produce a more accurate classifier than employing a single representation language and search bias. \nThis dissertation presents an approach to overcoming this problem by applying knowledge about the biases of a set of learning algorithms to conduct a recursive automatic algorithm search. The approach permits classifiers learned by the available algorithms to be mixed in a recursive tree-structured hybrid, thereby allowing different subproblems of the learning task to be learned by different algorithms. The Model Class Selection System (MCS), an implementation of the approach, combines decision trees, linear discriminant functions and instance-based classifiers in a tree-structured hybrid classifier. Heuristic knowledge about the characteristics that indicate one bias is better than another is encoded in the rule base that guides MCS's search for the best classifier. An empirical evaluation illustrates that MCS achieves classification accuracies equal to or higher than the best of its primitive learning components for a variety of data sets, demonstrating that domain-independent knowledge about the biases of machine learning algorithms can guide an automatic algorithm selection search.", "references": []},
{"id": "c4100faa2cc35bc72e61dcbb173f1fee5e8e8840", "title": "Semantic Role Labeling", "authors": ["C\u00edcero Nogueira dos Santos", "Ruy Luiz Milidi\u00fa"], "date": "2012", "abstract": "This chapter presents the application of the ETL approach to semantic role labeling (SRL). The SRL task consists in detecting basic event structures in a given text. Some of these event structures include who did what to whom, when and where. We evaluate the performance of ETL over two English language corpora: CoNLL-2004 and CoNLL-2005. ETL system achieves regular results for the two corpora. However, for the CoNLL-2004 Corpus, our ETL system outperforms the TBL system proposed by Higgins [4]. ETL committee significantly improves the ETL results for the two corpora. This chapter is organized as follows. In Sect. 8.1, we describe the selected corpora. In Sect. 8.2, we detail some modeling configurations used in our SRL system. In Sect. 8.3, we show some configurations used in the machine learning algorithms. Section 8.4 presents the application of ETL for the CoNLL-2004 Corpus. Section 8.5 presents the application of ETL for the CoNLL-2005 Corpus. Finally, Sect. 8.6 presents some concluding remarks.", "references": []},
{"id": "9464d15f4f8d578f93332db4aa1c9c182fd51735", "title": "Multitask Learning: A Knowledge-Based Source of Inductive Bias", "authors": ["Rich Caruana"], "date": "1993", "abstract": "This paper suggests that it may be easier to learn several hard tasks at one time than to learn these same tasks separately. In effect, the information provided by the training signal for each task serves as a domain-specific inductive bias for the other tasks. Frequently the world gives us clusters of related tasks to learn. When it does not, it is often straightforward to create additional tasks. For many domains, acquiring inductive bias by collecting additional teaching signal may be more practical than the traditional approach of codifying domain-specific biases acquired from human expertise. We call this approach Multitask Learning (MTL). Since much of the power of an inductive learner follows directly from its inductive bias, multitask learning may yield more powerful learning. An empirical example of multitask connectionist learning is presented where learning improves by training one network on several related tasks at the same time. Multitask decision tree induction is also outlined.", "references": ["https://www.semanticscholar.org/paper/Direct-Transfer-of-Learned-Information-Among-Neural-Pratt-Mostow/052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c", "https://www.semanticscholar.org/paper/Rule-Injection-Hints-as-a-Means-of-Improving-and-Suddarth-Kergosien/ffb199e36de4f34ea233a30d392fdcf0c3b25a14", "https://www.semanticscholar.org/paper/Symbolic-Neural-Systems-and-the-Use-of-Hints-for-Suddarth-Holden/dfe7dbbd6e8d5d3720f58c2c9a0b9bec040a8ef8", "https://www.semanticscholar.org/paper/Learning-representations-by-back-propagating-errors-Rumelhart-Hinton/052b1d8ce63b07fec3de9dbb583772d860b7c769", "https://www.semanticscholar.org/paper/Induction-of-Decision-Trees-Quinlan/bcee7c85d237b79491a773ef51e746bbbcf48e35", "https://www.semanticscholar.org/paper/Learning-from-hints-in-neural-networks-Abu-Mostafa/e3cd36c092abd65d6ac8e648f3468eeee90ee1fc", "https://www.semanticscholar.org/paper/A-Personal-Learning-Apprentice-Dent-Boticario/9cf3250f13d98bd2bd1b23ff81a333119f82ccc3", "https://www.semanticscholar.org/paper/Conceptual-Clustering%2C-Learning-from-Examples%2C-and-Fisher/358b49ab2eb6d981130a6d7facf9e40c71afe493", "https://www.semanticscholar.org/paper/NETtalk%3A-a-parallel-network-that-learns-to-read-Sejnowski-Rosenberg/406033f22b6a671b94bcbdfaf63070b7ce6f3e48", "https://www.semanticscholar.org/paper/Modularity-and-scaling-in-large-phonemic-neural-Waibel-Sawai/3f5966eba6336438ade570ea57e2e682fa0b5985"]},
{"id": "47e3d8a1f8e92923e739ca34bea17004a40514e9", "title": "Training Continuous Space Language Models: Some Practical Issues", "authors": ["Hai Son Le", "Alexandre Allauzen", "Guillaume Wisniewski", "Fran\u00e7ois Yvon"], "date": "2010", "abstract": "Using multi-layer neural networks to estimate the probabilities of word sequences is a promising research area in statistical language modeling, with applications in speech recognition and statistical machine translation. However, training such models for large vocabulary tasks is computationally challenging which does not scale easily to the huge corpora that are nowadays available. In this work, we study the performance and behavior of two neural statistical language models so as to highlight some important caveats of the classical training algorithms. The induced word embeddings for extreme cases are also analysed, thus providing insight into the convergence issues. A new initialization scheme and new training techniques are then introduced. These methods are shown to greatly reduce the training time and to significantly improve performance, both in terms of perplexity and on a large-scale translation task.", "references": ["https://www.semanticscholar.org/paper/Continuous-space-language-models-Schwenk/0fcc184b3b90405ec3ceafd6a4007c749df7c363", "https://www.semanticscholar.org/paper/Connectionist-language-modeling-for-large-speech-Schwenk-Gauvain/e41498c05d4c68e4750fb84a380317a112d97b01", "https://www.semanticscholar.org/paper/Continuous-Space-Language-Models-for-Statistical-Schwenk/d4a258df43cc14e46988de9a4a7b2f0ea817529b", "https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "https://www.semanticscholar.org/paper/Category-Based-Statistical-Language-Models-Niesler/24653b3b33d48c409deb672f8d8ee0eff31cd418", "https://www.semanticscholar.org/paper/Three-new-graphical-models-for-statistical-language-Mnih-Hinton/bd7d93193aad6c4b71cc8942e808753019e87706", "https://www.semanticscholar.org/paper/A-unified-architecture-for-natural-language-deep-Collobert-Weston/57458bc1cffe5caa45a885af986d70f723f406b4", "https://www.semanticscholar.org/paper/Empirical-study-of-neural-network-language-models-Emami-Mangu/26080f0969a520ebd82f252dd060f5a4948bbd6e", "https://www.semanticscholar.org/paper/Statistical-Machine-Translation-Osborne/b3e89f05876d47b9bd6ece225aaeee457a6824e8", "https://www.semanticscholar.org/paper/Minimum-Error-Rate-Training-in-Statistical-Machine-Och/1f12451245667a85d0ee225a80880fc93c71cc8b"]},
{"id": "b479d6a38a8e8f4cf86427a756b3ad0f04bb04ab", "title": "A Preliminary Investigation into Sentiment Analysis of Informal Political Discourse", "authors": ["Tony Mullen", "Robert Malouf"], "date": "2006", "abstract": "With the rise of weblogs and the increasing tendency of online publications to turn to message-board style reader feedback venues, informal political discourse is becoming an important feature of the intellectual landscape of the Internet, creating a challenging and worthwhile area for experimentation in techniques for sentiment analysis. We describe preliminary statistical tests on a new dataset of political discussion group postings which indicate that posts made in direct response to other posts in a thread have a strong tendency to represent an opposing political viewpoint to the original post. We conclude that traditional text classification methods will be inadequate to the task of sentiment analysis in this domain, and that progress is to be made by exploiting information about how posters interact with each", "references": ["https://www.semanticscholar.org/paper/Sentiment-analysis%3A-capturing-favorability-using-Nasukawa-Yi/ababc1999b5f31409c78c39d1842219821e37a6d", "https://www.semanticscholar.org/paper/A-Sentimental-Education%3A-Sentiment-Analysis-Using-Pang-Lee/167e1359943b96b9e92ee73db1df69a1f65d731d", "https://www.semanticscholar.org/paper/The-political-blogosphere-and-the-2004-U.S.-divided-Adamic-Glance/2d679de19dd1a04b49a45c42e32697e8779cde0c", "https://www.semanticscholar.org/paper/Recognizing-Contextual-Polarity-in-Phrase-Level-Wilson-Wiebe/9b4876f7313b111074e79a01f570e6e9e02c0dce", "https://www.semanticscholar.org/paper/Sentiment-Analysis-using-Support-Vector-Machines-Mullen-Collier/bc7308a97ec2d3f7985d48671abe7a8942a5b9f8", "https://www.semanticscholar.org/paper/Cultural-orientation%3A-Classifying-subjective-by-Efron/328f15d4d55bd0df3556fae639c1e01d74fa69b9", "https://www.semanticscholar.org/paper/Mining-product-reputations-on-the-Web-Morinaga-Yamanishi/4f0d67a3a8a61d3a7ef0f940274ecff5f0d640ea", "https://www.semanticscholar.org/paper/Measuring-praise-and-criticism%3A-Inference-of-from-Turney-Littman/5c8d05e27e36ebd64ee43fe1670262cdcc2123ba", "https://www.semanticscholar.org/paper/Thumbs-Up-or-Thumbs-Down-Semantic-Orientation-to-of-Turney/9e7c7853a16a378cc24a082153b282257a9675b7", "https://www.semanticscholar.org/paper/Implications-of-the-Recursive-Representation-for-in-Efron-Marchionini/dc94eecba60b65a4d613f3892fd5fc77016fb501"]},
{"id": "1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "title": "On Using Very Large Target Vocabulary for Neural Machine Translation", "authors": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "date": "2015", "abstract": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system.", "references": ["https://www.semanticscholar.org/paper/Addressing-the-Rare-Word-Problem-in-Neural-Machine-Luong-Sutskever/1956c239b3552e030db1b78951f64781101125ed", "https://www.semanticscholar.org/paper/Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "https://www.semanticscholar.org/paper/Sequence-to-Sequence-Learning-with-Neural-Networks-Sutskever-Vinyals/cea967b59209c6be22829699f05b8b1ac4dc092d", "https://www.semanticscholar.org/paper/On-the-Properties-of-Neural-Machine-Translation%3A-Cho-Merrienboer/1eb09fecd75eb27825dce4f964b97f4f5cc399d7", "https://www.semanticscholar.org/paper/Recurrent-Continuous-Translation-Models-Kalchbrenner-Blunsom/944a1cfd79dbfb6fef460360a0765ba790f4027a", "https://www.semanticscholar.org/paper/Learning-word-embeddings-efficiently-with-Mnih-Kavukcuoglu/53ca064b9f1b92951c1997e90b776e95b0880e52", "https://www.semanticscholar.org/paper/Statistical-Phrase-Based-Translation-Koehn-Och/a4b828609b60b06e61bea7a4029cc9e1cad5df87", "https://www.semanticscholar.org/paper/Learning-Phrase-Representations-using-RNN-for-Cho-Merrienboer/0b544dfe355a5070b60986319a3f51fb45d1348e", "https://www.semanticscholar.org/paper/Adaptive-Importance-Sampling-to-Accelerate-Training-Bengio-Senecal/699d5ab38deee78b1fd17cc8ad233c74196d16e9", "https://www.semanticscholar.org/paper/Statistical-Machine-Translation-Osborne/b3e89f05876d47b9bd6ece225aaeee457a6824e8"]},
{"id": "ba786c46373892554b98df42df7af6f5da343c9d", "title": "Large Language Models in Machine Translation", "authors": ["Thorsten Brants", "Ashok C. Popat", "Peng Xu", "Franz Josef Och", "Jeffrey Dean"], "date": "2007", "abstract": "This paper reports on the benefits of largescale statistical language modeling in machine translation. A distributed infrastructure is proposed which we use to train on up to 2 trillion tokens, resulting in language models having up to 300 billion n-grams. It is capable of providing smoothed probabilities for fast, single-pass decoding. We introduce a new smoothing method, dubbed Stupid Backoff, that is inexpensive to train on large data sets and approaches the quality of Kneser-Ney Smoothing as the amount of training data increases.", "references": ["https://www.semanticscholar.org/paper/Improved-backing-off-for-M-gram-language-modeling-Kneser-Ney/9548ac30c113562a51e603dbbc8e9fa651cfd3ab", "https://www.semanticscholar.org/paper/Large-Scale-Distributed-Language-Modeling-Emami-Papineni/591080c335daba2494f18cc04c9f7071501af0eb", "https://www.semanticscholar.org/paper/A-bit-of-progress-in-language-modeling-Goodman/09c76da2361d46689825c4efc37ad862347ca577", "https://www.semanticscholar.org/paper/The-Alignment-Template-Approach-to-Statistical-Och-Ney/c6a83c4fcc99ba6753109301949c5b7cfa978079", "https://www.semanticscholar.org/paper/Bleu%3A-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos/d7da009f457917aa381619facfa5ffae9329a6e9", "https://www.semanticscholar.org/paper/Distributed-Language-Modeling-for-N-best-List-Zhang-Hildebrand/0afe29a9a8871faab23cbf90cf499c735a3b0c51", "https://www.semanticscholar.org/paper/Statistical-Significance-Tests-for-Machine-Koehn/cb826a3899752b796f14df1c50378c64954a6b0a", "https://www.semanticscholar.org/paper/The-Mathematics-of-Statistical-Machine-Translation%3A-Brown-Pietra/ab7b5917515c460b90451e67852171a531671ab8", "https://www.semanticscholar.org/paper/Estimation-of-probabilities-from-sparse-data-for-of-Katz/b0130277677e5b915d5cd86b3afafd77fd08eb2e", "https://www.semanticscholar.org/paper/An-empirical-study-of-smoothing-techniques-for-ChenStanley-GoodmanJoshua/d4e8bed3b50a035e1eabad614fe4218a34b3b178"]},
{"id": "18d079a6d72e3f0b0c9214f597b6b178265b05ee", "title": "Identifying Agreement and Disagreement in Conversational Speech: Use of Bayesian Networks to Model Pragmatic Dependencies", "authors": ["Michel Galley", "Kathleen McKeown", "Julia Hirschberg", "Elizabeth Shriberg"], "date": "2004", "abstract": "We describe a statistical approach for modeling agreements and disagreements in conversational interaction. Our approach first identifies adjacency pairs using maximum entropy ranking based on a set of lexical, durational, and structural features that look both forward and backward in the discourse. We then classify utterances as agreement or disagreement using these adjacency pairs and features that represent various pragmatic influences of previous agreement or disagreement on the current utterance. Our approach achieves 86.9% accuracy, a 4.9% increase over previous work.", "references": ["https://www.semanticscholar.org/paper/Detection-Of-Agreement-vs.-Disagreement-In-Training-Hillard-Ostendorf/0d833ccbc824e81891e55baa2da6318e9651956a", "https://www.semanticscholar.org/paper/Dialogue-Act-Modeling-for-Automatic-Tagging-and-of-Stolcke-Ries/22d45dadde6b5837eff11dc031045754bc5901c3", "https://www.semanticscholar.org/paper/The-ICSI-Meeting-Recorder-Dialog-Act-(MRDA)-Corpus-Shriberg-Dhillon/f45269f092e15c1fb21eebace7f6ac56b88ad8af", "https://www.semanticscholar.org/paper/Conditional-Structure-versus-Conditional-Estimation-Klein-Manning/205b9f4891a2ead886604f161a44b3aed483609a", "https://www.semanticscholar.org/paper/Meeting-Recorder-Project%3A-Dialog-Act-Labeling-Guide-Dhillon-Bhagat/e8cb114b328711d2502e3502f29830562fd2b46c", "https://www.semanticscholar.org/paper/Empirical-Studies-on-the-Disambiguation-of-Cue-Hirschberg-Litman/ec179f9b30d3a4e0f8a50c86d951557dfcbb20d1", "https://www.semanticscholar.org/paper/Discriminative-Reranking-for-Natural-Language-Collins-Koo/844db702be4bc149b06b822b47247e15f5894cc3", "https://www.semanticscholar.org/paper/The-ICSI-Meeting-Corpus-Janin-Baron/ff7804a13efc26bf23ce813319641db69ddbb969", "https://www.semanticscholar.org/paper/Opening-up-Closings-Schegloff-Sacks/3120d07fdfef87f56a3abab57579b7f281ae8862", "https://www.semanticscholar.org/paper/Predicting-the-Semantic-Orientation-of-Adjectives-Hatzivassiloglou-McKeown/b62b80384f402d38c3425db6a99899d1cb9c50c6"]},
{"id": "621566b223a73c0a7d8cf918297ac02c5e58af38", "title": "Adding Semantic Annotation to the Penn TreeBank", "authors": ["Andrew J. Fleming", "Paul Kingsbury", "Martha Palmer", "Mitch Marcus"], "date": "1998", "abstract": "This paper presents our basic approach to creating Proposition Bank, which involves adding a layer of semantic annotation to the Penn English TreeBank. Without attempting to confirm or disconfirm any particular semantic theory, our goal is to provide consistent argument labeling that will facilitate the automatic extraction of relational data. An argument such asthe window in John broke the window and in The window brokewould receive the same label in both sentences. In order to ensure reliable human annotation, we provide our annotators with explicit guidelines for labeling all of the syntactic and semantic frames of each particular verb. We give several examples of these guidelines and discuss the inter\u2212annotator agreement figures. We also discuss our current experiments on the automatic expansion of our verb guidelines based on verb class membership. Our current rate of progress and our consistency of annotation demonstrate the feasibility of the task.", "references": ["https://www.semanticscholar.org/paper/A-Frame-Semantic-Approach-to-Semantic-Annotation-Lowe/782d19b7b07c8cdab8174a0c9b2b0a7e28917d6f", "https://www.semanticscholar.org/paper/Building-a-Large-Annotated-Corpus-of-English%3A-The-Marcus-Santorini/0b44fcbeea9415d400c5f5789d6b892b6f98daff", "https://www.semanticscholar.org/paper/Discriminative-Reranking-for-Natural-Language-Collins-Koo/844db702be4bc149b06b822b47247e15f5894cc3", "https://www.semanticscholar.org/paper/English-Verb-Classes-and-Alternations%3A-A-Levin/6cbc1eb25f4ab29a613418b3b0740e74141a0f17", "https://www.semanticscholar.org/paper/Class-Based-Construction-of-a-Verb-Lexicon-Schuler-Dang/031b4656032aad6699a642f62d993173b378b772", "https://www.semanticscholar.org/paper/Parsing-with-Context-Free-Grammars-and-Word-Charniak/407d55ee40f1bdb10745155fde211d566c2d0c71", "https://www.semanticscholar.org/paper/Five-papers-on-wordnet-Miller-Beckwith/1f736ee2f64f19cbebcebc0ae2b6d655eb43521e", "https://www.semanticscholar.org/paper/Three-generative%2C-lexicalised-models-for-parsing-Collins/d9748ee468f1f1403262c2ca345718fde2d1950b"]},
{"id": "380169dfdf019dd77f3316ab14fefab337113652", "title": "Machine Reading at the University of Washington", "authors": ["Hoifung Poon", "Janara Christensen", "Pedro Domingos", "Oren Etzioni", "Raphael Hoffmann", "Chlo\u00e9 Kiddon", "Thomas Lin", "Xiao Ling", "Mausam", "Alan Ritter", "Stefan Schoenmackers", "Stephen Soderland", "Daniel S. Weld", "Fei Wu", "Congle Zhang"], "date": "2010", "abstract": "Machine reading is a long-standing goal of AI and NLP. In recent years, tremendous progress has been made in developing machine learning approaches for many of its subtasks such as parsing, information extraction, and question answering. However, existing end-to-end solutions typically require substantial amount of human efforts (e.g., labeled data and/or manual engineering), and are not well poised for Web-scale knowledge acquisition. In this paper, we propose a unifying approach for machine reading by bootstrapping from the easiest extractable knowledge and conquering the long tail via a self-supervised learning process. This self-supervision is powered by joint inference based on Markov logic, and is made scalable by leveraging hierarchical structures and coarse-to-fine inference. Researchers at the University of Washington have taken the first steps in this direction. Our existing work explores the wide spectrum of this vision and shows its promise.", "references": ["https://www.semanticscholar.org/paper/Strategies-for-lifelong-knowledge-extraction-from-Banko-Etzioni/9a1336dfa2f4c2df080990df2eff6e2bb3389238", "https://www.semanticscholar.org/paper/Coarse-to-Fine-Natural-Language-Processing-Petrov/cbc8488784ee77d5a9921580788cb53f995aaaeb", "https://www.semanticscholar.org/paper/Learning-5000-Relational-Extractors-Hoffmann-Zhang/bd298c1bcefcd7feb108111cd72758c265d16ee6", "https://www.semanticscholar.org/paper/Information-extraction-from-Wikipedia%3A-moving-down-Wu-Hoffmann/c4bd8e9c69c5270905a1c0eb1a87fca7a92944cb", "https://www.semanticscholar.org/paper/Extracting-Semantic-Networks-from-Text-Via-Kok-Domingos/908b1f0c1f325355ebbfd9dbe06d37c376c16ef3", "https://www.semanticscholar.org/paper/Building-large-knowledge-bases-by-mass-Richardson-Domingos/399edd2bc226fa1b9e5ff46b37b8b21f82919948", "https://www.semanticscholar.org/paper/Semantic-Role-Labeling-for-Open-Information-Christensen-Mausam/cf53bda1fbaf6a70da4dd541423caab72267cf47", "https://www.semanticscholar.org/paper/Joint-Unsupervised-Coreference-Resolution-with-Poon-Domingos/ddde3c7371a8d3d8fbbf53a37aba96fa9b0ce1db", "https://www.semanticscholar.org/paper/Scaling-Textual-Inference-to-the-Web-Schoenmackers-Etzioni/cf3ba53a5030b8dd6ec65101b6f5a9b8e4d06f80", "https://www.semanticscholar.org/paper/Automatically-refining-the-wikipedia-infobox-Wu-Weld/0c236e611a90018e84d9de23d1cff241354079be"]},
{"id": "abb33d75dc297993fcc3fb75e0f4498f413eb4f6", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "authors": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "date": "2016", "abstract": "One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.", "references": ["https://www.semanticscholar.org/paper/MCTest%3A-A-Challenge-Dataset-for-the-Open-Domain-of-Richardson-Burges/564257469fa44cdb57e4272f85253efb9acfd69d", "https://www.semanticscholar.org/paper/Paraphrase-Driven-Learning-for-Open-Question-Fader-Zettlemoyer/c0be2ac2f45681f1852fc1d298af5dceb85834f4", "https://www.semanticscholar.org/paper/Semantic-Parsing-on-Freebase-from-Question-Answer-Berant-Chou/b29447ba499507a259ae9d8f685d60cc1597d7d3", "https://www.semanticscholar.org/paper/Learning-Dependency-Based-Compositional-Semantics-Liang-Jordan/3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "https://www.semanticscholar.org/paper/Towards-Understanding-Situated-Natural-Language-Bordes-Usunier/2b776119a1347e1455dc498ff5078b3a94029ed9", "https://www.semanticscholar.org/paper/Freebase-QA%3A-Information-Extraction-or-Semantic-Yao-Berant/b75329489baf067e6f7bbb74f16ffd49fba80dfa", "https://www.semanticscholar.org/paper/Factor-based-Compositional-Embedding-Models-Yu/8a93cd1b6fbf7c8c86637bae18d979dafeb9a7c1", "https://www.semanticscholar.org/paper/The-Unreasonable-Effectiveness-of-Data-Halevy-Norvig/c1787db25af5614f41e56938aa594f2dbb1dca07", "https://www.semanticscholar.org/paper/Simple-Coreference-Resolution-with-Rich-Syntactic-Haghighi-Klein/abba83b5747e98d10ab982df9ae94cc799668f16", "https://www.semanticscholar.org/paper/Twisty-Little-Passages%3A-An-Approach-to-Interactive-Montfort/403499d0fb67853c4fa659d41de7005274bb17cd"]},
{"id": "319e572fcddff77513eed8a25effbc7d9ff8ef85", "title": "Information Extraction over Structured Data: Question Answering with Freebase", "authors": ["Xuchen Yao", "Benjamin Van Durme"], "date": "2014", "abstract": "Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.", "references": ["https://www.semanticscholar.org/paper/Paraphrase-Driven-Learning-for-Open-Question-Fader-Zettlemoyer/c0be2ac2f45681f1852fc1d298af5dceb85834f4", "https://www.semanticscholar.org/paper/Natural-Language-Questions-for-the-Web-of-Data-Yahya-Berberich/c1052027ddbacc24fc4a244d2c073f86aca62747", "https://www.semanticscholar.org/paper/Semantic-Parsing-on-Freebase-from-Question-Answer-Berant-Chou/b29447ba499507a259ae9d8f685d60cc1597d7d3", "https://www.semanticscholar.org/paper/Question-answering-from-structured-knowledge-Frank-Krieger/eb17ae54ad729d06b5b21ac4555f35b8946581b5", "https://www.semanticscholar.org/paper/Template-based-question-answering-over-RDF-data-Unger-B%C3%BChmann/eb04bd49bdc2c9218624435eb277be2a973b21a5", "https://www.semanticscholar.org/paper/Question-answering-on-interlinked-data-Shekarpour-Ngomo/d7f2aae38f5fecb1d5131169d62da3cbe79e9646", "https://www.semanticscholar.org/paper/Scaling-Semantic-Parsers-with-On-the-Fly-Ontology-Kwiatkowski-Choi/b2ac51e10a3510eadac5eac5e4fb828f086fab88", "https://www.semanticscholar.org/paper/AquaLog%3A-An-Ontology-Portable-Question-Answering-L%C3%B3pez-Pasin/5b560c8b01040b0e3d97b8c8f067f504e8a94b6d", "https://www.semanticscholar.org/paper/Identifying-Relations-for-Open-Information-Fader-Soderland/d4b651d6a904f69f8fa1dcad4ebe972296af3a9a", "https://www.semanticscholar.org/paper/Large-scale-Semantic-Parsing-via-Schema-Matching-Cai-Yates/80c2d8c691b09f8b4e53f512b9d2641b49fda935"]},
{"id": "008d80bea26056ced48daac9834abe8ddf95d651", "title": "Factoid Question Answering over Unstructured and Structured Web Content", "authors": ["Silviu Cucerzan", "Eugene Agichtein"], "date": "2005", "abstract": "We describe our experience with two new, builtfrom-scratch, web-based question answering systems applied to the TREC 2005 Main Question Answering task, which use complementary models of answering questions over both structured and unstructured content on the Web. Our approaches depart from previous question answering (QA) work in several ways. For unstructured content, we used a web-based system with novel features such as web snippet pattern matching and generic answer type matching using web counts. We also experimented with a new, complementary question answering approach that uses information from the millions of tables and lists that abound on the web. This system attempts to answer factoid questions by guessing relevant rows and fields in matching web tables and integrating the results. We believe a combination of the two approaches holds promise.", "references": ["https://www.semanticscholar.org/paper/Web-question-answering%3A-is-more-always-better-Dumais-Banko/e8f9537a6cfb1ba2501c1c6ac3b114c274534095", "https://www.semanticscholar.org/paper/Scaling-question-answering-to-the-web-Kwok-Etzioni/016e9cc85c658c6a69710b4c617609ad2a5d3a74", "https://www.semanticscholar.org/paper/Mining-the-web-for-answers-to-natural-language-Radev-Qi/44e4081054990b613347d2d607ed218deee40c85", "https://www.semanticscholar.org/paper/Is-question-answering-an-acquired-skill-Ramakrishnan-Chakrabarti/84ce60af5fc862fb199ecdbea28136305e2de234", "https://www.semanticscholar.org/paper/Web-scale-information-extraction-in-knowitall%3A-Etzioni-Cafarella/9ef07373873cc0f0b940512dcdde4e7b54b0cfb0", "https://www.semanticscholar.org/paper/Snowball%3A-extracting-relations-from-large-Agichtein-Gravano/cee045e890270abae65455667b292db355d53728", "https://www.semanticscholar.org/paper/Automated-Discovery-of-WordNet-Relations-Hearst/364d793a1d268e94f30d96a5da77d7ee49eb1d09", "https://www.semanticscholar.org/paper/Probe%2C-cluster%2C-and-discover%3A-focused-extraction-of-Caverlee-Liu/1f5bde149c4220be9cd6f1f79c864648cf76f1c9", "https://www.semanticscholar.org/paper/Using-the-Web-to-Overcome-Data-Sparseness-Keller-Lapata/79883c30922037c93392ddbbecc6fd35674a6a1c", "https://www.semanticscholar.org/paper/Improving-trigram-language-modeling-with-the-World-Zhu-Rosenfeld/227f1a1d39e3b816179f7fd02a5d75454c179a8d"]},
{"id": "97cc2fab13bf20d130de19dfffe6670aac2076b5", "title": "The Fellowship of the Ring: Being the first part of The Lord of the Rings", "authors": ["J. R. R. Tolkien"], "date": "1954", "abstract": "Continuing the story begun in The Hobbit, this is the first part of Tolkien's epic masterpiece, The Lord of the Rings, featuring a striking black cover based on Tolkien's own design, the definitive text, and a detailed map of Middle-earth. Sauron, the Dark Lord, has gathered to him all the Rings of Power - the means by which he intends to rule Middle-earth. All he lacks in his plans for dominion is the One Ring - the ring that rules them all - which has fallen into the hands of the hobbit, Bilbo Baggins. In a sleepy village in the Shire, young Frodo Baggins finds himself faced with an immense task, as his elderly cousin Bilbo entrusts the Ring to his care. Frodo must leave his home and make a perilous journey across Middle-earth to the Cracks of Doom, there to destroy the Ring and foil the Dark Lord in his evil purpose. Part of a set of three paperbacks, this popular edition is once again available in its classic black livery designed by Tolkien himself.", "references": []},
{"id": "759956bb98689dbcc891528636d8994e54318f85", "title": "Strategies for Training Large Vocabulary Neural Language Models", "authors": ["Wenlin Chen", "David Grangier", "Michael Auli"], "date": "2016", "abstract": "Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.", "references": ["https://www.semanticscholar.org/paper/On-Using-Very-Large-Target-Vocabulary-for-Neural-Jean-Cho/1938624bb9b0f999536dcc8d8f519810bb4e1b3b", "https://www.semanticscholar.org/paper/A-fast-and-simple-algorithm-for-training-neural-Mnih-Teh/5b0d644f5c4b9880cbaf79932c0a4fa98996f068", "https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a", "https://www.semanticscholar.org/paper/Decoding-with-Large-Scale-Neural-Language-Models-Vaswani-Zhao/71480da09af638260801af1db8eff6acb4e1122f", "https://www.semanticscholar.org/paper/Exploring-the-Limits-of-Language-Modeling-J%C3%B3zefowicz-Vinyals/2f2d8f8072e5cc9b296fad551f65f183bdbff7aa", "https://www.semanticscholar.org/paper/Continuous-Space-Translation-Models-with-Neural-Le-Allauzen/1401b98a6eb0b0ddaf5bdfd559ed1766434446b8", "https://www.semanticscholar.org/paper/Deep-Neural-Network-Language-Models-Arisoy-Sainath/a17745f1d7045636577bcd5d513620df5860e9e5", "https://www.semanticscholar.org/paper/Large%2C-Pruned-or-Continuous-Space-Language-Models-a-Schwenk-Rousseau/e1f27c00ead87b2bde0e1a4ef732677dd9b989ff", "https://www.semanticscholar.org/paper/Adaptive-Importance-Sampling-to-Accelerate-Training-Bengio-Senecal/699d5ab38deee78b1fd17cc8ad233c74196d16e9", "https://www.semanticscholar.org/paper/One-billion-word-benchmark-for-measuring-progress-Chelba-Mikolov/5d833331b0e22ff359db05c62a8bca18c4f04b68"]},
{"id": "b29447ba499507a259ae9d8f685d60cc1597d7d3", "title": "Semantic Parsing on Freebase from Question-Answer Pairs", "authors": ["Jonathan Berant", "Andrew Chou", "Roy Frostig", "Percy Liang"], "date": "2013", "abstract": "In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline.", "references": ["https://www.semanticscholar.org/paper/Learning-Dependency-Based-Compositional-Semantics-Liang-Jordan/3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "https://www.semanticscholar.org/paper/Weakly-Supervised-Training-of-Semantic-Parsers-Krishnamurthy-Mitchell/2ffc1d8ec7b86a01b047d2c1ce66708a496a3f8a", "https://www.semanticscholar.org/paper/Paraphrase-Driven-Learning-for-Open-Question-Fader-Zettlemoyer/c0be2ac2f45681f1852fc1d298af5dceb85834f4", "https://www.semanticscholar.org/paper/Driving-Semantic-Parsing-from-the-World's-Response-Clarke-Goldwasser/92fb5e045bb23f13c11d8bb277925013b24b5930", "https://www.semanticscholar.org/paper/Template-based-question-answering-over-RDF-data-Unger-B%C3%BChmann/eb04bd49bdc2c9218624435eb277be2a973b21a5", "https://www.semanticscholar.org/paper/Grounded-Unsupervised-Semantic-Parsing-Poon/dd78a271722fb9e0eaade4208860398de1d80b7f", "https://www.semanticscholar.org/paper/Natural-Language-Questions-for-the-Web-of-Data-Yahya-Berberich/c1052027ddbacc24fc4a244d2c073f86aca62747", "https://www.semanticscholar.org/paper/Using-Semantic-Unification-to-Generate-Regular-from-Kushman-Barzilay/e510bae5437936c24c18dfb81e5659f9f08a9531", "https://www.semanticscholar.org/paper/Modeling-Relations-and-Their-Mentions-without-Text-Riedel-Yao/e7e7b9a731678bf0494fe29cbebb42a822224cc6", "https://www.semanticscholar.org/paper/Towards-a-theory-of-natural-language-interfaces-to-Popescu-Etzioni/7c7ba9df3ab69f0f3502a7a873c031c8244187ee"]},
{"id": "b56ab69d6d2ed840f3850df3c60ccdf1d5f284d9", "title": "Efficient Focusing and Face Detection", "authors": ["Yali Amit", "Donald Geman", "Bruno M. Jedynak"], "date": "1998", "abstract": "We present an algorithm for shape detection and apply it to frontal views of faces in still grey level images with arbitrary backgrounds. Detection is done in two stages: (i) \u201cfocusing,\u201d during which a relatively small number of regions-of-interest are identified, minimizing computation and false negatives at the (temporary) expense of false positives; and (ii) \u201cintensive classification,\u201d during which a selected region-of-interest is labeled face or background based on multiple decision trees and normalized data. In contrast to most detection algorithms, the processing is then very highly concentrated in the regions near faces and near false positives.", "references": []},
{"id": "2b776119a1347e1455dc498ff5078b3a94029ed9", "title": "Towards Understanding Situated Natural Language", "authors": ["Antoine Bordes", "Nicolas Usunier", "Ronan Collobert", "Jason Weston"], "date": "2010", "abstract": "We present a general framework and learning algorithm for the task of concept labeling: each word in a given sentence has to be tagged with the unique physical entity (e.g. person, object or location) or abstract concept it refers to. Our method allows both world knowledge and linguistic information to be used during learning and prediction. We show experimentally that we can learn to use world knowledge to resolve ambiguities in language, such as word senses or reference resolution, without the use of handcrafted rules or features.", "references": ["https://www.semanticscholar.org/paper/Learning-Semantic-Correspondences-with-Less-Liang-Jordan/b3051b3e18eee9c498a2e94d0811d1a3551e64e4", "https://www.semanticscholar.org/paper/Learning-Language-Semantics-from-Ambiguous-Kate-Mooney/ee710964af56cdbe5f2d494343b06898cd3b87f1", "https://www.semanticscholar.org/paper/L0-The-first-five-years-of-an-automated-language-Feldman-Lakoff/21a76410305a62e9ddc7c9b42fe048c437fb3103", "https://www.semanticscholar.org/paper/Learning-Context-Dependent-Mappings-from-Sentences-Zettlemoyer-Collins/07216ee1119f61b351b69e94b2e7c3698d96b026", "https://www.semanticscholar.org/paper/A-unified-architecture-for-natural-language-deep-Collobert-Weston/57458bc1cffe5caa45a885af986d70f723f406b4", "https://www.semanticscholar.org/paper/Natural-language-understanding-Allen/ba6797b59c38702725ea0f3e6c141d471164662e", "https://www.semanticscholar.org/paper/Word-sense-disambiguation-with-pictures-Barnard-Johnson/f442de2e76cfcd42e791c750edd337e8b80730d9", "https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/Guided-Learning-for-Bidirectional-Sequence-Shen-Satta/6503a3d9fb204c2a08ecfcfe6ba5b815fc65a030", "https://www.semanticscholar.org/paper/On-the-Integration-of-Grounding-Language-and-Yu-Ballard/85a0613d2c8d908aa9c3c42a2f3295bbb6ef6a90"]},
{"id": "ca97fc4dadcc564e5081743b52a4d77031d1c177", "title": "Psychophysical support for a two-dimensional view interpolation theory of object recognition.", "authors": ["H H B\u00fclthoff", "Sarah Edelman"], "date": "1992", "abstract": "Does the human brain represent objects for recognition by storing a series of two-dimensional snapshots, or are the object models, in some sense, three-dimensional analogs of the objects they represent? One way to address this question is to explore the ability of the human visual system to generalize recognition from familiar to unfamiliar views of three-dimensional objects. Three recently proposed theories of object recognition--viewpoint normalization or alignment of three-dimensional models [Ullman, S. (1989) Cognition 32, 193-254], linear combination of two-dimensional views [Ullman, S. & Basri, R. (1990) Recognition by Linear Combinations of Models (Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge), A. I. Memo No. 1152], and view approximation [Poggio, T. & Edelman, S. (1990) Nature (London) 343, 263-266]--predict different patterns of generalization to unfamiliar views. We have exploited the conflicting predictions to test the three theories directly in a psychophysical experiment involving computer-generated three-dimensional objects. Our results suggest that the human visual system is better described as recognizing these objects by two-dimensional view interpolation than by alignment or other methods that rely on object-centered three-dimensional models.", "references": []},
{"id": "ffb199e36de4f34ea233a30d392fdcf0c3b25a14", "title": "Rule-Injection Hints as a Means of Improving Network Performance and Learning Time", "authors": ["S. C. Suddarth", "Y. L. Kergosien"], "date": "1990", "abstract": "Neural networks can be given \u201chints\u201d by increasing the number of parameters learned to include parameters related to the original relationship. The effect of this hint, whether applied to back-propagation learning or to more general types of pattern associators is to reduce training time and improve generalization performance. A detailed vector field analysis of a hinted back-propagation network solving the XOR problem, shows that the hint is capable of eliminating pathological local minima. A set-theory/functional entropy analysis shows that the hint can be applied to any learning mechanism that has an internal (\u201chidden\u201d) layer of processing. These analyses and tests conducted on a variety of problems using different types of networks demonstrate the potential of the hint as a method of controlling training in order to predictably train systems to effectively model data.", "references": []},
{"id": "b3e89f05876d47b9bd6ece225aaeee457a6824e8", "title": "Statistical Machine Translation", "authors": ["Miles Osborne"], "date": "2010", "abstract": "Statistical Machine Translation (SMT) deals with automati cally mapping sentences in one human language (for example French) into another huma n language (such as English). The first language is called the source and the second language is called the target. This process can be thought of as a stochastic process. Ther e are many SMT variants, depending upon how translation is modelled. S ome approaches are in terms of a string-to-string mapping, some use trees-to-s trings, and some use treeto-tree models. All share in common the central idea that tra nslation is automatic, with models estimated from parallel corpora (source-targe t pairs) and also from monolingual corpora (examples of target sentences).", "references": ["https://www.semanticscholar.org/paper/A-Hierarchical-Phrase-Based-Model-for-Statistical-Chiang/ad3d2f463916784d0c14a19936c1544309a0a440", "https://www.semanticscholar.org/paper/The-Mathematics-of-Statistical-Machine-Translation%3A-Brown-Pietra/ab7b5917515c460b90451e67852171a531671ab8", "https://www.semanticscholar.org/paper/Statistical-Phrase-Based-Translation-Koehn-Och/a4b828609b60b06e61bea7a4029cc9e1cad5df87", "https://www.semanticscholar.org/paper/Bleu%3A-a-Method-for-Automatic-Evaluation-of-Machine-Papineni-Roukos/d7da009f457917aa381619facfa5ffae9329a6e9"]},
{"id": "f45269f092e15c1fb21eebace7f6ac56b88ad8af", "title": "The ICSI Meeting Recorder Dialog Act (MRDA) Corpus", "authors": ["Elizabeth Shriberg", "Rajdip Dhillon", "Sonali Bhagat", "Jeremy Ang", "Hannah Carvey"], "date": "2004", "abstract": "Abstract : We describe a new corpus of over 180,000 hand- annotated dialog act tags and accompanying adjacency pair annotations for roughly 72 hours of speech from 75 naturally-occurring meetings. We provide a brief summary of the annotation system and labeling procedure, inter-annotator reliability statistics, overall distributional statistics, a description of auxiliary files distributed with the corpus, and information on how to obtain the data.", "references": ["https://www.semanticscholar.org/paper/Meeting-Recorder-Project%3A-Dialog-Act-Labeling-Guide-Dhillon-Bhagat/e8cb114b328711d2502e3502f29830562fd2b46c", "https://www.semanticscholar.org/paper/The-ICSI-Meeting-Corpus-Janin-Baron/ff7804a13efc26bf23ce813319641db69ddbb969", "https://www.semanticscholar.org/paper/Multi-level-Dialogue-Act-Tags-Clark-Popescu-Belis/70a18ff066678b4ae98bb6577456ce9b037c15aa", "https://www.semanticscholar.org/paper/Coding-Dialogs-with-the-DAMSL-Annotation-Scheme-Core-Allen/ac2b71f9dbafb5678334a41a05cdf928e2dac447", "https://www.semanticscholar.org/paper/CLARITY%3A-INFERRING-DISCOURSE-STRUCTURE-FROM-SPEECH-Finke-Lapata/0449e77dc1975aa8034270bba666bca306b2c5b4", "https://www.semanticscholar.org/paper/Relationship-between-dialogue-acts-and-hot-spots-in-Wrede-Shriberg/827189ecd7519f44673e50a608458c932ebdc3df", "https://www.semanticscholar.org/paper/Advances-in-automatic-meeting-record-creation-and-Waibel-Bett/03856fbdb681403200dcb122ac519be9eaf8d395", "https://www.semanticscholar.org/paper/Research-methodologies%2C-observations-and-outcomes-Cieri-Miller/90ead4672a53c9a7235e8a0403973c64448add8f", "https://www.semanticscholar.org/paper/The-Hcrc-Map-Task-Corpus-Anderson-Bader/369c1dceb16d2ae051b8190e44ad463fda334eaa", "https://www.semanticscholar.org/paper/Assessing-Agreement-on-Classification-Tasks%3A-The-Carletta/613b6c9a85ae338cd3b405dc019c8edb1c15717c"]},
{"id": "bc7308a97ec2d3f7985d48671abe7a8942a5b9f8", "title": "Sentiment Analysis using Support Vector Machines with Diverse Information Sources", "authors": ["Tony Mullen", "Nigel Collier"], "date": "2004", "abstract": "This paper introduces an approach to sentiment analysis which uses support vector machines (SVMs) to bring together diverse sources of potentially pertinent information, including several favorability measures for phrases and adjectives and, where available, knowledge of the topic of the text. Models using the features introduced are further combined with unigram models which have been shown to be effective in the past (Pang et al., 2002) and lemmatized versions of the unigram models. Experiments on movie review data from Epinions.com demonstrate that hybrid SVMs which combine unigram-style feature-based SVMs with those based on real-valued favorability measures obtain superior performance, producing the best results yet published using this data. Further experiments using a feature set enriched with topic information on a smaller dataset of music reviews handannotated for topic are also reported, the results of which suggest that incorporating topic information into such models may also yield improvement.", "references": ["https://www.semanticscholar.org/paper/Sentiment-analysis%3A-capturing-favorability-using-Nasukawa-Yi/ababc1999b5f31409c78c39d1842219821e37a6d", "https://www.semanticscholar.org/paper/Thumbs-up-Sentiment-Classification-using-Machine-Pang-Lee/12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "https://www.semanticscholar.org/paper/Learning-Subjective-Adjectives-from-Corpora-Wiebe/5581992944c66522dd1b11f8a6150aeef2d95b7a", "https://www.semanticscholar.org/paper/Learning-to-classify-text-using-support-vector-and-Joachims/248a297d786228a183fcae64023092660550fcd2", "https://www.semanticscholar.org/paper/Thumbs-Up-or-Thumbs-Down-Semantic-Orientation-to-of-Turney/9e7c7853a16a378cc24a082153b282257a9675b7", "https://www.semanticscholar.org/paper/Word-Association-Norms%2C-Mutual-Information-and-Church-Hanks/9e2caa39ac534744a180972a30a320ad0ae41ea3", "https://www.semanticscholar.org/paper/An-Introduction-to-Support-Vector-Machines-and-Cristianini-Shawe-Taylor/5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53", "https://www.semanticscholar.org/paper/Measuring-praise-and-criticism%3A-Inference-of-from-Turney-Littman/5c8d05e27e36ebd64ee43fe1670262cdcc2123ba", "https://www.semanticscholar.org/paper/Words-with-attitude-Kamps-Marx/d27f2fce889b3fd4da8fef2b40d47f9651e84419", "https://www.semanticscholar.org/paper/Effects-of-Adjective-Orientation-and-Gradability-on-Hatzivassiloglou-Wiebe/3a8d4fd2c30e5031a574bc25363c8639912b3bbd"]},
{"id": "9e7c7853a16a378cc24a082153b282257a9675b7", "title": "Thumbs Up or Thumbs Down? Semantic Orientation Applied to Unsupervised Classification of Reviews", "authors": ["Peter D. Turney"], "date": "2002", "abstract": "This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., \"subtle nuances\") and a negative semantic orientation when it has bad associations (e.g., \"very cavalier\"). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word \"excellent\" minus the mutual information between the given phrase and the word \"poor\". A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74% when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84% for automobile reviews to 66% for movie reviews.", "references": ["https://www.semanticscholar.org/paper/Mining-the-Web-for-Synonyms%3A-PMI-IR-versus-LSA-on-Turney/e517e1645708e7b050787bb4734002ea194a1958", "https://www.semanticscholar.org/paper/Word-Association-Norms%2C-Mutual-Information-and-Church-Hanks/9e2caa39ac534744a180972a30a320ad0ae41ea3", "https://www.semanticscholar.org/paper/Predicting-the-Semantic-Orientation-of-Adjectives-Hatzivassiloglou-McKeown/b62b80384f402d38c3425db6a99899d1cb9c50c6", "https://www.semanticscholar.org/paper/Learning-Subjective-Adjectives-from-Corpora-Wiebe/5581992944c66522dd1b11f8a6150aeef2d95b7a", "https://www.semanticscholar.org/paper/A-solution-to-Plato's-problem%3A-The-latent-semantic-Landauer-Dumais/68dd4b89ce1407372a29d05ca9e4e1a2e0513617", "https://www.semanticscholar.org/paper/Direction-based-text-interpretation-as-an-access-Hearst/3c4e3462e3e9f0dd7b379f3e300ff47eefa803e5", "https://www.semanticscholar.org/paper/Text-based-intelligent-systems%3A-current-research-in-Jacobs/3a2d2c6d4b7b6ec9b9706873ca01bd3b4117dfb1", "https://www.semanticscholar.org/paper/A-Simple-Approach-to-Ordinal-Classification-Frank-Hall/76fc26939d73c565431e1f38107a4245e739284e", "https://www.semanticscholar.org/paper/Part-of-Speech-Tagging-Guidelines-for-the-Penn-(3rd-Santorini/a145854ede2f62098bf4e92de1584ab270b676c9", "https://www.semanticscholar.org/paper/Effects-of-Adjective-Orientation-and-Gradability-on-Hatzivassiloglou-Wiebe/3a8d4fd2c30e5031a574bc25363c8639912b3bbd"]},
{"id": "e8cb114b328711d2502e3502f29830562fd2b46c", "title": "Meeting Recorder Project: Dialog Act Labeling Guide", "authors": ["Rajdip Dhillon", "Sonali Bhagat", "Hannah Carvey", "Elizabeth Shriberg"], "date": "2004", "abstract": "Abstract : This labeling guide is adapted from work on the Switchboard recordings and the accompanying manual (Jurafsky et al. 1997). The Switchboard-DAMSL (SWBD-DAMSL) manual for labeling one-on-one phone conversations provided a useful starting point for the types of dialog acts (DAs) that arose in the ICSI meeting corpus. However, the tagset for labeling meetings presented here has been modified as necessary to better reflect the types of interaction we observed in multiparty face-to-face meetings. This guide consists of five major sections: Quick Reference Information, Segmentation, How to Label, Adjacency Pairs, and Tag Descriptions. The first section supplies definitions for terms used throughout this guide and contains the correspondence of the Meeting Recorder DA (MRDA) tagset, which is the tagset detailed within this guide, to the SWBD-DAMSL tagset. This section also contains the entire MRDA tagset organized into groups according to syntactic, semantic, pragmatic, and functional similarities of the utterances they mark. The section entitled Segmentation, as its name indicates, details the rules and guidelines governing what constitutes an utterance along with how to determine utterance boundaries. The third section, How to Label, provides instruction regarding label construction, the management of utterances requiring additional DAs or containing quotes, and the use of the annotation software. The section entitled Adjacency Pairs details how adjacency pairs are constructed and the rules governing their usage. The section entitled Tag Descriptions provides explanations of each tag within the MRDA tagset. Two appendices are also found within this guide. The first provides a labeled portion of a meeting and the second contains information regarding tags used for a select number of meetings.", "references": ["https://www.semanticscholar.org/paper/Switchboard-SWBD-DAMSL-shallow-discourse-function-Jurafsky-Shriberg/100ab5ac194a6a3bf12206ce477079a2a66733c1"]},
{"id": "d7da009f457917aa381619facfa5ffae9329a6e9", "title": "Bleu: a Method for Automatic Evaluation of Machine Translation", "authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "date": "2002", "abstract": "Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.", "references": ["https://www.semanticscholar.org/paper/Corpus-based-comprehensive-and-diagnostic-MT-and-Papineni-Roukos/3d4e0cdf981747af1d5c687e3c8238f791f95733", "https://www.semanticscholar.org/paper/The-ARPA-MT-Evaluation-Methodologies%3A-Evolution%2C-White-O'Connell/bf45f9e578cb4b43a2604d6149553ae8cfee3016", "https://www.semanticscholar.org/paper/Toward-Finely-Differentiated-Evaluation-Metrics-for-Hovy/baf3314a8883f8776557cc6c2007dbe195e7d132", "https://www.semanticscholar.org/paper/Additional-mt-eval-references-Reeder/3ecbfa0f89ba42dad7a63ea8cada9fd16bbd1f96", "https://www.semanticscholar.org/paper/Proficiency-and-Performance-in-Language-Testing.-Child/682cdcae291aee14bfc49a038b65c7f7701212ae"]},
{"id": "ec179f9b30d3a4e0f8a50c86d951557dfcbb20d1", "title": "Empirical Studies on the Disambiguation of Cue Phrases", "authors": ["Julia Hirschberg", "Diane J. Litman"], "date": "1993", "abstract": "Cue phrases are linguistic expressions such as now and well that function as explicit indicators of the structure of a discourse. For example, now may signal the beginning of a subtopic or a return to a previous topic, while well may mark subsequent material as a response to prior material, or as an explanatory comment. However, while cue phrases may convey discourse structure, each also has one or more alternate uses. While incidentally may be used sententially as an adverbial, for example, the discourse use initiates a digression. Although distinguishing discourse and sentential uses of cue phrases is critical to the interpretation and generation of discourse, the question of how speakers and hearers accomplish this disambiguation is rarely addressed.This paper reports results of empirical studies on discourse and sentential uses of cue phrases, in which both text-based and prosodic features were examined for disambiguating power. Based on these studies, it is proposed that discourse versus sentential usage may be distinguished by intonational features, specifically, pitch accent and prosodic phrasing. A prosodic model that characterizes these distinctions is identified. This model is associated with features identifiable from text analysis, including orthography and part of speech, to permit the application of the results of the prosodic analysis to the generation of appropriate intonational features for discourse and sentential uses of cue phrases in synthetic speech.", "references": ["https://www.semanticscholar.org/paper/Disambiguating-Cue-Phrases-in-Text-and-Speech-Litman-Hirschberg/f571572296ea90d4e2b2c35fab8f692dfad35823", "https://www.semanticscholar.org/paper/Now-let's-Talk-about-Now%3B-Identifying-Cue-Phrases-Hirschberg-Litman/3dbadff91d174ef937420d61130418138b2749d3", "https://www.semanticscholar.org/paper/Coherence-and-Coreference-Hobbs/f2e1d62340d111dacb3b1038eb0a8676df045566", "https://www.semanticscholar.org/paper/The-Meaning-of-Intonational-Contours-in-the-of-Pierrehumbert-Hirschberg/a87ca203325c7c2a57e319e069a36afde4a7d46a", "https://www.semanticscholar.org/paper/Attention%2C-Intentions%2C-and-the-Structure-of-Grosz-Sidner/0b144f3ca00d98df1d8f9456d29c7fce3290924d", "https://www.semanticscholar.org/paper/The-representation-and-use-of-focus-in-dialogue-Grosz/987767d4d3652be8e78ba8e6ac0658b7ab1fd432", "https://www.semanticscholar.org/paper/The-intonational-Structuring-of-Discourse-Hirschberg-Pierrehumbert/f2e90663c51f7be1505627b72f4f53bbaba70614", "https://www.semanticscholar.org/paper/Relational-propositions-in-discourse-Mann-Thompson/71cb2124efb540c786b8d5933edfa8278a06fb34", "https://www.semanticscholar.org/paper/Towards-using-Prosody-In-Speech-Systems%3A-Between-Silverman-Blaauw/06e853df1f68b3150192f06ae506be75902d1982", "https://www.semanticscholar.org/paper/A-Computational-Theory-Of-The-Function-Of-Clue-In-Cohen/a40117bb772c0b1c988a23418212f923ee2b6bd7"]},
{"id": "abba83b5747e98d10ab982df9ae94cc799668f16", "title": "Simple Coreference Resolution with Rich Syntactic and Semantic Features", "authors": ["Aria Haghighi", "Dan Klein"], "date": "2009", "abstract": "Coreference systems are driven by syntactic, semantic, and discourse constraints. We present a simple approach which completely modularizes these three aspects. In contrast to much current work, which focuses on learning and on the discourse component, our system is deterministic and is driven entirely by syntactic and semantic compatibility as learned from a large, unlabeled corpus. Despite its simplicity and discourse naivete, our system substantially outperforms all unsupervised systems and most supervised ones. Primary contributions include (1) the presentation of a simple-to-reproduce, high-performing baseline and (2) the demonstration that most remaining errors can be attributed to syntactic and semantic factors external to the coreference phenomenon (and perhaps best addressed by non-coreference systems).", "references": ["https://www.semanticscholar.org/paper/Improving-Machine-Learning-Approaches-to-Resolution-Ng-Gardent/08c81389b3ac4b8253d718a7cebe04a5536efa78", "https://www.semanticscholar.org/paper/Understanding-the-Value-of-Features-for-Coreference-Bengtson-Roth/454bfea18aebae023e9a716503e3a9956dcea8b4", "https://www.semanticscholar.org/paper/First-Order-Probabilistic-Models-for-Coreference-Culotta-Wick/a957e6cbc55f004f94b468f23c5149f1b0fd3389", "https://www.semanticscholar.org/paper/Unsupervised-Models-for-Coreference-Resolution-Ng/ed2f04fc0164e64533affc4acffe80bf934e3440", "https://www.semanticscholar.org/paper/Joint-Unsupervised-Coreference-Resolution-with-Poon-Domingos/ddde3c7371a8d3d8fbbf53a37aba96fa9b0ce1db", "https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/Unsupervised-Coreference-Resolution-in-a-Bayesian-Haghighi-Klein/d0e498810f28462ea4d172f1f6000b7c1cf2870c", "https://www.semanticscholar.org/paper/A-model-theoretic-coreference-scoring-scheme-Vilain-Burger/890c1201d3edef7104ba9d75078ed90f9c30c93b", "https://www.semanticscholar.org/paper/Enforcing-Transitivity-in-Coreference-Resolution-Finkel-Manning/f6b34966f9b1f3b3bc21f16e8a1867ccc091b8fc", "https://www.semanticscholar.org/paper/On-Coreference-Resolution-Performance-Metrics-Luo/de133c1f22d0dfe12539e25dda70f28672459b99"]},
{"id": "364d793a1d268e94f30d96a5da77d7ee49eb1d09", "title": "Automated Discovery of WordNet Relations", "authors": ["Marti A. Hearst"], "date": "2004", "abstract": "The WordNet lexical database is now quite large and offers broad coverage of general lexical relations in English. As is evident in this volume, WordNet has been employed as a resource for many applications in natural language processing (NLP) and information retrieval (IR). However, many potentially useful lexical relations are currently missing from WordNet. Some of these relations, while useful for NLP and IR applications, are not necessarily appropriate for a general, domain-independent lexical database. For example, WordNet\u2019s coverage of proper nouns is rather sparse, but proper nouns are often very important in application tasks. The standard way lexicographers find new relations is to look through huge lists of concordance lines. However, culling through long lists of concordance lines can be a rather daunting task (Church and Hanks, 1990), so a method that picks out those lines that are very likely to hold relations of interest should be an improvement over more traditional techniques. This chapter describes a method for the automatic discovery of WordNetstyle lexico-semantic relations by searching for corresponding lexico-syntactic patterns in large text collections. Large text corpora are now widely available, and can be viewed as vast resources from which to mine lexical, syntactic, and semantic information. This idea is reminiscent of what is known as \u201cdata mining\u201d in the artificial intelligence literature (Fayyad and Uthurusamy, 1996), however, in this case the ore is raw text rather than tables of numerical data. The Lexico-Syntactic Pattern Extraction (LSPE) method is meant to be useful as an automated or semi-automated aid for lexicographers and builders of domain-dependent knowledge-bases. The LSPE technique is light-weight; it does not require a knowledge base or complex interpretation modules in order to suggest new WordNet relations.", "references": []},
{"id": "b2ac51e10a3510eadac5eac5e4fb828f086fab88", "title": "Scaling Semantic Parsers with On-the-Fly Ontology Matching", "authors": ["Tom Kwiatkowski", "Eunsol Choi", "Yoav Artzi", "Luke Zettlemoyer"], "date": "2013", "abstract": "We consider the challenge of learning semantic parsers that scale to large, open-domain problems, such as question answering with Freebase. In such settings, the sentences cover a wide variety of topics and include many phrases whose meaning is difficult to represent in a fixed target ontology. For example, even simple phrases such as \u2018daughter\u2019 and \u2018number of people living in\u2019 cannot be directly represented in Freebase, whose ontology instead encodes facts about gender, parenthood, and population. In this paper, we introduce a new semantic parsing approach that learns to resolve such ontological mismatches. The parser is learned from question-answer pairs, uses a probabilistic CCG to build linguistically motivated logicalform meaning representations, and includes an ontology matching model that adapts the output logical forms for each target ontology. Experiments demonstrate state-of-the-art performance on two benchmark semantic parsing datasets, including a nine point accuracy improvement on a recent Freebase QA corpus.", "references": ["https://www.semanticscholar.org/paper/Learning-Dependency-Based-Compositional-Semantics-Liang-Jordan/3ecd3e00bbbfd94446c3adc9c6878de27e250f7c", "https://www.semanticscholar.org/paper/Weakly-Supervised-Training-of-Semantic-Parsers-Krishnamurthy-Mitchell/2ffc1d8ec7b86a01b047d2c1ce66708a496a3f8a", "https://www.semanticscholar.org/paper/Driving-Semantic-Parsing-from-the-World's-Response-Clarke-Goldwasser/92fb5e045bb23f13c11d8bb277925013b24b5930", "https://www.semanticscholar.org/paper/Semantic-Parsing-Freebase%3A-Towards-Open-domain-Cai-Yates/86dd5d1493bbc0c72a739fba5a79f2582d8a497c", "https://www.semanticscholar.org/paper/Template-based-question-answering-over-RDF-data-Unger-B%C3%BChmann/eb04bd49bdc2c9218624435eb277be2a973b21a5", "https://www.semanticscholar.org/paper/Large-scale-Semantic-Parsing-via-Schema-Matching-Cai-Yates/80c2d8c691b09f8b4e53f512b9d2641b49fda935", "https://www.semanticscholar.org/paper/Natural-Language-Questions-for-the-Web-of-Data-Yahya-Berberich/c1052027ddbacc24fc4a244d2c073f86aca62747", "https://www.semanticscholar.org/paper/Ontological-Smoothing-for-Relation-Extraction-with-Zhang-Hoffmann/cbeb681d7a2685bec300407cc87011a2d0f30401", "https://www.semanticscholar.org/paper/Lexical-Generalization-in-CCG-Grammar-Induction-for-Kwiatkowski-Zettlemoyer/36d69fec4884389c1709d3ca74394cac814ce4a4", "https://www.semanticscholar.org/paper/Using-Semantic-Unification-to-Generate-Regular-from-Kushman-Barzilay/e510bae5437936c24c18dfb81e5659f9f08a9531"]},
{"id": "016e9cc85c658c6a69710b4c617609ad2a5d3a74", "title": "Scaling question answering to the web", "authors": ["Cody C. T. Kwok", "Oren Etzioni", "Daniel S. Weld"], "date": "2001", "abstract": "The wealth of information on the web makes it an attractive resource for seeking quick answers to simple, factual questions such as &quote;who was the first American in space?&quote; or &quote;what is the second tallest mountain in the world?&quote; Yet today's most advanced web search services (e.g., Google and AskJeeves) make it surprisingly tedious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance.First we introduce Mulder, which we believe to be the first general-purpose, fully-automated question-answering system available on the web. Second, we describe Mulder's architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare Mulder's performance to that of Google and AskJeeves on questions drawn from the TREC-8 question answering track. We find that Mulder's recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as Mulder.", "references": ["https://www.semanticscholar.org/paper/From-Sentence-Processing-to-Information-Access-on-Katz/df8568c6e19d427aae989887a47c3a88f8124dda", "https://www.semanticscholar.org/paper/Wordnet%3A-an-on-line-lexical-database-Miller-Beckwith/539a5738cbb215ec2b61c37a88631728faf21a6c"]},
{"id": "227f1a1d39e3b816179f7fd02a5d75454c179a8d", "title": "Improving trigram language modeling with the World Wide Web", "authors": ["Xiaojin Zhu", "Ronald Rosenfeld"], "date": "2001", "abstract": "We propose a method for using the World Wide Web to acquire trigram estimates for statistical language modeling. We submit an N-gram as a phrase query to Web search engines. The search engines return the number of Web pages containing the phrase, from which the N-gram count is estimated. The N-gram counts are then used to form Web-based trigram probability estimates. We discuss the properties of such estimates, and methods to interpolate them with traditional corpus based trigram estimates. We show that the interpolated models improve speech recognition word error rate significantly over a small test set.", "references": ["https://www.semanticscholar.org/paper/Just-in-time-language-modelling-Berger-Miller/0ccb664faaf4221dfe20c5b321d017ce33af3fec", "https://www.semanticscholar.org/paper/1998-TREC-7-Spoken-Document-Retrieval-Track-and-Garofolo-Voorhees/fffdbdaa05bcb63f05213b9b7abd8538f167aa0d"]},
{"id": "a17745f1d7045636577bcd5d513620df5860e9e5", "title": "Deep Neural Network Language Models", "authors": ["Ebru Arisoy", "Tara N. Sainath", "Brian Kingsbury", "Bhuvana Ramabhadran"], "date": "2012", "abstract": "In recent years, neural network language models (NNLMs) have shown success in both peplexity and word error rate (WER) compared to conventional n-gram language models. Most NNLMs are trained with one hidden layer. Deep neural networks (DNNs) with more hidden layers have been shown to capture higher-level discriminative information about input features, and thus produce better networks. Motivated by the success of DNNs in acoustic modeling, we explore deep neural network language models (DNN LMs) in this paper. Results on a Wall Street Journal (WSJ) task demonstrate that DNN LMs offer improvements over a single hidden layer NNLM. Furthermore, our preliminary results are competitive with a model M language model, considered to be one of the current state-of-the-art techniques for language modeling.", "references": ["https://www.semanticscholar.org/paper/Recurrent-neural-network-based-language-model-Mikolov-Karafi%C3%A1t/9819b600a828a57e1cde047bbe710d3446b30da5", "https://www.semanticscholar.org/paper/Structured-Output-Layer-neural-network-language-Le-Oparin/3aaa1e4974800767fcbd2c24c2f2af42bf412f97", "https://www.semanticscholar.org/paper/Training-Neural-Network-Language-Models-on-Very-Schwenk-Gauvain/8b395470a57c48d174c4216ea21a7a58bc046917", "https://www.semanticscholar.org/paper/Hierarchical-Probabilistic-Neural-Network-Language-Morin-Bengio/c19fbefdeead6a4154a22a9c8551a18b1530033a", "https://www.semanticscholar.org/paper/Continuous-space-language-models-Schwenk/0fcc184b3b90405ec3ceafd6a4007c749df7c363", "https://www.semanticscholar.org/paper/Deep-Belief-Networks-for-phone-recognition-Mohamed-Dahl/f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "https://www.semanticscholar.org/paper/Strategies-for-training-large-scale-neural-network-Mikolov-Deoras/cb45e9217fe323fbc199d820e7735488fca2a9b3", "https://www.semanticscholar.org/paper/A-Scalable-Hierarchical-Distributed-Language-Model-Mnih-Hinton/a9fc84f8abe740cdc7ee82e69444d1d00dbe0ceb", "https://www.semanticscholar.org/paper/Extensions-of-recurrent-neural-network-language-Mikolov-Kombrink/07ca885cb5cc4328895bfaec9ab752d5801b14cd", "https://www.semanticscholar.org/paper/Feature-engineering-in-Context-Dependent-Deep-for-Seide-Li/7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9"]},
{"id": "07216ee1119f61b351b69e94b2e7c3698d96b026", "title": "Learning Context-Dependent Mappings from Sentences to Logical Form", "authors": ["Luke Zettlemoyer", "Michael Collins"], "date": "2009", "abstract": "We consider the problem of learning context-dependent mappings from sentences to logical form. The training examples are sequences of sentences annotated with lambda-calculus meaning representations. We develop an algorithm that maintains explicit, lambda-calculus representations of salient discourse entities and uses a context-dependent analysis pipeline to recover logical forms. The method uses a hidden-variable variant of the perception algorithm to learn a linear model used to select the best analysis. Experiments on context-dependent utterances from the ATIS corpus show that the method recovers fully correct logical forms with 83.7% accuracy.", "references": ["https://www.semanticscholar.org/paper/Learning-to-Map-Sentences-to-Logical-Form%3A-with-Zettlemoyer-Collins/74fe7ec751cd50295b15cfd46389a8fefb37c414", "https://www.semanticscholar.org/paper/A-Statistical-Semantic-Parser-that-Integrates-and-Ge-Mooney/7ae207cfaf01dc2b6799da67f454190b34994870", "https://www.semanticscholar.org/paper/Online-Learning-of-Relaxed-CCG-Grammars-for-Parsing-Zettlemoyer-Collins/774113732db34ce0b797fc3dcceded811fb6edbc", "https://www.semanticscholar.org/paper/Learning-Synchronous-Grammars-for-Semantic-Parsing-Wong-Mooney/c2ecc66c0e5f976b0e0d95c64ed2d1e283a2625d", "https://www.semanticscholar.org/paper/Learning-to-Parse-Database-Queries-Using-Inductive-Zelle-Mooney/b7c0e47f8b768258b7d536c21b218e6c46ab8791", "https://www.semanticscholar.org/paper/Unsupervised-Lexical-Learning-with-Categorical-the-Watkinson-Manandhar/20102c9e4c290cc81420207cb6217af1000fdfa8", "https://www.semanticscholar.org/paper/A-Fully-Statistical-Approach-to-Natural-Language-Miller-Stallard/e9c1f510bcf5933d3cf8ec8108a04a9ba601a843", "https://www.semanticscholar.org/paper/Hierarchical-feature-based-translation-for-scalable-Ramaswamy-Kleindienst/f2ffd8273d01948310feda2771943673aae4fe6f", "https://www.semanticscholar.org/paper/Wide-Coverage-Semantic-Representations-from-a-CCG-Bos-Clark/1c7a1d0b183d0db47a438299d023684491461eac", "https://www.semanticscholar.org/paper/Recovery-Strategies-for-Parsing-Extragrammatical-Carbonell-Hayes/b33e8093c332f185f8c7e074b73cd7c79838b16a"]},
{"id": "f442de2e76cfcd42e791c750edd337e8b80730d9", "title": "Word sense disambiguation with pictures", "authors": ["Kobus Barnard", "Matthew L. Johnson", "David A. Forsyth"], "date": "2003", "abstract": "We introduce a method for using images for word sense disambiguation, either alone, or in conjunction with traditional text based methods. The approach is based in recent work on a method for predicting words for images which can be learned from image datasets with associated text. When word prediction is constrained to a narrow set of choices such as possible senses, it can be quite reliable, and we use these predictions either by themselves or to reinforce standard methods. We provide preliminary results on a subset of the Corel image database which has three to five keywords per image. The subset was automatically selected to have a greater portion of keywords with sense ambiguity and the word senses were hand labeled to provide ground truth for testing. Results on this data strongly suggest that images can help with word sense disambiguation.", "references": []},
{"id": "a20bfec3c95aad003dcb45a21a220c19cca8bb66", "title": "A Machine Learning Approach to Coreference Resolution of Noun Phrases", "authors": ["Wee Meng Soon", "Hwee Tou Ng", "Chung Yong Lim"], "date": "2001", "abstract": "In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of organization, person, or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.", "references": ["https://www.semanticscholar.org/paper/Noun-Phrase-Coreference-as-Clustering-Cardie-Wagstaff/b74454ae15a72d08e3c401fd28c15afc0a663066", "https://www.semanticscholar.org/paper/A-trainable-approach-to-coreference-resolution-for-McCarthy-Lehnert/ae130776725555ec7b4683fd8089a7d55d66e083", "https://www.semanticscholar.org/paper/A-Statistical-Approach-to-Anaphora-Resolution-Ge-Hale/71c0698edd0cf489cd837c91ad22bbf51643bf6c", "https://www.semanticscholar.org/paper/CogNIAC%3A-high-precision-coreference-with-limited-Baldwin/8299a3ba1677224927f4f152664b5b59520c5838", "https://www.semanticscholar.org/paper/Using-Decision-Trees-for-Coreference-Resolution-McCarthy-Lehnert/02537122c7f00d63c8c9c861791a2026a4879e33", "https://www.semanticscholar.org/paper/Evaluating-Automated-and-Manual-Acquisition-of-Aone-Bennett/6f7ac41dc9321cb7478919de1da6da70009023da", "https://www.semanticscholar.org/paper/An-Algorithm-for-Pronominal-Anaphora-Resolution-Lappin-Leass/f977f5e25adef5f0569cb9eb9b930e5146be2571", "https://www.semanticscholar.org/paper/An-Algorithm-that-Learns-What's-in-a-Name-Bikel-Schwartz/927abef52678ed23edf6c508ef7a26569440a329", "https://www.semanticscholar.org/paper/Factors-in-anaphora-resolution%3A-they-are-not-the-A-Mitkov/c1e0017518042b6f7efd6f62b6b40e115c2a961c", "https://www.semanticscholar.org/paper/Overview-of-Results-of-the-MUC-6-Evaluation-Sundheim/f10967fa1863089553067b2413ca0af26d35081d"]},
{"id": "e510bae5437936c24c18dfb81e5659f9f08a9531", "title": "Using Semantic Unification to Generate Regular Expressions from Natural Language", "authors": ["Nate Kushman", "Regina Barzilay"], "date": "2013", "abstract": "We consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof-the-art semantic parsing baseline, yielding a 29% absolute improvement in accuracy. 1", "references": ["https://www.semanticscholar.org/paper/Learning-to-Transform-Natural-to-Formal-Languages-Kate-Wong/8dd9fd6a45afd266d48255c398429e01ea4fd6db", "https://www.semanticscholar.org/paper/Inducing-Probabilistic-CCG-Grammars-from-Logical-Kwiatkowski-Zettlemoyer/c7a40c3ef180d847bb3db40fd01990e08a6264f7", "https://www.semanticscholar.org/paper/Learning-to-Map-Sentences-to-Logical-Form%3A-with-Zettlemoyer-Collins/74fe7ec751cd50295b15cfd46389a8fefb37c414", "https://www.semanticscholar.org/paper/A-Multilingual-Natural-Language-Interface-to-Ranta/2b7604bcd0507d5abf914d798a4911f9c0b6e3a8", "https://www.semanticscholar.org/paper/Learning-for-Semantic-Parsing-with-Statistical-Wong-Mooney/687dce9ac01f5996601655035c34b449c27c3b6a", "https://www.semanticscholar.org/paper/Discriminative-Reranking-for-Semantic-Parsing-Ge-Mooney/4736635da3d32f8ff18ddc62552e180e303f47b6", "https://www.semanticscholar.org/paper/Driving-Semantic-Parsing-from-the-World's-Response-Clarke-Goldwasser/92fb5e045bb23f13c11d8bb277925013b24b5930", "https://www.semanticscholar.org/paper/Using-String-Kernels-for-Learning-Semantic-Parsers-Kate-Mooney/64a6439ed59e3c1dd54e778450f4758a59a2b0e0", "https://www.semanticscholar.org/paper/Acquiring-Word-Meaning-Mappings-for-Natural-Thompson-Mooney/cbd3bb68861808eadd8c0ef2734fd0cd666305d9", "https://www.semanticscholar.org/paper/NLP-(Natural-Language-Processing)-for-NLP-(Natural-Mihalcea-Liu/b1a6ec04c658e0069def94f04c3f2454b9a0f2f9"]},
{"id": "70a18ff066678b4ae98bb6577456ce9b037c15aa", "title": "Multi-level Dialogue Act Tags", "authors": ["Alexander Clark", "Andrei Popescu-Belis"], "date": "2004", "abstract": "In this paper we discuss the use of multi-layered tagsets for dialogue acts, in the context of dialogue understanding for multi-party meeting recording and retrieval applications. We discuss some desiderata for such tagsets and critically examine some previous proposals. We then define MALTUS, a new tagset based on the ICSI-MR and Switchboard tagsets, which satisfies these requirements. We present some experiments using MALTUS which attempt to compare the merits of integrated versus multi-level classifiers for the detection of dialogue acts.", "references": ["https://www.semanticscholar.org/paper/The-ICSI-Meeting-Recorder-Dialog-Act-(MRDA)-Corpus-Shriberg-Dhillon/f45269f092e15c1fb21eebace7f6ac56b88ad8af", "https://www.semanticscholar.org/paper/Natural-Language-Queries-on-Natural-Language-Data%3A-Armstrong-Clark/b74bd0e399876f3174de00966341c4abe5b4db27", "https://www.semanticscholar.org/paper/Automatically-Generated-Prosodic-Cues-to-Lexically-Bhagat-Carvey/4cc282494f2a22d26d107f9e756f16c1ad4caf04", "https://www.semanticscholar.org/paper/Meeting-Recorder-Project%3A-Dialog-Act-Labeling-Guide-Dhillon-Bhagat/e8cb114b328711d2502e3502f29830562fd2b46c", "https://www.semanticscholar.org/paper/Relationship-between-dialogue-acts-and-hot-spots-in-Wrede-Shriberg/827189ecd7519f44673e50a608458c932ebdc3df", "https://www.semanticscholar.org/paper/The-Reliability-of-a-Dialogue-Structure-Coding-Carletta-Isard/26e700cd2856a60cb82d5c94662564cda272ca69", "https://www.semanticscholar.org/paper/Dialogue-Act-Modeling-for-Automatic-Tagging-and-of-Stolcke-Ries/22d45dadde6b5837eff11dc031045754bc5901c3", "https://www.semanticscholar.org/paper/Meetings-about-meetings%3A-research-at-ICSI-on-speech-Morgan-Baron/dffbafb5c8d0ca39279f7f463765c991cd208895", "https://www.semanticscholar.org/paper/Coding-Dialogs-with-the-DAMSL-Annotation-Scheme-Core-Allen/ac2b71f9dbafb5678334a41a05cdf928e2dac447", "https://www.semanticscholar.org/paper/20-Questions-on-Dialogue-Act-Taxonomies-Traum/3972af9d462c743ca25b032bdfa89d3a202d9de1"]},
{"id": "ad3d2f463916784d0c14a19936c1544309a0a440", "title": "A Hierarchical Phrase-Based Model for Statistical Machine Translation", "authors": ["David Chiang"], "date": "2005", "abstract": "We present a statistical phrase-based translation model that uses hierarchical phrases---phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntax-based translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrase-based model achieves a relative improvement of 7.5% over Pharaoh, a state-of-the-art phrase-based system.", "references": ["https://www.semanticscholar.org/paper/Statistical-Phrase-Based-Translation-Koehn-Och/a4b828609b60b06e61bea7a4029cc9e1cad5df87", "https://www.semanticscholar.org/paper/Improvements-in-Phrase-Based-Statistical-Machine-Zens-Ney/8ceebe893cfb2dbf83d872523d9ddd140aa1649c", "https://www.semanticscholar.org/paper/Noun-phrase-translation-Knight-Koehn/3e4681ece0316438b9984097894fab3ef56d3b7a", "https://www.semanticscholar.org/paper/Improved-Statistical-Alignment-Models-Och-Ney/c9214ebe91454e6369720136ab7dd990d52a07d4", "https://www.semanticscholar.org/paper/Discriminative-Training-and-Maximum-Entropy-Models-Och-Ney/37fadfb6d60e83e24c72d8a90da5644b39d6e8f0", "https://www.semanticscholar.org/paper/The-Alignment-Template-Approach-to-Statistical-Och-Ney/c6a83c4fcc99ba6753109301949c5b7cfa978079", "https://www.semanticscholar.org/paper/Pharaoh%3A-A-Beam-Search-Decoder-for-Phrase-Based-Koehn/50b8e8d48f4973cdeefa835807b4e1a8ca65ced3", "https://www.semanticscholar.org/paper/A-Syntax-based-Statistical-Translation-Model-Yamada-Knight/11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "https://www.semanticscholar.org/paper/Two-Statistical-Parsing-Models-Applied-to-the-Bikel-Chiang/b5b0ab2d445a393341d292b26c8a9a6d01c62051", "https://www.semanticscholar.org/paper/A-weighted-finite-state-transducer-translation-for-Kumar-Deng/bad3c30a75a28dbb8ace36e8ddb8d65cd760dfe6"]},
{"id": "d27f2fce889b3fd4da8fef2b40d47f9651e84419", "title": "Words with attitude", "authors": ["Jaap Kamps", "Maarten Marx"], "date": "2002", "abstract": "The traditional notion of word meaning used in natural language processing is literal or lexical meaning as used in dictionaries and lexicons. This relatively objective notion of lexical meaning is different from more subjective notions of emotive or affective meaning. Our aim is to come to grips with subjective aspects of meaning expressed in written texts, such as the attitude or value expressed in them. This paper explores how the structure of the WordNet lexical database might be used to assess affective or emotive meaning. In particular, we construct measures based on Osgood\u2019s semantic differential technique.", "references": []},
{"id": "248a297d786228a183fcae64023092660550fcd2", "title": "Learning to classify text using support vector machines - methods, theory and algorithms", "authors": ["Thorsten Joachims"], "date": "2002", "abstract": "Based on ideas from Support Vector Machines (SVMs), Learning To Classify Text Using Support Vector Machines presents a new approach to generating text classifiers from examples. The approach combines high performance and efficiency with theoretical understanding and improved robustness. In particular, it is highly effective without greedy heuristic components. The SVM approach is computationally efficient in training and classification, and it comes with a learning theory that can guide real-world applications. Learning To Classify Text Using Support Vector Machines gives a complete and detailed description of the SVM approach to learning text classifiers, including training algorithms, transductive text classification, efficient performance estimation, and a statistical learning model of text classification. In addition, it includes an overview of the field of text classification, making it self-contained even for newcomers to the field. This book gives a concise introduction to SVMs for pattern recognition, and it includes a detailed description of how to formulate text-classification tasks for machine learning.", "references": []},
{"id": "5581992944c66522dd1b11f8a6150aeef2d95b7a", "title": "Learning Subjective Adjectives from Corpora", "authors": ["Janyce Wiebe"], "date": "2000", "abstract": "Subjectivity tagging is distinguishing sentences used to present opinions and evaluations from sentences used to objectively present factual information. There are numerous applications for which subjectivity tagging is relevant, including information extraction and information retrieval. This paper identifies strong clues of subjectivity using the results of a method for clustering words according to distributional similarity (Lin 1998), seeded by a small amount of detailed manual annotation. These features are then further refined with the addition of lexical semantic features of adjectives, specifically polarity and gradability (Hatzivassiloglou & McKeown 1997), which can be automatically learned from corpora. In 10-fold cross validation experiments, features based on both similarity clusters and the lexical semantic features are shown to have higher precision than features based on each alone.", "references": ["https://www.semanticscholar.org/paper/Effects-of-Adjective-Orientation-and-Gradability-on-Hatzivassiloglou-Wiebe/3a8d4fd2c30e5031a574bc25363c8639912b3bbd", "https://www.semanticscholar.org/paper/Recognizing-subjectivity%3A-a-case-study-in-manual-Bruce-Wiebe/7c89cbf5d860819c9b5e5217d079dc8aafcba336", "https://www.semanticscholar.org/paper/Inducing-a-Semantically-Annotated-Lexicon-via-Rooth-Riezler/7d0c052eabed016faeb1fba49dcd8ef6c551a79c", "https://www.semanticscholar.org/paper/Automatic-Retrieval-and-Clustering-of-Similar-Words-Lin/fd1901f34cc3673072264104885d70555b1a4cdc", "https://www.semanticscholar.org/paper/Building-a-Large-Annotated-Corpus-of-English%3A-The-Marcus-Santorini/0b44fcbeea9415d400c5f5789d6b892b6f98daff", "https://www.semanticscholar.org/paper/A-stochastic-parts-program-and-noun-phrase-parser-Church/a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "https://www.semanticscholar.org/paper/Building-task-specific-interfaces-to-high-volume-Terveen-Hill/2b87a30cb2923674cc54ace37489af772f781358", "https://www.semanticscholar.org/paper/A-Situated-Ontology-for-Practical-NLP-Mahesh-Nirenburg/86fe066f3dea849b795ec8ece8ee54c8f899b617", "https://www.semanticscholar.org/paper/Development-and-Use-of-a-Gold-Standard-Data-Set-for-Wiebe-Bruce/c4a974906a9f6c7380edc9e2281931bde78828b1", "https://www.semanticscholar.org/paper/PRINCIPAR-An-Efficient%2C-Broad-coverage%2C-Parser-Lin/8b8e5d7da8441f0d44dac7ac3b87050d653bfa1a"]},
{"id": "3a2d2c6d4b7b6ec9b9706873ca01bd3b4117dfb1", "title": "Text-based intelligent systems: current research and practice in information extraction and retrieval", "authors": ["Paul S. Jacobs"], "date": "1992", "abstract": "Contents: P.S. Jacobs, Introduction: Text Power and Intelligent Systems. Part I:Broad-Scale NLP. J.R. Hobbs, D.E. Appelt, J. Bear, M. Tyson, D. Magerman, Robust Processing of Real-World Natural-Language Texts. Y. Wilks, L. Guthrie, J. Guthrie, J. Cowie, Combining Weak Methods in Large-Scale Text Processing. G. Hirst, M. Ryan, Mixed-Depth Representations for Natural Language Text. D.D. McDonald, Robust Partial-Parsing Through Incremental, Multi-Algorithm Processing. Corpus-Based Thematic Analysis. Part II:\"Traditional\" Information Retrieval. W.B. Croft, H.R. Turtle, Text Retrieval and Inference. K.S. Jones, Assumptions and Issues in Text-Based Retrieval. D.D. Lewis, Text Representation for Intelligent Text Retrieval: A Classification-Oriented View. G. Salton, C. Buckley, Automatic Text Structuring Experiments. Part III:Emerging Applications. C. Stanfill, D.L. Waltz, Statistical Methods, Artificial Intelligence, and Information Retrieval. P.J. Hayes, Intelligent High-Volume Text Processing Using Shallow, Domain-Specific Techniques. Y.S. Maarek, Automatically Constructing Simple Help Systems from Natural Language Documentation. M.A. Hearst, Direction-Based Text Interpretation as an Information Access Refinement.", "references": ["https://www.semanticscholar.org/paper/On-the-Evaluation-of-IR-Systems-Robertson-Hancock-Beaulieu/15714c9f9d5ce616a107ae8cbc7fc30987592a5e", "https://www.semanticscholar.org/paper/Optimization-by-simulated-annealing.-Kirkpatrick-Gelatt/dd5061631a4d11fa394f4421700ebf7e78dcbc59", "https://www.semanticscholar.org/paper/Proceedings-of-the-third-message-understanding-Sundheim/ce56e59991e8b088a66048773d920102da2f3541"]},
{"id": "3dbadff91d174ef937420d61130418138b2749d3", "title": "Now let's Talk about Now; Identifying Cue Phrases Intonationally", "authors": ["Julia Hirschberg", "Diane J. Litman"], "date": "1987", "abstract": "Cue phrases are words and phrases such as now and by the way which may be used to convey explicit information about the structure of a discourse. However, while cue phrases may convey discourse structure, each may also be used to different effect. The question of how speakers and hearers distinguish between such uses of cue phrases has not been addressed in discourse studies to date. Based on a study of now in natural recorded discourse, we propose that cue and non-cue usage can be distinguished intonationally, on the basis of phrasing and accent.", "references": ["https://www.semanticscholar.org/paper/The-intonational-Structuring-of-Discourse-Hirschberg-Pierrehumbert/f2e90663c51f7be1505627b72f4f53bbaba70614", "https://www.semanticscholar.org/paper/A-Computational-Theory-Of-The-Function-Of-Clue-In-Cohen/a40117bb772c0b1c988a23418212f923ee2b6bd7", "https://www.semanticscholar.org/paper/Coherence-and-Coreference-Hobbs/f2e1d62340d111dacb3b1038eb0a8676df045566", "https://www.semanticscholar.org/paper/Relational-propositions-in-discourse-Mann-Thompson/71cb2124efb540c786b8d5933edfa8278a06fb34", "https://www.semanticscholar.org/paper/Dependencies-of-Discourse-Structure-on-the-Modality-Cohen-Fertig/bbebc24ebd5f678bb74ce62fa0236e8c11d9df2d", "https://www.semanticscholar.org/paper/The-representation-and-use-of-focus-in-dialogue-Grosz/987767d4d3652be8e78ba8e6ac0658b7ab1fd432", "https://www.semanticscholar.org/paper/Attention%2C-Intentions%2C-and-the-Structure-of-Grosz-Sidner/0b144f3ca00d98df1d8f9456d29c7fce3290924d", "https://www.semanticscholar.org/paper/Hesitation-and-semantic-planning-in-speech-Butterworth/397389558fbbc4d6801c4061f7ecde6b3ec2a4dd", "https://www.semanticscholar.org/paper/A-Plan-Recognition-Model-for-Subdialogues-in-Litman-Allen/c0c648dc61746be6e14edc6bcfc12ee3818886e9", "https://www.semanticscholar.org/paper/Comprehension-Driven-Generation-of-Meta-Technical-Zukerman-Pearl/cfeacaa2c653e34ffa6a7e39ce7c06b9809e5a15"]},
{"id": "df8568c6e19d427aae989887a47c3a88f8124dda", "title": "From Sentence Processing to Information Access on the World Wide Web", "authors": ["Boris Katz"], "date": "1997", "abstract": "This paper describes the START Information Server built at the MIT Artificial Intelligence Laboratory. Available on the World Wide Web since December 1993, the START Server provides users with access to multi-media information in response to questions formulated in English. Over the last 3 years, the START Server answered hundreds of thousands of questions from users all over the world. The START Server is built on two foundations: the sentence-level Natural Language processing capability provided by the START Natural Language system (Katz [1990])and the idea of natural language annotations for multi-media information segments. This paper starts with an overview of sentence-level processing in the START system and then explains how annotating information segments with collections of English sentences makes it possible to use the power of sentence-level natural language processing in the service of multi-media information access. The paper ends with a proposal to annotate the World Wide Web.", "references": ["https://www.semanticscholar.org/paper/An-Efficient-Easily-Adaptable-System-for-Natural-Warren-Pereira/42abe494a940aecf215ddc64ddd8827f077567ec", "https://www.semanticscholar.org/paper/Using-English-for-Indexing-and-Retrieving-Katz/27056c1fe6d6a34f4a0086d7abac333ecbe43871", "https://www.semanticscholar.org/paper/Exploiting-lexical-regularities-in-designing-Katz-Levin/1093084110ad9158dcebb14c391794d586de9b69", "https://www.semanticscholar.org/paper/Understanding-natural-language-for-spacecraft-Katz-Brooks/f781c87e4538a498d770a2a9c5c1c0d681e55b52", "https://www.semanticscholar.org/paper/The-TRAINS-Project-Allen-Schubert/760030861baea8154acc5dcd9da96e4220e54b7d", "https://www.semanticscholar.org/paper/Learning-New-Principles-from-Precedents-and-Winston/9e4e3ba827a7acfcf961196d9bae21a6518d0ed6", "https://www.semanticscholar.org/paper/SNePS-Considered-as-a-Fully-Intensional-Semantic-Shapiro-Rapaport/4720d56c3f8180e8677b4b2b0503f5b7e0349ec0", "https://www.semanticscholar.org/paper/Subjects%2C-Topics-and-Agents%3A-Evidence-from-Oosten/b6e946afbbf6b3a287459371ec826bb1daa48d9b"]},
{"id": "454bfea18aebae023e9a716503e3a9956dcea8b4", "title": "Understanding the Value of Features for Coreference Resolution", "authors": ["Eric Bengtson", "Dan Roth"], "date": "2008", "abstract": "In recent years there has been substantial work on the important problem of coreference resolution, most of which has concentrated on the development of new models and algorithmic techniques. These works often show that complex models improve over a weak pairwise baseline. However, less attention has been given to the importance of selecting strong features to support learning a coreference model. \n \nThis paper describes a rather simple pairwise classification model for coreference resolution, developed with a well-designed set of features. We show that this produces a state-of-the-art system that outperforms systems built with complex models. We suggest that our system can be used as a baseline for the development of more complex models -- which may have less impact when a more robust set of features is used. The paper also presents an ablation study and discusses the relative contributions of various features.", "references": ["https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/Improving-Machine-Learning-Approaches-to-Resolution-Ng-Gardent/08c81389b3ac4b8253d718a7cebe04a5536efa78", "https://www.semanticscholar.org/paper/Algorithms-for-Scoring-Coreference-Chains-Bagga-Baldwin/4b512f10838e05f5b2eee94bfbd20f3d9c4ecb9b", "https://www.semanticscholar.org/paper/First-Order-Probabilistic-Models-for-Coreference-Culotta-Wick/a957e6cbc55f004f94b468f23c5149f1b0fd3389", "https://www.semanticscholar.org/paper/Multi-Lingual-Coreference-Resolution-With-Syntactic-Luo-Zitouni/82bd33eada250831d54aa8bd9158a4b27b450367", "https://www.semanticscholar.org/paper/A-model-theoretic-coreference-scoring-scheme-Vilain-Burger/890c1201d3edef7104ba9d75078ed90f9c30c93b", "https://www.semanticscholar.org/paper/A-Mention-Synchronous-Coreference-Resolution-Based-Luo-Ittycheriah/4f8dd94c1a1454cc34475a4f533e137c7e4afd8d", "https://www.semanticscholar.org/paper/Joint-Determination-of-Anaphoricity-and-Coreference-Denis-Baldridge/7b9516f264fa75a87bfd4cc19318bbd1d26586a0", "https://www.semanticscholar.org/paper/Identifying-Anaphoric-and-Non-Anaphoric-Noun-to-Ng-Cardie/e414e980005c1066c539dd893a14188371792a01", "https://www.semanticscholar.org/paper/Using-Semantic-Relations-to-Refine-Coreference-Ji-Westbrook/b2589af6b6150f80f6a492ad0d519362dd981270"]},
{"id": "f6b34966f9b1f3b3bc21f16e8a1867ccc091b8fc", "title": "Enforcing Transitivity in Coreference Resolution", "authors": ["Jenny Rose Finkel", "Christopher D. Manning"], "date": "2008", "abstract": "A desirable quality of a coreference resolution system is the ability to handle transitivity constraints, such that even if it places high likelihood on a particular mention being coreferent with each of two other mentions, it will also consider the likelihood of those two mentions being coreferent when making a final assignment. This is exactly the kind of constraint that integer linear programming (ILP) is ideal for, but, surprisingly, previous work applying ILP to coreference resolution has not encoded this type of constraint. We train a coreference classifier over pairs of mentions, and show how to encode this type of constraint on top of the probabilities output from our pairwise classifier to extract the most probable legal entity assignments. We present results on two commonly used datasets which show that enforcement of transitive closure consistently improves performance, including improvements of up to 3.6% using the b3 scorer, and up to 16.5% using cluster f-measure.", "references": ["https://www.semanticscholar.org/paper/Joint-Determination-of-Anaphoricity-and-Coreference-Denis-Baldridge/7b9516f264fa75a87bfd4cc19318bbd1d26586a0", "https://www.semanticscholar.org/paper/Conditional-Models-of-Identity-Uncertainty-with-to-McCallum-Wellner/cbb786e7e42d02de0080299dc2e114357e816002", "https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/First-Order-Probabilistic-Models-for-Coreference-Culotta-Wick/a957e6cbc55f004f94b468f23c5149f1b0fd3389", "https://www.semanticscholar.org/paper/A-model-theoretic-coreference-scoring-scheme-Vilain-Burger/890c1201d3edef7104ba9d75078ed90f9c30c93b", "https://www.semanticscholar.org/paper/A-Mention-Synchronous-Coreference-Resolution-Based-Luo-Ittycheriah/4f8dd94c1a1454cc34475a4f533e137c7e4afd8d", "https://www.semanticscholar.org/paper/Learning-Noun-Phrase-Anaphoricity-to-Improve-Issues-Ng/a21b0262436d1a5013f4a64cc17fdc6ec5b57be7", "https://www.semanticscholar.org/paper/Improving-Machine-Learning-Approaches-to-Resolution-Ng-Gardent/08c81389b3ac4b8253d718a7cebe04a5536efa78", "https://www.semanticscholar.org/paper/Algorithms-for-Scoring-Coreference-Chains-Bagga-Baldwin/4b512f10838e05f5b2eee94bfbd20f3d9c4ecb9b", "https://www.semanticscholar.org/paper/Machine-Learning-for-Coreference-Resolution%3A-From-Ng/ce86b7b568758a8d245e3a31fcedaf82d23b0a80"]},
{"id": "539a5738cbb215ec2b61c37a88631728faf21a6c", "title": "Wordnet: an on-line lexical database", "authors": ["George A. Miller", "Richard Beckwith", "Christiane Fellbaum", "Daniel Gross", "Kimberly Miller"], "date": "1990", "abstract": "Semantic Scholar extracted view of \"Wordnet: an on-line lexical database\" by George A. Miller et al.", "references": []},
{"id": "e9c1f510bcf5933d3cf8ec8108a04a9ba601a843", "title": "A Fully Statistical Approach to Natural Language Interfaces", "authors": ["Scott Miller", "David Stallard", "Robert J. Bobrow", "Richard M. Schwartz"], "date": "1996", "abstract": "We present a natural language interface system which is based entirely on trained statistical models. The system consists of three stages of processing: parsing, semantic interpretation, and discourse. Each of these stages is modeled as a statistical process. The models are fully integrated, resulting in an end-to-end system that maps input utterances into meaning representation frames.", "references": ["https://www.semanticscholar.org/paper/TINA%3A-A-Natural-Language-System-for-Spoken-Language-Seneff/ac8f1fd58be8a8c9f9599fc4da981ea3040945f6", "https://www.semanticscholar.org/paper/Hidden-Understanding-Models-of-Natural-Language-Miller-Bobrow/70d2b25dfe4fbadfa3c816e01666f811dae9a4d1", "https://www.semanticscholar.org/paper/A-stochastic-parts-program-and-noun-phrase-parser-Church/a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "https://www.semanticscholar.org/paper/Developing-an-Evaluation-Methodology-for-Spoken-Bates-Boisen/55a1bf99b29436197fd4d872a78155a196a77a88", "https://www.semanticscholar.org/paper/Statistical-Decision-Tree-Models-for-Parsing-Magerman/f0a14be7e7f5614b91d0f648ae5f2baafc6d7036", "https://www.semanticscholar.org/paper/Evaluation-of-Spoken-Language-Systems%3A-the-ATIS-Price/8df509919b31397e225280962c59384fbe83144e", "https://www.semanticscholar.org/paper/The-estimation-of-powerful-language-models-from-and-Placeway-Schwartz/a8fcc058607129590fa6ab692a38807efc7d7f1f", "https://www.semanticscholar.org/paper/Benchmark-Tests-For-The-Darpa-Spoken-Language-Pallett-Fiscus/4b04d2f1f7e0d4eb51937e8980b01b5ed8876bd0", "https://www.semanticscholar.org/paper/An-Efficient-Probabilistic-Context-Free-Parsing-Stolcke/79fbfc1dc8846379074aaf4deb7fb0a96722eeed", "https://www.semanticscholar.org/paper/1993-Benchmark-Tests-for-the-ARPA-Spoken-Language-Pallett-Fiscus/71a286f8e6eeb611bd216f07bb749d3cc04ed2b7"]},
{"id": "08c81389b3ac4b8253d718a7cebe04a5536efa78", "title": "Improving Machine Learning Approaches to Coreference Resolution", "authors": ["Vincent Ng", "Claire Gardent"], "date": "2002", "abstract": "We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.", "references": ["https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/Text-and-Knowledge-Mining-for-Coreference-Harabagiu-Bunescu/e20d2d6c57032114ee57118ff620fdef7c548c46", "https://www.semanticscholar.org/paper/A-model-theoretic-coreference-scoring-scheme-Vilain-Burger/890c1201d3edef7104ba9d75078ed90f9c30c93b", "https://www.semanticscholar.org/paper/Using-Decision-Trees-for-Coreference-Resolution-McCarthy-Lehnert/02537122c7f00d63c8c9c861791a2026a4879e33", "https://www.semanticscholar.org/paper/Evaluating-Automated-and-Manual-Acquisition-of-Aone-Bennett/6f7ac41dc9321cb7478919de1da6da70009023da", "https://www.semanticscholar.org/paper/The-NYU-system-for-MUC-6-or-where's-the-syntax-Grishman/d5bfbc7a36b3a39a0fa70ad09a83fbec052f7cef", "https://www.semanticscholar.org/paper/University-of-Manitoba%3A-description-of-the-PIE-used-Lin/d5a3254e370b5ba59f0fc3b55768e09135da7531", "https://www.semanticscholar.org/paper/An-Algorithm-for-Pronominal-Anaphora-Resolution-Lappin-Leass/f977f5e25adef5f0569cb9eb9b930e5146be2571", "https://www.semanticscholar.org/paper/Factors-in-anaphora-resolution%3A-they-are-not-the-A-Mitkov/c1e0017518042b6f7efd6f62b6b40e115c2a961c", "https://www.semanticscholar.org/paper/Towards-a-computational-theory-of-definite-anaphora-Sidner/cfe4ab7bba290363e5fb52d6bde48f0e52626823"]},
{"id": "f2ffd8273d01948310feda2771943673aae4fe6f", "title": "Hierarchical feature-based translation for scalable natural language understanding", "authors": ["Ganesh N. Ramaswamy", "Jan Kleindienst"], "date": "2000", "abstract": "For complex natural language understanding systems with a large number of statistically confusable but semantically different formal commands, there are many difficulties in performing an accurate translation of a user input into a formal command in a single step. This paper addresses scalability issues in natural language understanding, and describes a method for performing the translation in a hierarchical manner. The hierarchical method improves the system accuracy, reduces the computational complexity of the translation, provides additional numerical robustness during training and decoding, and permits a more efficient packaging of the components of the natural language understanding system.", "references": ["https://www.semanticscholar.org/paper/Towards-speech-understanding-across-multiple-Ward-Roukos/929be8e4bd731fb2777245edaf9895ef3b4eda09", "https://www.semanticscholar.org/paper/Towards-multi-domain-speech-understanding-using-a-Chung-Seneff/723267b88259d2c413d890b1b227e337a9550103", "https://www.semanticscholar.org/paper/A-Maximum-Entropy-Approach-to-Natural-Language-Berger-Pietra/fb486e03369a64de2d5b0df86ec0a7b55d3907db", "https://www.semanticscholar.org/paper/Multimodal-discourse-modelling-in-a-multi-user-Seneff-Goddeau/23dd48bc98a237df3f6f92b55691a973d4259cef", "https://www.semanticscholar.org/paper/JUPlTER%3A-a-telephone-based-conversational-interface-Zue-Seneff/e22ffd1dbbf9dca915ee167c502788029ef19bba", "https://www.semanticscholar.org/paper/A-pervasive-conversational-interface-for-Ramaswamy-Kleindienst/16d3e653a6cbfe6e164a49384dd5e6795fcb12c7", "https://www.semanticscholar.org/paper/Inducing-Features-of-Random-Fields-Pietra-Pietra/b951b9f78b98a186ba259027996a48e4189d37e5", "https://www.semanticscholar.org/paper/The-LIMSI-ARISE-system-for-train-travel-information-Lamel-Rosset/0d5c439cfda89a0128f886939fc1750b6bc31dfd", "https://www.semanticscholar.org/paper/Feature-based-language-understanding-Papineni-Roukos/23645d7e433009061d8dd6c81f8556166f97acdd"]},
{"id": "7599dfed1de67c726f9e4fd372cc9ef03d2cf3e9", "title": "Feature engineering in Context-Dependent Deep Neural Networks for conversational speech transcription", "authors": ["Frank Seide", "Gang Li", "Xie Chen", "Dong Yu"], "date": "2011", "abstract": "We investigate the potential of Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, from a feature-engineering perspective. Recently, we had shown that for speaker-independent transcription of phone calls (NIST RT03S Fisher data), CD-DNN-HMMs reduced the word error rate by as much as one third\u2014from 27.4%, obtained by discriminatively trained Gaussian-mixture HMMs with HLDA features, to 18.5%\u2014using 300+ hours of training data (Switchboard), 9000+ tied triphone states, and up to 9 hidden network layers.", "references": ["https://www.semanticscholar.org/paper/Conversational-Speech-Transcription-Using-Deep-Seide-Li/473f0739666af2791ad6592822118240ed968b70", "https://www.semanticscholar.org/paper/Context-Dependent-Pre-Trained-Deep-Neural-Networks-Dahl-Yu/6658bbf68995731b2083195054ff45b4eca38b3a", "https://www.semanticscholar.org/paper/Recent-innovations-in-speech-to-text-transcription-Stolcke-Chen/4e3ba28fb3493afd2c3db4bd8be6d8d41cf3647a", "https://www.semanticscholar.org/paper/Deep-Belief-Networks-for-phone-recognition-Mohamed-Dahl/f37cfdc4520c56c1eaf87cee5ec2a4028ceaa9c5", "https://www.semanticscholar.org/paper/Roles-of-Pre-Training-and-Fine-Tuning-in-DBN-HMMs-Yu-Deng/ecd4bc32bb2717c96f76dd100fcd1255a07bd656", "https://www.semanticscholar.org/paper/ACID%2FHNN%3A-clustering-hierarchies-of-neural-networks-Fritsch-Finke/7f58b2140534a391c2decb4ab09ab4cecdb548a4", "https://www.semanticscholar.org/paper/Context-dependent-connectionist-probability-in-a-Franco-Cohen/583f48d72127c00e0499513c1b2f4e8ac71e4406", "https://www.semanticscholar.org/paper/Deep-Belief-Networks-using-discriminative-features-Mohamed-Sainath/be53d4def5e0601f2416e9345babc7ef1b30a664", "https://www.semanticscholar.org/paper/Connectionist-speaker-normalization-and-adaptation-Abrash-Franco/c256a54a5f3f07a6dcf2dea3a220d0024cf3bfe5", "https://www.semanticscholar.org/paper/Connectionist-probability-estimators-in-HMM-speech-Renals-Morgan/a08c99425ad94eed67d059813511fe9ca55e73eb"]},
{"id": "02537122c7f00d63c8c9c861791a2026a4879e33", "title": "Using Decision Trees for Coreference Resolution", "authors": ["Joseph F. McCarthy", "Wendy G. Lehnert"], "date": "1995", "abstract": "This paper describes RESOLVE, a system that uses decision trees to learn how to classify conference phrases in the domain of business joint ventures. An experiment is presented in which the performance of RESOLVE is compared to the performance of a manually engineered set of rules for the same task. The results show that decision trees achieve higher performance than the rules in two of three evaluation metrics developed for the coreference task. In addition to achieving better performance than the rules, RESOLVE provides a framework that facilitates the exploration of the types of knowledge that are useful for solving the conference problem.", "references": ["https://www.semanticscholar.org/paper/A-model-theoretic-coreference-scoring-scheme-Vilain-Burger/890c1201d3edef7104ba9d75078ed90f9c30c93b", "https://www.semanticscholar.org/paper/University-of-Massachusetts%3A-description-of-the-as-Lehnert-Cardie/f8a044c3935bd18aba4064e861ea854a84f2f52a", "https://www.semanticscholar.org/paper/BBN%3A-description-of-the-PLUM-system-as-used-for-Ayuso-Boisen/4016f03789471ddd8fa5feafc71563fa03f72732", "https://www.semanticscholar.org/paper/USC%3A-MUC-4-test-results-and-analysis-Moldovan-Cha/251f69e648f40ae96b5330625942b33c4c10898f", "https://www.semanticscholar.org/paper/University-of-Massachusetts%3A-description-of-the-as-Lehnert-Cardie/72c8167620e213b1c28ad34096594e758825d91a", "https://www.semanticscholar.org/paper/Computational-aspects-of-discourse-in-the-context-Iwanska-Appelt/694e53f0a524e22170c5f45565abe23479500568", "https://www.semanticscholar.org/paper/UNISYS%3A-description-of-the-CBAS-system-used-for-Weir-Fritzson/8038b8f16ff80a2f660d3c92485c7cf9b4a1a162", "https://www.semanticscholar.org/paper/Symbolic%2FSubsymbolic-Sentence-Analysi%3A-Exploiting-Lehnert/a29ff9c3d0c5c80716ef1ddb23079c7ca6c1e5c1", "https://www.semanticscholar.org/paper/MITRE-Bedford-ALEMBIC%3A-MUC-4-test-results-and-Aberdeen-Burger/b4ea236d39e13c82cf600ef1be90defc2b88e105", "https://www.semanticscholar.org/paper/Towards-a-computational-theory-of-definite-anaphora-Sidner/cfe4ab7bba290363e5fb52d6bde48f0e52626823"]},
{"id": "dffbafb5c8d0ca39279f7f463765c991cd208895", "title": "Meetings about meetings: research at ICSI on speech in multiparty conversations", "authors": ["Nelson Morgan", "Don Baron", "Sonali Bhagat", "Hannah Carvey", "Rajdip Dhillon", "Jane Edwards", "David Gelbart", "Adam Janin", "Ashley Krupski", "Barbara Peskin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke", "Chuck Wooters"], "date": "2003", "abstract": "In early 2001, we reported (at the Human Language Technology meeting) the early stages of an ICSI (International Computer Science Institute) project on processing speech from meetings (in collaboration with other sites, principally SRI, Columbia, and UW). We report our progress from the first few years of this effort, including: the collection and subsequent release of a 75-meeting corpus (over 70 meeting-hours and up to 16 channels for each meeting); the development of a prosodic database for a large subset of these meetings, and its subsequent use for punctuation and disfluency detection; the development of a dialog annotation scheme and its implementation for a large subset of the meetings; and the improvement of both near-mic and far-mic speech recognition results for meeting speech test sets.", "references": ["https://www.semanticscholar.org/paper/The-ICSI-Meeting-Corpus-Janin-Baron/ff7804a13efc26bf23ce813319641db69ddbb969", "https://www.semanticscholar.org/paper/The-Meeting-Project-at-ICSI-Morgan-Baron/7560cb8b0bf0fea82f1aa11413007c5d53935ebd", "https://www.semanticscholar.org/paper/Prosody-based-automatic-segmentation-of-speech-into-Shriberg-Stolcke/05607f5a0c0a1aeb403d9755ca4e2e3d9df58297", "https://www.semanticscholar.org/paper/Automatic-punctuation-and-disfluency-detection-in-Baron-Shriberg/109af87d5673a3a8cadc40cf6a814d73b4fe32ef", "https://www.semanticscholar.org/paper/Multispeaker-speech-activity-detection-for-the-ICSI-Pfau-Ellis/66167b8ab5de62138605be607ba41d91cf9db038", "https://www.semanticscholar.org/paper/Modeling-dynamic-prosodic-variation-for-speaker-S%C3%B6nmez-Shriberg/75aee7ad0a97c5b91ebdfe48e9ae9f82f9842905", "https://www.semanticscholar.org/paper/Double-the-trouble%3A-handling-noise-and-in-far-field-Gelbart-Morgan/9b653084df3716f8e537992a65f6b5d5f4a305da", "https://www.semanticscholar.org/paper/Far-field-ASR-on-inexpensive-microphones-Fern%C3%A1ndez-Gelbart/2d776fb2e4ec187ff05a249a54f35679112810f7", "https://www.semanticscholar.org/paper/Qualcomm-ICSI-OGI-features-for-ASR-Adami-Burget/11cc25eb27169dd37ce5e12b79fa32d8399e9fcc", "https://www.semanticscholar.org/paper/Switchboard-DAMSL-Labeling-Project-Coder''s-Manual-Jurafsky-Shriberg/24ef3b2a8b8c4382eec891f3308f7564bbbc4bfe"]},
{"id": "2b87a30cb2923674cc54ace37489af772f781358", "title": "Building task-specific interfaces to high volume conversational data", "authors": ["Loren G. Terveen", "William C. Hill", "Brian Amento", "David W. McDonald", "Josh Creter"], "date": "1997", "abstract": "As people participate in the thousands of global conversations that comprise Usenet news, one thing they do is post their opinions of web resources. Phoaks is a collaborative filtering system that continuously parses, classifies, abstracts and tallies those opinions. About 3,500 users per day consult Phoaks web pages that reflect the results. Phoaks also features a general architecture for building similar collaborative filtering interfaces to conversational data. We report here on the Phoaks resource recommendation interface, the architecture, and the issues and experience that make up its rationale.", "references": []},
{"id": "8dd9fd6a45afd266d48255c398429e01ea4fd6db", "title": "Learning to Transform Natural to Formal Languages", "authors": ["Rohit J. Kate", "Yuk Wah Wong", "Raymond J. Mooney"], "date": "2005", "abstract": "This paper presents a method for inducing transformation rules that map natural-language sentences into a formal query or command language. The approach assumes a formal grammar for the target representation language and learns transformation rules that exploit the non-terminal symbols in this grammar. The learned transformation rules incrementally map a natural-language sentence or its syntactic parse tree into a parse-tree for the target formal language. Experimental results are presented for two corpora. one which maps English instructions into an existing formal coaching language for simulated RoboCup soccer agents, and another which maps English U.S.-geography questions into a database query language. We show that our method performs overall better and faster than previous approaches in both domains.", "references": ["https://www.semanticscholar.org/paper/A-Statistical-Semantic-Parser-that-Integrates-and-Ge-Mooney/7ae207cfaf01dc2b6799da67f454190b34994870", "https://www.semanticscholar.org/paper/Learning-to-Parse-Database-Queries-Using-Inductive-Zelle-Mooney/b7c0e47f8b768258b7d536c21b218e6c46ab8791", "https://www.semanticscholar.org/paper/Modern-Natural-Language-Interfaces-to-Databases%3A-Popescu-Armanasu/49a8ae5eb474e7f6ce3669f9c55efaee1d43cdec", "https://www.semanticscholar.org/paper/Towards-a-theory-of-natural-language-interfaces-to-Popescu-Etzioni/7c7ba9df3ab69f0f3502a7a873c031c8244187ee", "https://www.semanticscholar.org/paper/Using-Multiple-Clause-Constructors-in-Inductive-for-Tang-Mooney/f60d8dd8ca3a7dfa7d0a14988af73084ad93619d", "https://www.semanticscholar.org/paper/Intricacies-of-Collins'-Parsing-Model-Bikel/0606291dae96446e812ea8f09d9fbdc6acc3ec37", "https://www.semanticscholar.org/paper/Efficient-Induction-of-Logic-Programs-Muggleton-Feng/6a669636e0ada62a0fb444e95435e24fdbdf4dbd", "https://www.semanticscholar.org/paper/Guiding-a-Reinforcement-Learner-with-Natural-in-Shavlik/dd2d5ddc0399e0b87c339ebea4042ef2ad6f0317", "https://www.semanticscholar.org/paper/Transformation-Based-Error-Driven-Learning-and-A-in-Brill/2b2eb4a9bb146e3ffaa0b025fba0ed14240c683f", "https://www.semanticscholar.org/paper/Conversational-interfaces%3A-advances-and-challenges-Zue/157f5fc669dfc440492fa01cb7fe46593ed55304"]},
{"id": "bad3c30a75a28dbb8ace36e8ddb8d65cd760dfe6", "title": "A weighted finite state transducer translation template model for statistical machine translation", "authors": ["Shankar Kumar", "Yonggang Deng", "William J. Byrne"], "date": "2006", "abstract": "We present a Weighted Finite State Transducer Translation Template Model for statistical machine translation. This is a source-channel model of translation inspired by the Alignment Template translation model. The model attempts to overcome the deficiencies of word-to-word translation models by considering phrases rather than words as units of translation. The approach we describe allows us to implement each constituent distribution of the model as a weighted finite state transducer or acceptor. We show that bitext word alignment and translation under the model can be performed with standard finite state machine operations involving these transducers. One of the benefits of using this framework is that it avoids the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses. We report and analyze bitext word alignment and translation performance on the Hansards French-English task and the FBIS Chinese-English task under the Alignment Error Rate, BLEU, NIST and Word Error-Rate metrics. These experiments identify the contribution of each of the model components to different aspects of alignment and translation performance. We finally discuss translation performance with large bitext training sets on the NIST 2004 Chinese-English and Arabic-English MT tasks.", "references": ["https://www.semanticscholar.org/paper/A-Weighted-Finite-State-Transducer-Implementation-Kumar-Byrne/c4138748eb5dc1bbd1df2951f299d701304147a2", "https://www.semanticscholar.org/paper/Improved-Alignment-Models-for-Statistical-Machine-Och-Tillmann/8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "https://www.semanticscholar.org/paper/Statistical-machine-translation%3A-from-single-word-Och/e4d3bf856ce5259360a8033d50abcdd22873bcd6", "https://www.semanticscholar.org/paper/The-Johns-Hopkins-University-2003-Chinese-English-Byrne-Khudanpur/0ccfb8fab02999e08b5b66108320890349d5a222", "https://www.semanticscholar.org/paper/A-Finite-State-Approach-to-Machine-Translation-Bangalore-Riccardi/f2f30c882a3380a96373b0e8731be2edcd12e075", "https://www.semanticscholar.org/paper/A-Projection-Extension-Algorithm-for-Statistical-Tillmann/f013d1eaf0b5e089281c64c8a75d2b7ec390891d", "https://www.semanticscholar.org/paper/The-Mathematics-of-Statistical-Machine-Translation%3A-Brown-Pietra/ab7b5917515c460b90451e67852171a531671ab8", "https://www.semanticscholar.org/paper/Statistical-Phrase-Based-Translation-Koehn-Och/a4b828609b60b06e61bea7a4029cc9e1cad5df87", "https://www.semanticscholar.org/paper/Word-re-ordering-and-dynamic-programming-based-for-Tillmann/7ad3903a0c2645585946d00d075676dfe5f220f1", "https://www.semanticscholar.org/paper/Translation-with-Finite-State-Devices-Knight-Al-Onaizan/7f3e90eae5d24f502603163aed4bdfc32203207c"]},
{"id": "11cd7fbc0ea8605ea498ecfc82b3ff6a44c027e9", "title": "A Syntax-based Statistical Translation Model", "authors": ["Kenji Yamada", "Kevin Knight"], "date": "2001", "abstract": "We presenta syntax-basedstatistical translationmodel. Our model transforms a source-languageparse tree into a target-languagestring by applying stochasticoperationsat eachnode. Theseoperationscapturelinguistic differencessuchas word order and case marking. Model parametersare estimatedin polynomialtime usinganEM algorithm. The model producesword alignmentsthat are better than those producedby IBM Model5.", "references": ["https://www.semanticscholar.org/paper/Improved-Alignment-Models-for-Statistical-Machine-Och-Tillmann/8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "https://www.semanticscholar.org/paper/Improved-Statistical-Alignment-Models-Och-Ney/c9214ebe91454e6369720136ab7dd990d52a07d4", "https://www.semanticscholar.org/paper/Semi-Automatic-Acquisition-of-Domain-Specific-Resnik-Melamed/1931742167393426ef56523d0ecf53a6e92d0ed0", "https://www.semanticscholar.org/paper/Grammar-Inference-and-Statistical-Machine-Wang/cb9f9d320ad39febc6246901d47fb40446f2344b", "https://www.semanticscholar.org/paper/Twisted-Pair-Grammar%3A-Support-for-Rapid-Development-Jones-Havrilla/07f8dc8ac06b0b3305ea96a62ea98145dc9620f4", "https://www.semanticscholar.org/paper/Word-Sense-Disambiguation-Using-Statistical-Methods-Brown-Pietra/85b9eb556c211d954b31d9d58fed6891a07ab473", "https://www.semanticscholar.org/paper/Head-Driven-Statistical-Models-for-Natural-Language-Collins/3fc44ff7f37ec5585310666c183c65e0a0bb2446", "https://www.semanticscholar.org/paper/Models-of-Translational-Equivalence-among-Words-Melamed/38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1", "https://www.semanticscholar.org/paper/The-Mathematics-of-Statistical-Machine-Translation%3A-Brown-Pietra/ab7b5917515c460b90451e67852171a531671ab8", "https://www.semanticscholar.org/paper/A-statistical-approach-to-language-translation-Brown-Cocke/2166fa493a8c6e40f7f8562d15712dd3c75f03df"]},
{"id": "c9214ebe91454e6369720136ab7dd990d52a07d4", "title": "Improved Statistical Alignment Models", "authors": ["Franz Josef Och", "Hermann Ney"], "date": "2000", "abstract": "In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications. We present different methods to combine alignments. As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment. We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.", "references": []},
{"id": "397389558fbbc4d6801c4061f7ecde6b3ec2a4dd", "title": "Hesitation and semantic planning in speech", "authors": ["Brian Butterworth"], "date": "1975", "abstract": "Samples of spontaneous speech were analyzed according to their distributions of phonations and silences. Some of these exhibited cyclic, or \u201crhythmic,\u201d patterns, in the sense defined by Goldman-Eisler. Transcripts of three such samples were subjected to a segmentation procedure carried out by independent judges utilizing a common semantic intuition. Points in the transcripts where agreement was high among the judges were found to correspond with the beginnings of temporal cycles, and agreed semantic segments coincided with sentence or clause boundaries and usually consisted of several clauses and more than one sentence. It is argued that a theory of speech generation must contain provision for semantic integration at the suprasentential level.", "references": []},
{"id": "760030861baea8154acc5dcd9da96e4220e54b7d", "title": "The TRAINS Project", "authors": ["James F. Allen", "Lenhart K. Schubert"], "date": "1991", "abstract": "Abstract : The TRAINS project is a long-term research effort on building an intelligent planning assistant that is conversationally proficient in natural language. The TRAINS project serves as an umbrella for research that involves pushing the state of the art in real-time planning, planning in uncertain worlds, plan monitoring and execution, natural language understanding techniques applicable to spoken language, and natural language dialog and discourse modelling. Significant emphasis is being put on the knowledge representation issues that arise in supporting the tasks in the domain. This report describes the general goals of the TRAINS project and the particular research directions that we are pursuing. Planning, Natural language understanding, Dialog systems.", "references": []},
{"id": "27056c1fe6d6a34f4a0086d7abac333ecbe43871", "title": "Using English for Indexing and Retrieving", "authors": ["Boris Katz"], "date": "1988", "abstract": "This paper describes a natural language system, START. The system analyzes English text and automatically transforms it into an appropriate representation, the {\\it knowledge base}, which incorporates the information found in the text. The user gains access to information stored in the knowledge base by querying it in English. The system analyzes the query and decides through a matching process what information in the knowledge base is relevant to the question. Then it retrieves this information and formulates its response in English.", "references": []},
{"id": "dd5061631a4d11fa394f4421700ebf7e78dcbc59", "title": "Optimization by simulated annealing.", "authors": ["Scott Kirkpatrick", "C. D. Gelatt", "Mario P. Vecchi"], "date": "1983", "abstract": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.", "references": ["https://www.semanticscholar.org/paper/The-Design-and-Analysis-of-Computer-Algorithms-Aho-Hopcroft/10a463bb00b44bdd3a8620f2bedb9e1564bfcf32"]},
{"id": "79fbfc1dc8846379074aaf4deb7fb0a96722eeed", "title": "An Efficient Probabilistic Context-Free Parsing Algorithm that Computes Prefix Probabilities", "authors": ["Andreas Stolcke"], "date": "1995", "abstract": "We describe an extension of Earley's parser for stochastic context-free grammars that computes the following quantities given a stochastic context-free grammar and an input string: a) probabilities of successive prefixes being generated by the grammar; b) probabilities of substrings being generated by the nonterminals, including the entire string being generated by the grammar; c) most likely (Viterbi) parse of the string; d) posterior expected number of applications of each grammar production, as required for reestimating rule probabilities. Probabilities (a) and (b) are computed incrementally in a single left-to-right pass over the input. Our algorithm compares favorably to standard bottom-up parsing methods for SCFGs in that it works efficiently on sparse grammars by making use of Earley's top-down control structure. It can process any context-free rule format without conversion to some normal form, and combines computations for (a) through (d) in a single algorithm. Finally, the algorithm has simple extensions for processing partially bracketed inputs, and for finding partial parses and their likelihoods on ungrammatical inputs.", "references": ["https://www.semanticscholar.org/paper/Basic-Methods-of-Probabilistic-Context-Free-Jelinek-Lafferty/617241818e8ddd6edcb4ee7682992673c18c6f3d", "https://www.semanticscholar.org/paper/Generalized-Probabilistic-LR-Parsing-of-Natural-Briscoe-Carroll/8ad8e98574a275930bf04a477ce3532fd13c503c", "https://www.semanticscholar.org/paper/Hidden-Markov-estimation-for-unrestricted-grammars-Kupiec/befbe8bd0a8aa0fc0a5a92a9679f87e81b8ba40b", "https://www.semanticscholar.org/paper/An-efficient-context-free-parsing-algorithm-Earley/62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "https://www.semanticscholar.org/paper/Pearl%3A-A-Probabilistic-Chart-Parser-Magerman-Marcus/da838db79e7593018894ada44db35eee670941d6", "https://www.semanticscholar.org/paper/Precise-N-Gram-Probabilities-from-Stochastic-Stolcke-Segal/823233093025cdc6aec13dbc17d9e4116f686eba", "https://www.semanticscholar.org/paper/An-efficient-probabilistic-context-free-parsing-StolckeAndreas/36a0308e21157b58a66f4485c55c733bfedec75f", "https://www.semanticscholar.org/paper/A-Probabilistic-Parser-and-Its-Application-Jones-Eisner/4f6e1e510be6daea31e9f9c56fb7db714d375725", "https://www.semanticscholar.org/paper/Applications-of-stochastic-context-free-grammars-Lari-Young/9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74", "https://www.semanticscholar.org/paper/Using-a-stochastic-context-free-grammar-as-a-model-Jurafsky-Wooters/f2d00cf1be1f129c88d0471263b90a3f3f06e942"]},
{"id": "a21b0262436d1a5013f4a64cc17fdc6ec5b57be7", "title": "Learning Noun Phrase Anaphoricity to Improve Conference Resolution: Issues in Representation and Optimization", "authors": ["Vincent Ng"], "date": "2004", "abstract": "Knowledge of the anaphoricity of a noun phrase might be profitably exploited by a coreference system to bypass the resolution of non-anaphoric noun phrases. Perhaps surprisingly, recent attempts to incorporate automatically acquired anaphoricity information into coreference systems, however, have led to the degradation in resolution performance. This paper examines several key issues in computing and using anaphoricity information to improve learning-based coreference systems. In particular, we present a new corpus-based approach to anaphoricity determination. Experiments on three standard coreference data sets demonstrate the effectiveness of our approach.", "references": ["https://www.semanticscholar.org/paper/Identifying-Anaphoric-and-Non-Anaphoric-Noun-to-Ng-Cardie/e414e980005c1066c539dd893a14188371792a01", "https://www.semanticscholar.org/paper/Improving-Machine-Learning-Approaches-to-Resolution-Ng-Gardent/08c81389b3ac4b8253d718a7cebe04a5536efa78", "https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/Corpus-Based-Identification-of-Non-Anaphoric-Noun-Bean-Riloff/328bd9254d97f30d82471b45dd18c53529862333", "https://www.semanticscholar.org/paper/Anaphora-for-Everyone%3A-Pronominal-Anaphora-without-Kennedy-Boguraev/5bbf53a9a8ee5a52e8b3e4c4e04cfebc8cc1b1c9", "https://www.semanticscholar.org/paper/A-New%2C-Fully-Automatic-Version-of-Mitkov's-Pronoun-Mitkov-Evans/730df745e4e4d5b09e137391bbf25704ab1f873e", "https://www.semanticscholar.org/paper/A-utomatic-Resolution-of-Anaphora-in-English-Denber/f3ee1003bd15f7022fee12291f2f377f0cf5413f", "https://www.semanticscholar.org/paper/A-model-theoretic-coreference-scoring-scheme-Vilain-Burger/890c1201d3edef7104ba9d75078ed90f9c30c93b", "https://www.semanticscholar.org/paper/An-Algorithm-for-Pronominal-Anaphora-Resolution-Lappin-Leass/f977f5e25adef5f0569cb9eb9b930e5146be2571", "https://www.semanticscholar.org/paper/Coreference-Resolution-Using-Competition-Learning-Yang-Zhou/f43b57fa22d3b8275caa403d7b26f3681a888421"]},
{"id": "ac8f1fd58be8a8c9f9599fc4da981ea3040945f6", "title": "TINA: A Natural Language System for Spoken Language Applications", "authors": ["Stephanie Seneff"], "date": "1992", "abstract": "A new natural language system, TINA, has been developed for applications involving spoken language tasks. TINA integrates key ideas from context free grammars, Augmented Transition Networks (ATN's), and the unification concept. TINA provides a seamless interface between syntactic and semantic analysis, and also produces a highly constraining probabilistic language model to improve recognition performance. An initial set of context-free rewrite rules provided by hand is first converted to a network structure. Probability assignments on all arcs in the network are obtained automatically from a set of example sentences. The parser uses a stack decoding search strategy, with a top-down control flow, and includes a feature-passing mechanism to deal with long-distance movement, agreement, and semantic constraints. TINA provides an automatic sentence generation capability that has been effective for identifying overgeneralization problems as well as in producing a word-pair language model for a recognizer. The parser is currently integrated with MIT's SUMMIT recognizer for use in two application domains, with the parser screening recognizer outputs either at the sentential level or to filter partial theories during the active search process.", "references": ["https://www.semanticscholar.org/paper/Transition-network-grammars-for-natural-language-Woods/09550accec47459a61fe1710a0a32c2ec22449bd", "https://www.semanticscholar.org/paper/The-MINDS-System%3A-Using-Context-and-Dialog-to-Young/6ca5729fb789b50385d9c38627f2947ff7db86dd", "https://www.semanticscholar.org/paper/Integration-of-speech-recognition-and-natural-in-Zue-Glass/6600ffacc96755cee573953313c53af69418c05e", "https://www.semanticscholar.org/paper/Experimental-results-on-large-vocabulary-continuous-Mattia-Giachin/c5634e860cd4bd2160d77c918c5cb663dd56a960", "https://www.semanticscholar.org/paper/Development-and-Preliminary-Evaluation-of-the-MIT-Seneff-Glass/4911a09ce25987e260c4a8c7773a703335977c99", "https://www.semanticscholar.org/paper/Discovery-Procedures-for-Sublanguage-Selectional-Grishman-Hirschman/819c80bc13dc40da5d0dd25f496c0cfa7dfcf832", "https://www.semanticscholar.org/paper/The-Interaction-of-Word-Recognition-and-Linguistic-Niemann-Sagerer/b21a10ca83d8c73622e7b19553c551b54fe67ad1", "https://www.semanticscholar.org/paper/Semantics-and-Quantification-in-Natural-Language-Woods/09296827532a91527396edfac76461fb5e10377a", "https://www.semanticscholar.org/paper/The-BBN-Spoken-Language-System-Boisen-Chow/c730d6ec9ffe803eb4d874565b39c5d967fca219", "https://www.semanticscholar.org/paper/Grammatically-based-automatic-word-class-formation-Hirschman-Grishman/819b44e81eb2a204f4c59ecabb9ad09f12128a50"]},
{"id": "cbb786e7e42d02de0080299dc2e114357e816002", "title": "Conditional Models of Identity Uncertainty with Application to Noun Coreference", "authors": ["Andrew McCallum", "Ben Wellner"], "date": "2004", "abstract": "Coreference analysis, also known as record linkage or identity uncertainty, is a difficult and important problem in natural language processing, databases, citation matching and many other tasks. This paper introduces several discriminative, conditional-probability models for coreference analysis, all examples of undirected graphical models. Unlike many historical approaches to coreference, the models presented here are relational\u2014they do not assume that pairwise coreference decisions should be made independently from each other. Unlike other relational models of coreference that are generative, the conditional model here can incorporate a great variety of features of the input without having to be concerned about their dependencies\u2014paralleling the advantages of conditional random fields over hidden Markov models. We present positive results on noun phrase coreference in two standard text data sets.", "references": ["https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/Text-and-Knowledge-Mining-for-Coreference-Harabagiu-Bunescu/e20d2d6c57032114ee57118ff620fdef7c548c46", "https://www.semanticscholar.org/paper/Improving-Machine-Learning-Approaches-to-Resolution-Ng-Gardent/08c81389b3ac4b8253d718a7cebe04a5536efa78", "https://www.semanticscholar.org/paper/Identity-Uncertainty-and-Citation-Matching-Pasula-Marthi/25308818aa94ffb3a9809540b530f6e8c7bfb83c", "https://www.semanticscholar.org/paper/Discriminative-Probabilistic-Models-for-Relational-Taskar-Abbeel/9cc36397e1fef5c922d64e88211a7e08ecc64759", "https://www.semanticscholar.org/paper/Early-results-for-Named-Entity-Recognition-with-and-McCallum-Li/8d65ee7aa0a9dac3957093985e9179e1ccb9bd3b", "https://www.semanticscholar.org/paper/Using-Decision-Trees-for-Coreference-Resolution-McCarthy-Lehnert/02537122c7f00d63c8c9c861791a2026a4879e33", "https://www.semanticscholar.org/paper/A-Statistical-Approach-to-Anaphora-Resolution-Ge-Hale/71c0698edd0cf489cd837c91ad22bbf51643bf6c", "https://www.semanticscholar.org/paper/Coreference-for-NLP-Applications-Morton/ae287b7ecbb3dffaa0e302acfd7bbf44f733acfc", "https://www.semanticscholar.org/paper/Learning-Probabilistic-Relational-Models-Getoor/611dac316bf03112c778cf7365d08e4a9d171876"]},
{"id": "f8a044c3935bd18aba4064e861ea854a84f2f52a", "title": "University of Massachusetts: description of the CIRCUS system as used for MUC-3", "authors": ["Wendy G. Lehnert", "Claire Cardie", "David Fisher", "Ellen Riloff", "Robert Williams"], "date": "1991", "abstract": "In 1988 Professor Wendy Lehnert completed the initial implementation of a semantically-oriented sentence analyzer named CIRCUS [1]. The original design for CIRCUS was motivated by two basic research interests: (1) we wanted to increase the level of syntactic sophistication associated with semantically-oriented parsers, and (2) we wanted to integrate traditional symbolic techniques in natural language processing with connectionist techniques in an effort to exploit the complementary strengths of these two computational paradigms.", "references": ["https://www.semanticscholar.org/paper/Symbolic%2FSubsymbolic-Sentence-Analysi%3A-Exploiting-Lehnert/a29ff9c3d0c5c80716ef1ddb23079c7ca6c1e5c1", "https://www.semanticscholar.org/paper/Integration-of-Semantic-and-Syntactic-Constraints-Wermter/12f738cd1d9d274247809d7eb3ab45b8419a9766", "https://www.semanticscholar.org/paper/A-Hybrid-Symbolic%2FConnectionist-Model-for-Noun-Wermter-Lehnert/2c70c978fe42c2834fcddbb0a4ca653a459c40b7", "https://www.semanticscholar.org/paper/Bibliography-Morady-Marx/f86c0a4caae451dc8ba319013e30f14987fa5b26", "https://www.semanticscholar.org/paper/Analyzing-research-papers-using-citation-sentences-Lehnert-Cardie/d83cf58987413ad3c132bbbabd12d149b47acc2e"]},
{"id": "66167b8ab5de62138605be607ba41d91cf9db038", "title": "Multispeaker speech activity detection for the ICSI meeting recorder", "authors": ["Thilo Pfau", "Daniel P. W. Ellis", "Andreas Stolcke"], "date": "2001", "abstract": "As part of a project into speech recognition in meeting environments, we have collected a corpus of multichannel meeting recordings. We expected the identification of speaker activity to be straightforward given that the participants had individual microphones, but simple approaches yielded unacceptably erroneous labelings, mainly due to crosstalk between nearby speakers and wide variations in channel characteristics. Therefore, we have developed a more sophisticated approach for multichannel speech activity detection using a simple hidden Markov model (HMM). A baseline HMM speech activity detector has been extended to use mixtures of Gaussians to achieve robustness for different speakers under different conditions. Feature normalization and crosscorrelation processing are used to increase the channel independence and to detect crosstalk. The use of both energy normalization and crosscorrelation based postprocessing results in a 35% relative reduction of the frame error rate. Speech recognition experiments show that it is beneficial in this multispeaker setting to use the output of the speech activity detector for presegmenting the recognizer input, achieving word error rates within 10% of those achieved with manual turn labeling.", "references": ["https://www.semanticscholar.org/paper/THE-SRI-MARCH-2000-HUB-5-CONVERSATIONAL-SPEECH-Stolcke-Bratt/23318a16f8a049409be848be6e5fcdef22e78012", "https://www.semanticscholar.org/paper/Observations-on-overlap%3A-findings-and-implications-Shriberg-Stolcke/84eb734fed7fb46359a4f6bd3f7482b742da0dc6", "https://www.semanticscholar.org/paper/A-block-least-squares-approach-to-acoustic-echo-Woudenberg-Soong/0d7f23bc5be4cfb09bc9670610d5007203f57254", "https://www.semanticscholar.org/paper/The-Meeting-Project-at-ICSI-Morgan-Baron/7560cb8b0bf0fea82f1aa11413007c5d53935ebd", "https://www.semanticscholar.org/paper/Robust-HMM-based-endpoint-detector-Acero-Crespo-Casas/a87b30feba7717e90aa5891a2798ef4ef9ad8740", "https://www.semanticscholar.org/paper/Methoden-der-Automatischen-Spracherkennung-Ruske/70d0558637b9a6b6f506b0ccb83ea02d6e739135", "https://www.semanticscholar.org/paper/Adaptiver-stochastischer-Sprache%2FPause-Detektor-Beham-Ruske/44ee61e23a6bf3b9f14dfa28189f0f82c7a892cd"]},
{"id": "8df509919b31397e225280962c59384fbe83144e", "title": "Evaluation of Spoken Language Systems: the ATIS Domain", "authors": ["P. J. Price"], "date": "1990", "abstract": "Progress can be measured and encouraged via standards for comparison and evaluation. Though qualitative assessments can be useful in initial stages, quantifiable measures of systems under the same conditions are essential for comparing results and assessing claims. This paper will address the emerging standards for evaluation of spoken language systems.", "references": ["https://www.semanticscholar.org/paper/Developing-an-Evaluation-Methodology-for-Spoken-Bates-Boisen/55a1bf99b29436197fd4d872a78155a196a77a88", "https://www.semanticscholar.org/paper/The-ATIS-Spoken-Language-Systems-Pilot-Corpus-Hemphill-Godfrey/1d19708290ef3cc3f43c2c95b07acdd4f52f5cda", "https://www.semanticscholar.org/paper/Beyond-Class-A%3A-A-Proposal-for-Automatic-Evaluation-Hirschman-Dahl/97bb32862f2bd95f2dd81ca1e29c1e003de32ff6", "https://www.semanticscholar.org/paper/Plans-For-a-Task-Oriented-Evaluation-of-Natural-Sundheim/7deaa8cdacdd453b78d5284d41be0f05cd78aa64", "https://www.semanticscholar.org/paper/The-DARPA-1000-Word-Resource-Management-Database-Poritz-Richter/c1c52336c170641247ffafc2be17e2e759855682"]},
{"id": "11cc25eb27169dd37ce5e12b79fa32d8399e9fcc", "title": "Qualcomm-ICSI-OGI features for ASR", "authors": ["Andre Gustavo Adami", "Luk\u00e1s Burget", "St\u00e9phane Dupont", "Harinath Garudadri", "Frantisek Gr\u00e9zl", "Hynek Hermansky", "Pratibha Jain", "Sachin S. Kajarekar", "Nelson Morgan", "Sunil Sivadas"], "date": "2002", "abstract": "Our feature extraction module for theAurora task is based on a combination of a conventional noise supression technique (Wiener filtering) with our temporal processing technigues (linear discriminant RASTA filtering and nonlinear TempoRAl Pattern (TRAP) classifier). We observe better than 58% relative error improvement on the prescribed Aurora Digit Task, a performance level that is somewhat better than the new ETSI Advanced Feature standard. Furthermore, to test generalization of our approach to an independent test set not available during development, we evaluate performance on American English SpeechDatCar digits and show 10.54% relative improvement over the new ETSI standard.", "references": ["https://www.semanticscholar.org/paper/Distributed-speech-recognition-using-noise-robust-Jain-Hermansky/9706c7d6ab6cc737350f8b3090395b5065e6a897", "https://www.semanticscholar.org/paper/Data-Derived-Non-Linear-Mapping-for-Feature-in-HMM-Hermansky-Sharma/6d2da776b4a1c40bb7aeb59b5fa29897b5c66ccc", "https://www.semanticscholar.org/paper/Enabling-new-speech-driven-services-for-mobile-An-Pearce/4db594b948e95b3239f733b1625b54b0fb476380"]},
{"id": "c4138748eb5dc1bbd1df2951f299d701304147a2", "title": "A Weighted Finite State Transducer Implementation of the Alignment Template Model for Statistical Machine Translation", "authors": ["Shankar Kumar", "William J. Byrne"], "date": "2003", "abstract": "We present a derivation of the alignment template model for statistical machine translation and an implementation of the model using weighted finite state transducers. The approach we describe allows us to implement each constituent distribution of the model as a weighted finite state transducer or acceptor. We show that bitext word alignment and translation under the model can be performed with standard FSM operations involving these transducers. One of the benefits of using this framework is that it obviates the need to develop specialized search procedures, even for the generation of lattices or N-Best lists of bitext word alignments and translation hypotheses. We evaluate the implementation of the model on the French-to-English Hansards task and report alignment and translation performance.", "references": ["https://www.semanticscholar.org/paper/A-Finite-State-Approach-to-Machine-Translation-Bangalore-Riccardi/f2f30c882a3380a96373b0e8731be2edcd12e075", "https://www.semanticscholar.org/paper/Improved-Alignment-Models-for-Statistical-Machine-Och-Tillmann/8b0495331238da6c0e7be0bfdb9b5453b33c1f98", "https://www.semanticscholar.org/paper/Statistical-machine-translation%3A-from-single-word-Och/e4d3bf856ce5259360a8033d50abcdd22873bcd6", "https://www.semanticscholar.org/paper/Weighted-finite-state-transducers-in-speech-Mohri-Pereira/a80a452e587bd7f06ece1be101d6775fcee0f7af", "https://www.semanticscholar.org/paper/Translation-with-Finite-State-Devices-Knight-Al-Onaizan/7f3e90eae5d24f502603163aed4bdfc32203207c", "https://www.semanticscholar.org/paper/Improved-Statistical-Alignment-Models-Och-Ney/c9214ebe91454e6369720136ab7dd990d52a07d4", "https://www.semanticscholar.org/paper/The-Mathematics-of-Statistical-Machine-Translation%3A-Brown-Pietra/ab7b5917515c460b90451e67852171a531671ab8", "https://www.semanticscholar.org/paper/Generation-of-Word-Graphs-in-Statistical-Machine-Ueffing-Och/7181f7a664fbbf34c7c147c8a90f0343cdd1674c", "https://www.semanticscholar.org/paper/Automatic-evaluation-of-machine-translation-quality-Doddington/417f9ce1b1cb3c98e5c2a66d586c7a2eb7438a9f", "https://www.semanticscholar.org/paper/SRILM-an-extensible-language-modeling-toolkit-Stolcke/399da68d3b97218b6c80262df7963baa89dcc71b"]},
{"id": "7f58b2140534a391c2decb4ab09ab4cecdb548a4", "title": "ACID/HNN: clustering hierarchies of neural networks for context-dependent connectionist acoustic modeling", "authors": ["J\u00fcrgen Fritsch", "Michael Finke"], "date": "1998", "abstract": "We present the ACID/HNN framework, a principled approach to hierarchical connectionist acoustic modeling in large vocabulary conversational speech recognition (LVCSR). Our approach consists of an agglomerative clustering algorithm based on information divergence (ACID) to automatically design and robustly estimate hierarchies of neural networks (HNN) for arbitrarily large sets of context-dependent decision tree clustered HMM states. We argue that a hierarchical approach is crucial in applying locally discriminative connectionist models to the typically very large state spaces observed in LVCSR systems. We evaluate the ACID/HNN framework on the Switchboard conversational telephone speech corpus. Furthermore, we focus on the benefits of the proposed connectionist acoustic model, namely exploiting the hierarchical structure for speaker adaptation and decoding speed-up algorithms.", "references": ["https://www.semanticscholar.org/paper/ACID%2FHNN%3A-a-framework-for-hierarchical-acoustic-Fritsch/692af6e8ca5b28d6e21cabddaba43b9b490bd3f6", "https://www.semanticscholar.org/paper/Context-dependent-hybrid-HME%2FHMM-speech-recognition-Fritsch-Finke/0bcc22a892790476721aba256e151d2df37e0671", "https://www.semanticscholar.org/paper/Context-dependent-connectionist-probability-in-a-Franco-Cohen/583f48d72127c00e0499513c1b2f4e8ac71e4406", "https://www.semanticscholar.org/paper/Context-Dependent-Classes-in-a-Hybrid-Recurrent-Kershaw-Robinson/74ab2a9419f5356d77bad207ccb1b2caaf196a4f", "https://www.semanticscholar.org/paper/Connectionist-Speech-Recognition%3A-A-Hybrid-Approach-Bourlard-Morgan/3d82e058a5c40954b8f5db170a298a889a254c37", "https://www.semanticscholar.org/paper/The-bucket-box-intersection-(BBI)-algorithm-for-of-Fritsch-Rogina/8a0a14d8f84fcd8003cc0417cb97b2455dac3eee", "https://www.semanticscholar.org/paper/Transcription-of-broadcast-television-and-radio-the-Cook-Kershaw/0b45d669ac412a98569fcb04ac766a8440927e8e", "https://www.semanticscholar.org/paper/A-decision-theoretic-approach-to-hierarchical-Sch%C3%BCrmann-Doster/81f87408690b155ee08896acf33cf1b74be60460", "https://www.semanticscholar.org/paper/Speaker-adaptation-of-HMMs-using-linear-regression-Leggetter-Woodland/f743e6b3e71cb99cf336c5f320efd21acb126dc3", "https://www.semanticscholar.org/paper/The-JanusRTk-Switchboard%2FCallhome-1997-Evaluation-Finke-Fritsch/0d8c323cf5af3ed2ce85cd78692fcf53b6d0635b"]},
{"id": "07f8dc8ac06b0b3305ea96a62ea98145dc9620f4", "title": "Twisted Pair Grammar: Support for Rapid Development of Machine Translation for Low Density Languages", "authors": ["Douglas Jones", "Rick Havrilla"], "date": "1998", "abstract": "We describe a streamlined knowledge acquisition method for semi-automatically constructing knowledge bases for a Knowledge Based Machine Translation (KBMT) system. This method forms the basis of a very simple Java-based user interface that enables a language expert to build lexical and syntactic transfer knowledge bases without extensive specialized training as an MT system builder. Following [Wu 1997], we assume that the permutation of binary-branching structures is a sufficient reordering mechanism for MT. Our syntactic knowledge is based on a novel, highly constrained grammar construction environment in which the only re-ordering mechanism is the permutation of binary-branching structures (Twisted Pair Grammar). We describe preliminary results for several fully implemented components of a Hindi/Urdu to English MT prototype being built with this interface.", "references": []},
{"id": "85b9eb556c211d954b31d9d58fed6891a07ab473", "title": "Word-Sense Disambiguation Using Statistical Methods", "authors": ["Peter F. Brown", "Stephen Della Pietra", "Vincent J. Della Pietra", "Robert L. Mercer"], "date": "1991", "abstract": "We describe a statistical technique for assigning senses to words. An instance of a word is assigned a sense by asking a question about the context in which the word appears. The question is constructed to have high mutual information with the translation of that instance in another language. When we incorporated this method of assigning senses into our statistical machine translation system, the error rate of the system decreased by thirteen percent.", "references": ["https://www.semanticscholar.org/paper/Automatic-sense-disambiguation-using-machine-how-to-Lesk/76e4e034c20bea86edcc6e71bbaddb47fafeecbc", "https://www.semanticscholar.org/paper/Tagging-text-with-a-probabilistic-model-M%C3%A9rialdo/8a9b6828c5e4339025bb78af6b025d21b4830800", "https://www.semanticscholar.org/paper/A-tree-based-statistical-language-model-for-natural-Bahl-Brown/af386a4e0f2615ed929fdc64a86df8e383bd6121", "https://www.semanticscholar.org/paper/An-information-theoretic-approach-to-the-automatic-Lucassen-Mercer/12666047cb4588405c0c111396c34ceaa0d0f3e0", "https://www.semanticscholar.org/paper/An-iterative-'flip-flop'-approximation-of-the-most-Nadas-Nahamoo/63d2bce4d0d1c1a61ea5d12ade750f558a57b8b6", "https://www.semanticscholar.org/paper/Determination-of-lexical-semantic-relations-for-White/f2f56ab99ea90301af95c2a9565e110792f645dd"]},
{"id": "38224f0aa39e4d4b9a0060e0fe3941f9e6d1bee1", "title": "Models of Translational Equivalence among Words", "authors": ["I. Dan Melamed"], "date": "2000", "abstract": "Parallel texts (bitexts) have properties that distinguish them from other kinds of parallel data. First, most words translate to only one other word. Second, bitext correspondence is typically only partialmany words in each text have no clear equivalent in the other text. This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs. Even the simplest kinds of language-specific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks. Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms.", "references": ["https://www.semanticscholar.org/paper/Identifying-Word-Correspondences-in-Parallel-Texts-Gale-Church/5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "https://www.semanticscholar.org/paper/Automatic-Construction-of-Clean-Broad-Coverage-Melamed/49373d6fac6196753efa8c976e6076d63c0c5f4b", "https://www.semanticscholar.org/paper/Multilingual-domain-modeling-in-Twenty-One%3A-of-a-a-Hiemstra/f02cd2a3a8b7fe72d0be23cbf57bc6e5bc8b7797", "https://www.semanticscholar.org/paper/The-Mathematics-of-Statistical-Machine-Translation%3A-Brown-Pietra/ab7b5917515c460b90451e67852171a531671ab8", "https://www.semanticscholar.org/paper/Models-of-Co-occurrence-Melamed/15693f74eee3728eb576b466de51296298de1e82", "https://www.semanticscholar.org/paper/Automatic-Evaluation-and-Uniform-Filter-Cascades-Melamed/42fd4d469c53e4eedd7eb76e7859e3270367f795", "https://www.semanticscholar.org/paper/Building-An-MT-Dictionary-From-Parallel-Texts-Based-Kumano-Hirakawa/230ffcb119b64b4d0c9e46d1f603a5dc60e93fa8", "https://www.semanticscholar.org/paper/But-Dictionaries-Are-Data-Too-Brown-Pietra/fc593d91a7974bb1d3fac1ffe47b787ce1853a88", "https://www.semanticscholar.org/paper/Manual-Annotation-of-Translational-Equivalence%3A-The-Melamed/027224cecc5cc50f664e7fd249bd4f4545397b96", "https://www.semanticscholar.org/paper/A-statistical-approach-to-language-translation-Brown-Cocke/2166fa493a8c6e40f7f8562d15712dd3c75f03df"]},
{"id": "7560cb8b0bf0fea82f1aa11413007c5d53935ebd", "title": "The Meeting Project at ICSI", "authors": ["Nelson Morgan", "Don Baron", "Jane Edwards", "Daniel P. W. Ellis", "David Gelbart", "Adam Janin", "Thilo Pfau", "Elizabeth Shriberg", "Andreas Stolcke"], "date": "2001", "abstract": "In collaboration with colleagues at UW, OGI, IBM, and SRI, we are developing technology to process spoken language from informal meetings. The work includes a substantial data collection and transcription effort, and has required a nontrivial degree of infrastructure development. We are undertaking this because the new task area provides a significant challenge to current HLT capabilities, while offering the promise of a wide range of potential applications. In this paper, we give our vision of the task, the challenges it represents, and the current state of our development, with particular attention to automatic transcription.", "references": ["https://www.semanticscholar.org/paper/Experiments-in-automatic-meeting-transcription-JRTK-Yu-Clark/d80a535ba470be8df3bdbc362e7f9436ad0dcbef", "https://www.semanticscholar.org/paper/THE-SRI-MARCH-2000-HUB-5-CONVERSATIONAL-SPEECH-Stolcke-Bratt/23318a16f8a049409be848be6e5fcdef22e78012", "https://www.semanticscholar.org/paper/Can-Prosody-Aid-the-Automatic-Classification-of-in-Shriberg-Bates/26e7a79fc061e5907298732e8ad2a5055f34b32c", "https://www.semanticscholar.org/paper/Prosody-based-automatic-segmentation-of-speech-into-Shriberg-Stolcke/05607f5a0c0a1aeb403d9755ca4e2e3d9df58297", "https://www.semanticscholar.org/paper/Dialogue-Act-Modeling-for-Automatic-Tagging-and-of-Stolcke-Ries/22d45dadde6b5837eff11dc031045754bc5901c3", "https://www.semanticscholar.org/paper/RASTA-processing-of-speech-Hermansky-Morgan/a2b439b063874df0a074449c7c8616ac0880c9c5", "https://www.semanticscholar.org/paper/Variation-across-Speech-and-Writing-Biber/f829fbe07e951cb18922225a4d8527db59afe6b9", "https://www.semanticscholar.org/paper/Tandem-connectionist-feature-extraction-for-HMM-Hermansky-Ellis/5e9082caea65c76bfd23b8763872804473ee7872", "https://www.semanticscholar.org/paper/Adaptiver-stochastischer-Sprache%2FPause-Detektor-Beham-Ruske/44ee61e23a6bf3b9f14dfa28189f0f82c7a892cd", "https://www.semanticscholar.org/paper/D-Bonjour-Lettinga/776f6d45b38cf048ff30cea41f0cd3ceeefed622"]},
{"id": "9ba08e0a53bfdcbe0b70a4761c3e2b62f150fc74", "title": "Applications of stochastic context-free grammars using the Inside-Outside algorithm", "authors": ["K. Lari", "Steve J. Young"], "date": "1990", "abstract": "Abstract This paper describes two applications in speech recognition of the use of stochastic context-free grammars (SCFGs) trained automatically via the Inside-Outside Algorithm. First, SCFGs are used to model VQ encoded speech for isolated word recognition and are compared directly to HMMs used for the same task. It is shown that SCFGs can model this low-level VQ data accurately and that a regular grammar based pre-training algorithm is effective both for reducing training time and obtaining robust solutions. Second, an SCFG is inferred from a transcription of the speech used to train a phoneme-based recognizer in an attempt to model phonotactic constraints. When used as a language model, this SCFG gives improved performance over a comparable regular grammar or bigram.", "references": []},
{"id": "2166fa493a8c6e40f7f8562d15712dd3c75f03df", "title": "A statistical approach to language translation", "authors": ["Peter F. Brown", "John Cocke", "Stephen Della Pietra", "Vincent J. Della Pietra", "Frederick Jelinek", "Robert L. Mercer", "Paul S. Roossin"], "date": "1988", "abstract": "An approach to automatic translation is outlined that utilizes techniques of statistical information extraction from large data bases. The method is based on the availability of pairs of large corresponding texts that are translations of each other. In our case, the texts are in English and French.Fundamental to the technique is a complex glossary of correspondence of fixed locutions. The steps of the proposed translation process are: (1) Partition the source text into a set of fixed locutions. (2) Use the glossary plus contextual information to select the corresponding set of fixed locutions into a sequence forming the target sentence. (3) Arrange the words of the target fixed locutions into a sequence forming the target sentence.We have developed statistical techniques facilitating both the automatic creation of the glossary, and the performance of the three translation steps, all on the basis of an alignment of corresponding sentences in the two texts.While we are not yet able to provide examples of French / English translation, we present some encouraging intermediate results concerning glossary creation and the arrangement of target word sequences.", "references": ["https://www.semanticscholar.org/paper/A-Stochastic-Approach-to-Parsing-Sampson/6119418ebd2ebdbf642e62d746ea3d5d06d42b48", "https://www.semanticscholar.org/paper/Stochastic-modeling-for-automatic-speech-Baker/8cf661487d8708a3e9a74e9cc83ce290aa5355b8", "https://www.semanticscholar.org/paper/Classification-and-Regression-Trees-Breiman-Friedman/8017699564136f93af21575810d557dba1ee6fc6"]},
{"id": "1931742167393426ef56523d0ecf53a6e92d0ed0", "title": "Semi-Automatic Acquisition of Domain-Specific Translation Lexicons", "authors": ["Philip Resnik", "I. Dan Melamed"], "date": "1997", "abstract": "We investigate the utility of an algorithm for translation lexicon acquisition (SABLE), used previously on a very large corpus to acquire general translation lexicons, when that algorithm is applied to a much smaller corpus to produce candidates for domain-specific translation lexicons.", "references": ["https://www.semanticscholar.org/paper/Automatic-Construction-of-Clean-Broad-Coverage-Melamed/49373d6fac6196753efa8c976e6076d63c0c5f4b", "https://www.semanticscholar.org/paper/Aligning-Sentences-in-Bilingual-Corpora-using-Chen/1f55d2bca810edbf9870934a41d956d06ae2d9cf", "https://www.semanticscholar.org/paper/New-Experiments-In-Cross-Language-Text-Retrieval-At-Davis/137f12c6bf0f8dad3119419483e6098e2141f7ba", "https://www.semanticscholar.org/paper/Learning-an-English-Chinese-Lexicon-from-a-Parallel-Wu-Xia/13a5c0e48580415e825ecba20d6db50b93949c1b", "https://www.semanticscholar.org/paper/A-Scalable-Architecture-for-Bilingual-Lexicography-Melamed/9737a8b4731179571ae4a3f7e28cc5dfab5daef7", "https://www.semanticscholar.org/paper/Translating-Collocations-for-Bilingual-Lexicons%3A-A-Smadja-McKeown/0285f18f1642c3684e6abb7d5162348278c41abf", "https://www.semanticscholar.org/paper/A-TREC-Evaluation-of-Query-Translation-Methods-For-Davis-Dunning/9ae79906de513c40eb72f73fa9253162e159fc8c", "https://www.semanticscholar.org/paper/Identifying-Word-Correspondences-in-Parallel-Texts-Gale-Church/5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "https://www.semanticscholar.org/paper/Automatic-Evaluation-and-Uniform-Filter-Cascades-Melamed/42fd4d469c53e4eedd7eb76e7859e3270367f795", "https://www.semanticscholar.org/paper/Study-and-Implementation-of-Combined-Techniques-for-Daille/2a4a9c37db04225f21a5b8ce764495a4897783d6"]},
{"id": "823233093025cdc6aec13dbc17d9e4116f686eba", "title": "Precise N-Gram Probabilities from Stochastic Context-Free Grammars", "authors": ["Andreas Stolcke", "Jonathan Segal"], "date": "1994", "abstract": "We present an algorithm for computing n-gram probabilities from stochastic context-free grammars, a procedure that can alleviate some of the standard problems associated with n-grams (estimation from sparse data, lack of linguistic structure, among others). The method operates via the computation of substring expectations, which in turn is accomplished by solving systems of linear equations derived from the grammar. The procedure is fully implemented and has proved viable and useful in practice.", "references": ["https://www.semanticscholar.org/paper/Basic-Methods-of-Probabilistic-Context-Free-Jelinek-Lafferty/617241818e8ddd6edcb4ee7682992673c18c6f3d", "https://www.semanticscholar.org/paper/An-Efficient-Probabilistic-Context-Free-Parsing-Stolcke/79fbfc1dc8846379074aaf4deb7fb0a96722eeed", "https://www.semanticscholar.org/paper/Generalized-Probabilistic-LR-Parsing-of-Natural-Briscoe-Carroll/8ad8e98574a275930bf04a477ce3532fd13c503c", "https://www.semanticscholar.org/paper/Trainable-grammars-for-speech-recognition-Baker/6c79a9bb8f885050cad70b4c69e016b186ffa538", "https://www.semanticscholar.org/paper/Computation-of-Probabilities-for-an-Island-Driven-Corazza-Mori/7db2105e7df22c83cb7b44b2dc736027277042ac", "https://www.semanticscholar.org/paper/Applying-Probability-Measures-to-Abstract-Languages-Booth-Thompson/c9d7b1f9b13d6ea4ff45b908285cc65af959cc5b", "https://www.semanticscholar.org/paper/Class-Based-n-gram-Models-of-Natural-Language-Brown-Pietra/3de5d40b60742e3dfa86b19e7f660962298492af", "https://www.semanticscholar.org/paper/An-efficient-context-free-parsing-algorithm-Earley/62ea2ef8b7d98cf3a3b912a62a7a42ee82650e6b", "https://www.semanticscholar.org/paper/An-Improved-Context-Free-Recognizer-Graham-Harrison/9274889f719b8308a0e389b2251d2a2b64c03e84", "https://www.semanticscholar.org/paper/Pearl%3A-A-Probabilistic-Chart-Parser-Magerman-Marcus/da838db79e7593018894ada44db35eee670941d6"]},
{"id": "10a463bb00b44bdd3a8620f2bedb9e1564bfcf32", "title": "The Design and Analysis of Computer Algorithms", "authors": ["Alfred V. Aho", "John E. Hopcroft", "Jeffrey D. Ullman"], "date": "1974", "abstract": "From the Publisher: \nWith this text, you gain an understanding of the fundamental concepts of algorithms, the very heart of computer science. It introduces the basic data structures and programming techniques often used in efficient algorithms. Covers use of lists, push-down stacks, queues, trees, and graphs. Later chapters go into sorting, searching and graphing algorithms, the string-matching algorithms, and the Schonhage-Strassen integer-multiplication algorithm. Provides numerous graded exercises at the end of each chapter. \n \n \n0201000296B04062001", "references": ["https://www.semanticscholar.org/paper/Symposium-on-complexity-of-sequential-and-parallel-Traub/e99971282397045d7c1019386da960f61c32aa60", "https://www.semanticscholar.org/paper/Computational-complexity-of-random-access-stored-Hartmanis/fc7d54efef1eb4e12936f6209bc2595e49d00e65", "https://www.semanticscholar.org/paper/An-improved-equivalence-algorithm-Galler-Fischer/3eed5bd2c67c8f8a44044f7defd7d245aa139647", "https://www.semanticscholar.org/paper/Efficiency-of-Equivalence-Algorithms-Fischer/b53abfb9bd4e5a16945b0d757030208fe2f6b3d4", "https://www.semanticscholar.org/paper/Analysis-and-Synthesis-of-Sorting-Algorithms-Liu/3928a8780a368c06348bfc1ee9ffca06db2cf6cd", "https://www.semanticscholar.org/paper/A-sorting-problem-and-its-complexity-Pohl/2b451c3113705398a754c2162fb64efdcf1d6678", "https://www.semanticscholar.org/paper/Determining-graph-properties-from-matrix-Kirkpatrick/6ac25e53203669662547805a1b0f788b1c1a38ef", "https://www.semanticscholar.org/paper/AN-IMPROVED-OVERLAP-ARGUMENT-FOR-ON-LINE-Paterson-Fischer/9f89d28ef7834b18c1cde9574e1dbb6bc7d6625a", "https://www.semanticscholar.org/paper/ON-THE-ALGEBRAIC-COMPLEXITY-OF-FUNCTIONS-Winograd/a92fc7910a9dc862e428d79803bc182792def8dc", "https://www.semanticscholar.org/paper/Testing-flow-graph-reducibility-Tarjan/69fc1a5a0168467ffdd262a06049894390f2192b"]},
{"id": "d83cf58987413ad3c132bbbabd12d149b47acc2e", "title": "Analyzing research papers using citation sentences", "authors": ["Wendy G. Lehnert", "Claire Cardie"], "date": "1990", "abstract": "Semantic Scholar extracted view of \"Analyzing research papers using citation sentences\" by Wendy G. Lehnert et al.", "references": []},
{"id": "ae287b7ecbb3dffaa0e302acfd7bbf44f733acfc", "title": "Coreference for NLP Applications", "authors": ["Thomas S. Morton"], "date": "2000", "abstract": "This paper presents several techniques for performing automatic coreference annotation and performance results for each of them. To demonstrate that they can be applied to real-world data, we have built a simple question-answering system which uses the techniques. A system using coreference is compared to a baseline system with the result that the addition of the coreference annotation improves performance.", "references": []},
{"id": "c1c52336c170641247ffafc2be17e2e759855682", "title": "The DARPA 1000-Word Resource Management Database for Continuous Speech Recognition", "authors": ["A. B. Poritz", "A. G. Richter"], "date": "1986", "abstract": "Semantic Scholar extracted view of \"The DARPA 1000-Word Resource Management Database for Continuous Speech Recognition\" by A. B. Poritz et al.", "references": []},
{"id": "44ee61e23a6bf3b9f14dfa28189f0f82c7a892cd", "title": "Adaptiver stochastischer Sprache/Pause-Detektor", "authors": ["Manfred Beham", "G\u00fcnther Ruske"], "date": "1995", "abstract": "Bei der automatischen Spracherkennung mus eine Reihe von einzeben Teilaufgaben gelost werden, zu denen auch die Anzeige von Beginn und Ende der gesprochenen Auserung gehort. Diese Aufgabe ist durchaus nichttrivial, besonders wenn eine fehlerhafte Anzeige zu einer Beschneidung der Auserung und damit letztlich zu Erkennungsfehlem fuhrt. Daher wird in vielen Spraeherkennungssystemen angestrebt, vor Beginn der Auserung und nach dessen Ende moglichst einen Signalbereich mit aufzunehmen, der eine Sprachpause enthalt. Es wird dann versucht, wahrend der Erkennung mit Hilfe spezieller Pausenmodelle die Pausenbereiche zu kennzeichnen und zu eliminieren. Dieses Vorgehen kann man als \u201cindirekte\u201c Sprache/Pause- Detektion bezeichnen. Sinnvoll einsetzen last sich diese Methode vor allem im sogenannten Offline-Betrieb, bei dem die Sprachdaten in Dateien gespeichert sind Problematisch ist die indirekte Sprache/Pause-Detektion aber im Onune-Betrieb, bei dem das eingehende Sprachsignal unmittelbar und schritthaltend verarbeitet werden soll. Hier wirkt sich nachteilig aus, das die Feststellung des Sprachbeginns erst nach der Erkennung geliefert wird, so das ein zu spat angezeigter Beginn nicht mehr korrigiert werden kann.", "references": ["https://www.semanticscholar.org/paper/Robust-HMM-based-endpoint-detector-Acero-Crespo-Casas/a87b30feba7717e90aa5891a2798ef4ef9ad8740", "https://www.semanticscholar.org/paper/A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner/8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "https://www.semanticscholar.org/paper/A-continuous-speech-recognition-system-integrating-Plannerer-Einsele/f599f8b5a1bba16026ab7a108c35104d3f1d8920"]},
{"id": "6d2da776b4a1c40bb7aeb59b5fa29897b5c66ccc", "title": "Data-Derived Non-Linear Mapping for Feature Extraction in HMM", "authors": ["Hynek Hermansky", "S. Rahul Sharma", "Pratibha Jain"], "date": "1999", "abstract": "Rather long temporal trajectory of critical band logarithmic power spectrum energy at a given frequency is used as an input feature vector in a MLP-based phoneme classi er, trained on a task-independent hand-labeled development data. Class-speci c log likelihood vectors from the individual sub-classi ers form input to a merging MLP classi er trained on the training data. Output of this merging classi er forms a feature vector for subsequent HMM ASR.", "references": ["https://www.semanticscholar.org/paper/Data-driven-design-of-RASTA-like-filters-Vuuren-Hermansky/b593228c12fb788ea233d558547a40c49b55424b", "https://www.semanticscholar.org/paper/Relevancy-of-time-frequency-features-for-phonetic-Yang-Vuuren/9acda72e66b8709be5e939cf6fccb67e1db0b253", "https://www.semanticscholar.org/paper/A-mew-ASR-approach-based-on-independent-processing-Bourlard-Dupont/38b8f6097118c8d7e81ce9d26723af80be77ba39", "https://www.semanticscholar.org/paper/Global-optimization-of-a-neural-network-hidden-Bengio-Mori/ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "https://www.semanticscholar.org/paper/Towards-ASR-on-partially-corrupted-speech-Hermansky-Tibrewala/6c8c474a84dbcf5928ee31051bfdcf9f9de2c944", "https://www.semanticscholar.org/paper/Multi-band-and-adaptation-approaches-to-robust-Tibrewala-Hermansky/888ab218d2e7cc134402b5aacaeab2025f7e5d49", "https://www.semanticscholar.org/paper/Temporal-patterns-(TRAPs)-in-ASR-of-noisy-speech-Hermansky-Sharma/fcb2050b13b206876c76dec5b9051cfc7509b26f", "https://www.semanticscholar.org/paper/Connectionist-Speech-Recognition%3A-A-Hybrid-Approach-Bourlard-Morgan/3d82e058a5c40954b8f5db170a298a889a254c37", "https://www.semanticscholar.org/paper/Recognition-of-consonant-based-on-the-perceptron-Makino-Kawabata/8f93ee2e335d83ee32787693861f4d48e78e6786", "https://www.semanticscholar.org/paper/TRAPS-classifiers-of-temporal-patterns-Hermansky-Sharma/72f3f68664e27745e8f63fdc6cfd0eb5c37dc844"]},
{"id": "4db594b948e95b3239f733b1625b54b0fb476380", "title": "Enabling new speech driven services for mobile devices: An overview of the ETSI standards activities", "authors": ["Darren Pearce"], "date": "2000", "abstract": "Semantic Scholar extracted view of \"Enabling new speech driven services for mobile devices: An overview of the ETSI standards activities\" by Darren Pearce", "references": []},
{"id": "f43b57fa22d3b8275caa403d7b26f3681a888421", "title": "Coreference Resolution Using Competition Learning Approach", "authors": ["Xiaofeng Yang", "Guodong Zhou", "Jian Su", "Chew Lim Tan"], "date": "2003", "abstract": "In this paper we propose a competition learning approach to coreference resolution. Traditionally, supervised machine learning approaches adopt the single-candidate model. Nevertheless the preference relationship between the antecedent candidates cannot be determined accurately in this model. By contrast, our approach adopts a twin-candidate learning model. Such a model can present the competition criterion for antecedent candidates reliably, and ensure that the most preferred candidate is selected. Furthermore, our approach applies a candidate filter to reduce the computational cost and data noises during training and resolution. The experimental results on MUC-6 and MUC-7 data set show that our approach can outperform those based on the single-candidate model.", "references": ["https://www.semanticscholar.org/paper/A-Machine-Learning-Approach-to-Coreference-of-Noun-Soon-Ng/a20bfec3c95aad003dcb45a21a220c19cca8bb66", "https://www.semanticscholar.org/paper/Improving-Machine-Learning-Approaches-to-Resolution-Ng-Gardent/08c81389b3ac4b8253d718a7cebe04a5536efa78", "https://www.semanticscholar.org/paper/A-model-theoretic-coreference-scoring-scheme-Vilain-Burger/890c1201d3edef7104ba9d75078ed90f9c30c93b", "https://www.semanticscholar.org/paper/A-trainable-approach-to-coreference-resolution-for-McCarthy-Lehnert/ae130776725555ec7b4683fd8089a7d55d66e083", "https://www.semanticscholar.org/paper/Robust-Pronoun-Resolution-with-Limited-Knowledge-Mitkov/d032db4e8c1cf2b02867f95fa8fe847f5bfdf6ab", "https://www.semanticscholar.org/paper/Evaluating-Automated-and-Manual-Acquisition-of-Aone-Bennett/6f7ac41dc9321cb7478919de1da6da70009023da", "https://www.semanticscholar.org/paper/Identifying-Anaphoric-and-Non-Anaphoric-Noun-to-Ng-Cardie/e414e980005c1066c539dd893a14188371792a01", "https://www.semanticscholar.org/paper/A-Centering-Approach-to-Pronouns-Brennan-Friedman/834d400f167fcf6ba27642eafa42e63f9e699a94", "https://www.semanticscholar.org/paper/Never-Look-Back%3A-An-Alternative-to-Centering-Strube/d50602b6aad3dfa6c098bfea59b8fa5b3eb75edc", "https://www.semanticscholar.org/paper/A-Corpus-Based-Evaluation-of-Centering-and-Pronoun-Tetreaul/c70156cd37773f40d3c8a43c6ac3135499fe8891"]},
{"id": "0b45d669ac412a98569fcb04ac766a8440927e8e", "title": "Transcription of broadcast television and radio news: the 1996 ABBOT system", "authors": ["Gary D. Cook", "Dan J. Kershaw", "James Christie", "Carl W. Seymour", "Steve R. Waterhouse"], "date": "1997", "abstract": "This paper describes the development of the CU-CON system which participated in the 1996 ARPA Hub 4 Evaluations. The system is based on ABBOT, a hybrid connectionist-HMM large vocabulary continuous speech recognition system developed at the Cambridge University Engineering Department by Hochberg et al. (1995). The Hub 4 Evaluation task involves the transcription of broadcast television and radio news programmes. This is an extremely demanding task for a state-of-the-art speech recognition system. Typical programmes include a wide variety of speaking styles and acoustic conditions. These range from read speech recorded in the studio to extemporaneous speech recorded over telephone channels.", "references": ["https://www.semanticscholar.org/paper/The-1995-ABBOT-LVCSR-system-for-multiple-unknown-Kershaw-Robinson/fcf4460ca1e9903dde83ec13833f4f8a643d5933", "https://www.semanticscholar.org/paper/The-1994-Abbot-hybrid-connectionist-HMM-large-Hochberg-Cook/c2729ba54d693730b88ac4a748f0caf286b5bf71", "https://www.semanticscholar.org/paper/DECODER-TECHNOLOGY-FOR-CONNECTIONIST-LARGE-SPEECH-Renals-Hochberg/b5aaf01ed4b8769116e90ecf3132f8f221b8f54e", "https://www.semanticscholar.org/paper/Continuous-speech-recognition-by-connectionist-Bourlard-Morgan/c4914df5c8b886e85502bba35a35741d05639bd8", "https://www.semanticscholar.org/paper/Connectionist-model-combination-for-large-speech-Hochberg-Cook/f01ee064aa9fedf17acaaeb888eac74c9a0499e7", "https://www.semanticscholar.org/paper/The-Use-of-Recurrent-Neural-Networks-in-Continuous-Robinson-Hochberg/03bc854feaee144b54924b440eff02ed9082cc6b", "https://www.semanticscholar.org/paper/Speaker-adaptation-for-hybrid-HMM-ANN-continuous-Neto-Almeida/83fab3b9a434cd3a93e2367bd52af5c3ea66c7b2", "https://www.semanticscholar.org/paper/Boosting-the-performance-of-connectionist-large-Cook-Robinson/fa59dbbdaebb2bb4f138e645c0ec7b550c05b385", "https://www.semanticscholar.org/paper/Connectionist-Speech-Recognition%3A-A-Hybrid-Approach-Bourlard-Morgan/3d82e058a5c40954b8f5db170a298a889a254c37", "https://www.semanticscholar.org/paper/An-application-of-recurrent-nets-to-phone-Robinson/c6629770cb6a00ad585918e71fe6dbad829ad0d1"]},
{"id": "81f87408690b155ee08896acf33cf1b74be60460", "title": "A decision theoretic approach to hierarchical classifier design", "authors": ["J\u00fcrgen Sch\u00fcrmann", "Wolfgang Doster"], "date": "1984", "abstract": "Abstract The design of tree classifiers is considered from the statistical point of view. The procedure for calculating the a posteriori probabilities is decomposed into a sequence of steps. In every step the a posteriori probabilities for a certain subtask of the given pattern recognition task are calculated. The resulting tree classifier realizes a soft-decision strategy in contrast to the hard-decision strategy of the conventional decision tree. At the different nonterminal nodes, mean square polynomial classifiers are applied having the property of estimating the desired a posteriori probabilities together with an integrated feature selection capability.", "references": []},
{"id": "3d82e058a5c40954b8f5db170a298a889a254c37", "title": "Connectionist Speech Recognition: A Hybrid Approach", "authors": ["Herv\u00e9 Bourlard", "Nelson Morgan"], "date": "1993", "abstract": "From the Publisher: \nConnectionist Speech Recognition: A Hybrid Approach describes the theory and implementation of a method to incorporate neural network approaches into state-of-the-art continuous speech recognition systems based on Hidden Markov Models (HMMs) to improve their performance. In this framework, neural networks (and in particular, multilayer perceptrons or MLPs) have been restricted to well-defined subtasks of the whole system, i.e., HMM emission probability estimation and feature extraction. The book describes a successful five year international collaboration between the authors. The lessons learned form a case study that demonstrates how hybrid systems can be developed to combine neural networks with more traditional statistical approaches. The book illustrates both the advantages and limitations of neural networks in the framework of a statistical system. Using standard databases and comparing with some conventional approaches, it is shown that MLP probability estimation can improve recognition performance. Other approaches are discussed, though there is no such unequivocal experimental result for these methods. Connectionist Speech Recognition: A Hybrid Approach is of use to anyone intending to use neural networks for speech recognition or within the framework provided by an existing successful statistical approach. This includes research and development groups working in the field of speech recognition, both with standard and neural network approaches, as well as other pattern recognition and/or neural network researchers. This book is also suitable as a text for advanced courses on neural networks or speech processing.", "references": []},
{"id": "f743e6b3e71cb99cf336c5f320efd21acb126dc3", "title": "Speaker adaptation of HMMs using linear regression", "authors": ["Chris Leggetter", "Philip C. Woodland"], "date": "1994", "abstract": "Semantic Scholar extracted view of \"Speaker adaptation of HMMs using linear regression\" by Chris Leggetter et al.", "references": []},
{"id": "5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "title": "Identifying Word Correspondences in Parallel Texts", "authors": ["William A. Gale", "Kenneth Ward Church"], "date": "1991", "abstract": "Researchers in both machine translation (e.g., Brown et a/, 1990) arm bilingual lexicography (e.g., Klavans and Tzoukermarm, 1990) have recently become interested in studying parallel texts (also known as bilingual corpora), bodies of text such as the Canadian Hansards (parliamentary debates) which are available in multiple languages (such as French and English). Much of the current excitement surrounding parallel texts was initiated by Brown et aL (1990), who outline a selforganizing method for using these parallel texts to build a machine translation system.", "references": ["https://www.semanticscholar.org/paper/A-Program-for-Aligning-Sentences-in-Bilingual-Gale-Church/4fe2a45babab10c1bfae05d2464363f4e52bbaf9", "https://www.semanticscholar.org/paper/Text-Translation-Alignment-Kay-R%C3%B6scheisen/a0b61625e60a419bd5ea1d892047a65a73d9f0c4", "https://www.semanticscholar.org/paper/Aligning-Sentences-in-Parallel-Corpora-Brown-Lai/a76563076016fb1cb813deba45db2409772a51da", "https://www.semanticscholar.org/paper/A-Statistical-Approach-to-Machine-Translation-Brown-Cocke/a1066659ec1afee9dce586f6f49b7d44527827e1", "https://www.semanticscholar.org/paper/A-stochastic-parts-program-and-noun-phrase-parser-Hubbell/f853daccfcb2350f9adcd75331d148b04c21e5ef", "https://www.semanticscholar.org/paper/Bilingual-Concordancing-and-Bilingual-Lexicography-Warwick-Russell/983548e49ce6693103b68bb579d04cf61bcaac8c", "https://www.semanticscholar.org/paper/Deriving-Translation-Data-from-Bilingual-Texts-Catizone-Russell/6d9a739c2bdbadf0d355f9c19d09ed89117a9589", "https://www.semanticscholar.org/paper/The-BICORD-System-Klavans-Tzoukermann/b4c6cd6879df810fee431ae1cf16ac35d3b2c239"]},
{"id": "9737a8b4731179571ae4a3f7e28cc5dfab5daef7", "title": "A Scalable Architecture for Bilingual Lexicography", "authors": ["I. Dan Melamed"], "date": "1997", "abstract": "SABLE is a Scalable Architecture for Bilingual LExicography. It is designed to produce clean broad-coverage translation lexicons from raw, unaligned parallel texts. Its black-box functionality makes it suitable for naive users. The architecture has been implemented for different language pairs, and has been tested on very large and noisy input. SABLE does not rely on language-specific resources such as part-of-speech taggers, but it can take advantage of them when they are available. Comments University of Pennsylvania Department of Computer and Information Science Technical Report No. MSCIS-97-01. This technical report is available at ScholarlyCommons: http://repository.upenn.edu/cis_reports/209 A Scalable Architecture for Bilingual Lexicography", "references": ["https://www.semanticscholar.org/paper/Automatic-Evaluation-and-Uniform-Filter-Cascades-Melamed/42fd4d469c53e4eedd7eb76e7859e3270367f795", "https://www.semanticscholar.org/paper/Identifying-Word-Correspondences-in-Parallel-Texts-Gale-Church/5051e54af8c0d6b84f8459c57d55bd19be9f0cee", "https://www.semanticscholar.org/paper/A-Geometric-Approach-to-Mapping-Bitext-Melamed/798d4a8a8f2ea65c3b7eb0ac1100698b269e68f9", "https://www.semanticscholar.org/paper/A-Freely-Available-Wide-Coverage-Morphological-for-Karp-Schabes/d5bd9ad9a3b49ce936679a3c2bf5ec7277bcb5f5", "https://www.semanticscholar.org/paper/Semi-Automatic-Acquisition-of-Domain-Specific-Resnik-Melamed/1931742167393426ef56523d0ecf53a6e92d0ed0", "https://www.semanticscholar.org/paper/The-Mathematics-of-Statistical-Machine-Translation%3A-Brown-Pietra/ab7b5917515c460b90451e67852171a531671ab8", "https://www.semanticscholar.org/paper/Porting-SIMR-to-New-Language-Pairs-Melamed/7bd2ccbb7893434bbaf7dd54f23ea31ee7d1e9b5", "https://www.semanticscholar.org/paper/Peut-on-v%C3%A9rifier-automatiquement-la-coh%C3%A9rence-Macklovitch/f9eb0ed8d028171f1e2cf69db0010584ce07b44a"]},
{"id": "a2b439b063874df0a074449c7c8616ac0880c9c5", "title": "RASTA processing of speech", "authors": ["Hynek Hermansky", "Nelson Morgan"], "date": "1994", "abstract": "Performance of even the best current stochastic recognizers severely degrades in an unexpected communications environment. In some cases, the environmental effect can be modeled by a set of simple transformations and, in particular, by convolution with an environmental impulse response and the addition of some environmental noise. Often, the temporal properties of these environmental effects are quite different from the temporal properties of speech. We have been experimenting with filtering approaches that attempt to exploit these differences to produce robust representations for speech recognition and enhancement and have called this class of representations relative spectra (RASTA). In this paper, we review the theoretical and experimental foundations of the method, discuss the relationship with human auditory perception, and extend the original method to combinations of additive noise and convolutional noise. We discuss the relationship between RASTA features and the nature of the recognition models that are required and the relationship of these features to delta features and to cepstral mean subtraction. Finally, we show an application of the RASTA technique to speech enhancement. >", "references": ["https://www.semanticscholar.org/paper/Integrating-RASTA-PLP-into-speech-recognition-Koehler-Morgan/25e14210bb13f8c2db2626e3c0a5ff2e6fdcb05c", "https://www.semanticscholar.org/paper/Quality-improvement-of-LPC-processed-noisy-speech-Kang-Fransen/b63bbb5b5a509b6f8f12df9350cba00aa3fdfc82", "https://www.semanticscholar.org/paper/Continuous-speech-recognition-using-PLP-analysis-Morgan-Hermansky/3e49679db76aae02c07b802e304e3ab142a4390e", "https://www.semanticscholar.org/paper/Speaker-independent-isolated-word-recognition-based-Furui/8201b613108b843b026d31a8018943f053bad7d5", "https://www.semanticscholar.org/paper/Compensation-for-the-effect-of-the-communication-in-Hermansky-Morgan/989001bcd31cf3f2d0088cee665dc25404e3baf6"]},
{"id": "2b451c3113705398a754c2162fb64efdcf1d6678", "title": "A sorting problem and its complexity", "authors": ["Ira Pohl"], "date": "1972", "abstract": "A technique for proving min-max norms of sorting algorithms is given. One new algorithm for finding the minimum and maximum elements of a set with fewest comparisons is proved optimal with this technique.", "references": []},
{"id": "6c79a9bb8f885050cad70b4c69e016b186ffa538", "title": "Trainable grammars for speech recognition", "authors": ["James K. Baker"], "date": "1979", "abstract": "Algorithms which are based on modeling speech as a finite\u2010state, hidden Markov process have been very successful in recent years. This paper presents a generalization of these algorithms to certain denumerable\u2010state, hidden Markov processes. This algorithm permits automatic training of the stochastic analog of an arbitrary context free grammar. In particular, in contrast to many grammatical inference methods, the new algorithm allows the grammar to have an arbitrary degree of ambiguity. Since natural language is often syntactically ambiguous, it is necessary for the grammatical inference algorithm to allow for this ambiguity. Furthermore, allowing ambiguity in the grammar allows errors in the recognition process to be explicitly modeled in the grammar rather than added as an extra component.", "references": []},
{"id": "3eed5bd2c67c8f8a44044f7defd7d245aa139647", "title": "An improved equivalence algorithm", "authors": ["Bernard A. Galler", "Michael J. Fischer"], "date": "1964", "abstract": "An algorithm for assigning storage on the basis of EQUIVALENCE, DIMENSION and COMMON declarations is presented. The algorithm is based on a tree structure, and has reduced computation time by 40 percent over a previously published algorithm by identifying all equivalence classes with one scan of the EQUIVALENCE declarations. The method is applicable in any problem in which it is necessary to identify equivalence classes, given the element pairs defining the equivalence relation.", "references": []},
{"id": "26e7a79fc061e5907298732e8ad2a5055f34b32c", "title": "Can Prosody Aid the Automatic Classification of Dialog Acts in Conversational Speech?", "authors": ["Elizabeth Shriberg", "Rebecca A. Bates", "Andreas Stolcke", "Paul Taylor", "Dan Jurafsky", "Klaus Ries", "Noah Coccaro", "Rachel Martin", "Marie Meteer", "Carol Van Ess-Dykema"], "date": "1998", "abstract": "Identifying whether an utterance is a statement, question, greeting, and so forth is integral to effective automatic understanding of natural dialog. Little is known, however, about how such dialog acts (DAs) can be automatically classified in truly natural conversation. This study asks whether current approaches, which use mainly word information, could be improved by adding prosodic information. The study is based on more than 1000 conversations from the Switchboard corpus. DAs were hand-annotated, and prosodic features (duration, pause, F0, energy, and speaking rate) were automatically extracted for each DA. In training, decision trees based on these features were inferred; trees were then applied to unseen test data to evaluate performance. Performance was evaluated for prosody models alone, and after combining the prosody models with word information--either from true words or from the output of an automatic speech recognizer. For an overall classification task, as well as three subtasks, prosody made significant contributions to classification. Feature-specific analyses further revealed that although canonical features (such as F0 for questions) were important, less obvious features could compensate if canonical features were removed. Finally, in each task, integrating the prosodic model with a DA-specific statistical language model improved performance over that of the language model alone, especially for the case of recognized words. Results suggest that DAs are redundantly marked in natural conversation, and that a variety of automatically extractable prosodic features could aid dialog processing in speech applications.", "references": ["https://www.semanticscholar.org/paper/Dialog-Act-Modeling-for-Conversational-Speech-Shriberg-Jurafsky/436c3119d16ce2e3c243ffe7a4a1a5dc40b128aa", "https://www.semanticscholar.org/paper/First-steps-towards-statistical-modeling-of-to-the-Nagata-Morimoto/f6d2d51adbf12ab3740cb7ce07ebb6ee85b387fd", "https://www.semanticscholar.org/paper/Prosodic-and-lexical-indications-of-discourse-in-Swerts-Ostendorf/e4ed4a8cbfd5f1b161151a743e7bc67f9f648d51", "https://www.semanticscholar.org/paper/A-Study-on-Prosody-and-Discourse-Structure-in-Nakajima-Allen/09ee63e2a8d112d9fde0de244ddc5d1dc5697061", "https://www.semanticscholar.org/paper/Coding-Dialogs-with-the-DAMSL-Annotation-Scheme-Core-Allen/ac2b71f9dbafb5678334a41a05cdf928e2dac447", "https://www.semanticscholar.org/paper/Dialog-act-classification-with-the-help-of-prosody-Mast-Kompe/92d5079b696d8ec2c35761c4d22256bc298967f6", "https://www.semanticscholar.org/paper/Integrated-dialog-act-segmentation-and-using-and-Warnke-Kompe/bba48db6de8fdacc5db6203c06e2230dc30f962d", "https://www.semanticscholar.org/paper/Prosodic-scoring-of-word-hypotheses-graphs-Kompe-Kie%C3%9Fling/23f998081393772f40ee936238b138ae801dded8", "https://www.semanticscholar.org/paper/%22Roger%22%2C-%22Sorry%22%2C-%22I'm-still-listening%22-%3A-dialog-in-Kie%C3%9Fling-Kompe/c343f96efeef603c5880edfb85668aacd2f78f59", "https://www.semanticscholar.org/paper/Inferring-linguistic-structure-in-spoken-language-Woszczyna-Waibel/23cba88fec7f54185b286ce332a51907ff15e2fb"]},
{"id": "69fc1a5a0168467ffdd262a06049894390f2192b", "title": "Testing flow graph reducibility", "authors": ["Robert E. Tarjan"], "date": "1973", "abstract": "Many problems in program optimization have been solved by applying a technique called interval analysis to the flow graph of the program. A flow graph which is susceptible to this type of analysis is called reducible. This paper describes an algorithm for testing whether a flow graph is reducible. The algorithm uses depth-first search to reveal the structure of the flow graph and a good method for computing disjoint set unions to determine reducibility from the search information. When the algorithm is implemented on a random access computer, it requires O(E log* E) time to analyze a graph with E edges, where log* x &equil; min{i/logix\u22641}. The time bound compares favorably with the O(E log E) bound of a previously known algorithm.", "references": []},
{"id": "fc7d54efef1eb4e12936f6209bc2595e49d00e65", "title": "Computational complexity of random access stored program machines", "authors": ["Juris Hartmanis"], "date": "2005", "abstract": "In this paper we explore the computational complexity measure defined by running times of programs on random access stored program machines, RASP's. The purpose of this work is to study more realistic complexity measures and to provide a setting and some techniques to explore different computer organizations. The more interesting results of this paper are obtained by an argument about the size of the computed functions. For example, we show (without using diagonalization) that there exist arbitrarily complex functions with optimal RASP programs whose running time cannot be improved by any multiplicative constant. We show, furthermore, that these optimal programs cannot be fixed procedures and determine the difference in computation speed between fixed procedures and self-modifying programs. The same technique is used to compare computation speed of machines with and without built-in multiplication. We conclude the paper with a look at machines with associative memory and distributed logic machines.", "references": ["https://www.semanticscholar.org/paper/Tape-Reversal-Complexity-Hierarchies-Fischer-Hartmanis/410ea9b1ec562f05d52b75cefe1fc9aa231bd52e", "https://www.semanticscholar.org/paper/Computational-Complexity-of-One-Tape-Turing-Machine-Hartmanis/e5e44b3ee7a44b56212b1f19a8df042e4e1617cd"]},
{"id": "ffbe67c217967b6bfb0a5ecc0dc4cdd5cda65776", "title": "Global optimization of a neural network-hidden Markov model hybrid", "authors": ["Yoshua Bengio", "Renato De Mori", "Giovanni Flammia", "Ralf Kompe"], "date": "1992", "abstract": "The integration of multilayered and recurrent artificial neural networks (ANNs) with hidden Markov models (HMMs) is addressed. ANNs are suitable for approximating functions that compute new acoustic parameters, whereas HMMs have been proven successful at modeling the temporal structure of the speech signal. In the approach described, the ANN outputs constitute the sequence of observation vectors for the HMM. An algorithm is proposed for global optimization of all the parameters. Results on speaker-independent recognition experiments using this integrated ANN-HMM system on the TIMIT continuous speech database are reported.", "references": ["https://www.semanticscholar.org/paper/Phonetically-motivated-acoustic-parameters-for-Bengio-Mori/74e6672060e48a05de79ff9b09b093417e75a4db", "https://www.semanticscholar.org/paper/Speaker-independent-phone-recognition-using-hidden-Lee-Hon/3034afcd45fc190ed71982828b77f6e4154bdc5c", "https://www.semanticscholar.org/paper/Competitive-training-in-hidden-Markov-models-Young/15d5d39ca4333de3f18b4125233d8567d3aae6cb", "https://www.semanticscholar.org/paper/Speaker-Independent-Speech-Recognition-with-Neural-Bengio-Mori/49a09dbad1eef44b360d2a87621a544b64e1c5b3", "https://www.semanticscholar.org/paper/Connectionist-Viterbi-training%3A-a-new-hybrid-method-Franzini-Lee/f866ac085771f5676800db2d9b102975b2a1b2d7", "https://www.semanticscholar.org/paper/A-systolic-neural-network-architecture-for-hidden-Hwang-Vlontzos/c1e32f7d9fb6d849ceadd63cb7e446f39b911af5", "https://www.semanticscholar.org/paper/On-a-model-robust-training-method-for-speech-N%C3%A1das-Nahamoo/039900eaeeddd13752aa8d6c61759f0b0e54f0de", "https://www.semanticscholar.org/paper/Speaker-independent-isolated-digit-recognition%3A-vs.-Bottou-Fogelman-Souli%C3%A9/830a0c617b99a2cd39517f699fe376442c662816", "https://www.semanticscholar.org/paper/The-Potential-Role-of-Property-Detectors-in-the-of-Stevens/1c9781457be3b0eb58c47576d4c8bc58daf82114", "https://www.semanticscholar.org/paper/Phoneme-Recognition-from-the-TIMIT-database-using-Robinson-Fallside/d33c7c733a5960827fe6abe841ef1faa68cef6f4"]},
{"id": "b593228c12fb788ea233d558547a40c49b55424b", "title": "Data-driven design of RASTA-like filters", "authors": ["Sarel van Vuuren", "Hynek Hermansky"], "date": "1997", "abstract": "We describe use of Linear Discriminant Analysis (LDA) for data-driven automatic design of RASTA-like lters. The LDA applied to rather long segments of time trajectories of critical-band energies yields FIR lters to be applied to these time trajectories in the feature extraction module. Frequency responses of the rst three discriminant vectors are in principle consistent with the ad hoc designed RASTA, delta and double-delta lters. On a connected digit task the new features outperform the original RASTA processing.", "references": ["https://www.semanticscholar.org/paper/Channel-normalization-techniques-for-automatic-over-Veth-Boves/caf8a93ff5011dbafce9af352cc70a0a6772c271", "https://www.semanticscholar.org/paper/A-statistical-approach-to-metrics-for-word-and-Hunt/cd3d6af0796aae744bb3fbc4640a58f96abd6057", "https://www.semanticscholar.org/paper/RASTA-processing-of-speech-Hermansky-Morgan/a2b439b063874df0a074449c7c8616ac0880c9c5", "https://www.semanticscholar.org/paper/Exploring-Temporal-Domain-for-Robustness-in-Speech-Hermansky/4458f641aeb1e0d8c8915a6f71c0c60830ddc5ec", "https://www.semanticscholar.org/paper/On-the-robust-automatic-segmentation-of-spontaneous-Petek-Andersen/b111102d0a9ef39a1b7dea36b368f7945ea8cd2d", "https://www.semanticscholar.org/paper/The-acoustic-modeling-problem-in-automatic-speech-Brown/1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "https://www.semanticscholar.org/paper/Cepstral-Analysis-for-Automatic-Speaker-Furui/5862853a2b1e09cb0d95695ab13ac5ecd6cb2584", "https://www.semanticscholar.org/paper/The-Switchboard-Transcription-Project-Greenberg/6d339d36c81e6b46b38eb9599897fa1a53f4605f"]},
{"id": "c70156cd37773f40d3c8a43c6ac3135499fe8891", "title": "A Corpus-Based Evaluation of Centering and Pronoun Resolution", "authors": ["Joel R. Tetreaul"], "date": "2001", "abstract": "In this paper we compare pronoun resolution algorithms and introduce a centering algorithm(Left-Right Centering) that adheres to the constraints and rules of centering theory and is an alternative to Brennan, Friedman, and Pollard's (1987) algorithm. We then use the Left-Right Centering algorithm to see if two psycholinguistic claims on Cf-list ranking will actually improve pronoun resolution accuracy. Our results from this investigation lead to the development of a new syntax-based ranking of the Cf-list and corpus-based evidence that contradicts the psycholinguistic claims.", "references": ["https://www.semanticscholar.org/paper/Analysis-of-Syntax-Based-Pronoun-Resolution-Methods-Tetreault/54744d6219a0722ffe68621730f61a0ac068af4c", "https://www.semanticscholar.org/paper/A-Centering-Approach-to-Pronouns-Brennan-Friedman/834d400f167fcf6ba27642eafa42e63f9e699a94", "https://www.semanticscholar.org/paper/Functional-Centering-Grounding-Referential-in-Strube-Hahn/f72fc3de9c493dd8de5dcd852a8eaccbdd482094", "https://www.semanticscholar.org/paper/Never-Look-Back%3A-An-Alternative-to-Centering-Strube/d50602b6aad3dfa6c098bfea59b8fa5b3eb75edc", "https://www.semanticscholar.org/paper/Intrasentential-Centering%3A-A-Case-Study-Kameyama/78bd476c27236d18dfaf9e80b286bacfa843a978", "https://www.semanticscholar.org/paper/Resolving-pronoun-references-Hobbs/61c0eaf156647ffad23921a65e3d7b3296f7afd5", "https://www.semanticscholar.org/paper/Functional-Centering-Strube-Hahn/a46bc99d90bd064e9ccf4def70ac89b27537283b", "https://www.semanticscholar.org/paper/Pronouns%2C-Names%2C-and-the-Centering-of-Attention-in-Gordon-Grosz/124bda80bbd3813aed802610de238456f3672c68", "https://www.semanticscholar.org/paper/Robust-Pronoun-Resolution-with-Limited-Knowledge-Mitkov/d032db4e8c1cf2b02867f95fa8fe847f5bfdf6ab", "https://www.semanticscholar.org/paper/A-Statistical-Approach-to-Anaphora-Resolution-Ge-Hale/71c0698edd0cf489cd837c91ad22bbf51643bf6c"]},
{"id": "c4914df5c8b886e85502bba35a35741d05639bd8", "title": "Continuous speech recognition by connectionist statistical methods", "authors": ["Herv\u00e9 Bourlard", "Nelson Morgan"], "date": "1993", "abstract": "Over the period of 1987-1991, a series of theoretical and experimental results have suggested that multilayer perceptrons (MLP) are an effective family of algorithms for the smooth estimation of high-dimension probability density functions that are useful in continuous speech recognition. The early form of this work has focused on hidden Markov models (HMM) that are independent of phonetic context. More recently, the theory has been extended to context-dependent models. The authors review the basic principles of their hybrid HMM/MLP approach and describe a series of improvements that are analogous to the system modifications instituted for the leading conventional HMM systems over the last few years. Some of these methods directly trade off computational complexity for reduced requirements of memory and memory bandwidth. Results are presented on the widely used Resource Management speech database that has been distributed by the US National Institute of Standards and Technology.", "references": ["https://www.semanticscholar.org/paper/CDNN%3A-a-context-dependent-neural-network-for-speech-Bourlard-Morgan/c1116b32168ca91607d81e8aa6be64ee7b539449", "https://www.semanticscholar.org/paper/Context-Dependent-Multiple-Distribution-Phonetic-Cohen-Franco/77af08ca01844fc4e1aaf35d353764dc012cb98c", "https://www.semanticscholar.org/paper/Continuous-speech-recognition-using-multilayer-with-Morgan-Bourlard/cd0568b4faa03910ae3c07d00c627666f404305d", "https://www.semanticscholar.org/paper/An-introduction-to-the-application-of-the-theory-of-Levinson-Rabiner/090f3ea5bc188bbb03aec02aba9ed9c7b38ff870", "https://www.semanticscholar.org/paper/Context-dependent-modeling-for-acoustic-phonetic-of-Schwartz-Chow/df53e0dc66eb13bb51c6e4803ceae56d3ebe6f23", "https://www.semanticscholar.org/paper/Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa/cd62c9976534a6a2096a38244f6cbb03635a127e", "https://www.semanticscholar.org/paper/Continuous-speech-recognition-using-PLP-analysis-Morgan-Hermansky/3e49679db76aae02c07b802e304e3ab142a4390e", "https://www.semanticscholar.org/paper/Connectionist-Viterbi-training%3A-a-new-hybrid-method-Franzini-Lee/f866ac085771f5676800db2d9b102975b2a1b2d7", "https://www.semanticscholar.org/paper/An-introduction-to-computing-with-neural-nets-Lippmann/b8778bb692cf105254fe767ef11a3a8afac4a068", "https://www.semanticscholar.org/paper/On-a-model-robust-training-method-for-speech-N%C3%A1das-Nahamoo/039900eaeeddd13752aa8d6c61759f0b0e54f0de"]},
{"id": "d032db4e8c1cf2b02867f95fa8fe847f5bfdf6ab", "title": "Robust Pronoun Resolution with Limited Knowledge", "authors": ["Ruslan Mitkov"], "date": "1998", "abstract": "Most traditional approaches to anaphora resolution rely heavily on linguistic and domain knowledge. One of the disadvantages of developing a knowledge-based system, however, is that it is a very labour-intensive and time-consuming task. This paper presents a robust, knowledge-poor approach to resolving pronouns in technical manuals, which operates on texts pre-processed by a part-of-speech tagger. Input is checked against agreement and for a number of antecedent indicators. Candidates are assigned scores by each indicator and the candidate with the highest score is returned as the antecedent. Evaluation reports a success rate of 89.7% which is better than the success rates of the approaches selected for comparison and tested on the same data. In addition, preliminary experiments show that the approach can be successfully adapted for other languages with minimum modifications.", "references": ["https://www.semanticscholar.org/paper/Robust-reference-resolution-with-limited-knowledge%3A-Mitkov-Stys/fa9125d18d3c0d565c207b61ec95b830fc3ede18", "https://www.semanticscholar.org/paper/CogNIAC%3A-high-precision-coreference-with-limited-Baldwin/8299a3ba1677224927f4f152664b5b59520c5838", "https://www.semanticscholar.org/paper/Resolving-pronoun-references-Hobbs/61c0eaf156647ffad23921a65e3d7b3296f7afd5", "https://www.semanticscholar.org/paper/An-Algorithm-for-Pronominal-Anaphora-Resolution-Lappin-Leass/f977f5e25adef5f0569cb9eb9b930e5146be2571", "https://www.semanticscholar.org/paper/Anaphora-for-Everyone%3A-Pronominal-Anaphora-without-Kennedy-Boguraev/5bbf53a9a8ee5a52e8b3e4c4e04cfebc8cc1b1c9", "https://www.semanticscholar.org/paper/Automatic-Processing-of-Large-Corpora-for-the-of-Dagan-Itai/9b2244369ff1ae711e614d47c84226d1399597a3", "https://www.semanticscholar.org/paper/Anaphora-resolution%3A-a-multi-strategy-approach-Carbonell-Brown/08c4176dcc331cf479c73f77e40dccb7e15496fb", "https://www.semanticscholar.org/paper/A-Centering-Approach-to-Pronouns-Brennan-Friedman/834d400f167fcf6ba27642eafa42e63f9e699a94", "https://www.semanticscholar.org/paper/An-Integrated-Model-For-Anaphora-Resolution-Mitkov/bcc14d3eb368be2eea3f93c406cf09202e4c727e", "https://www.semanticscholar.org/paper/Anaphora-Resolution-in-Slot-Grammar-Lappin-McCord/466b0e6cbbd284eaabe11e6386949c332b4f488e"]},
{"id": "f01ee064aa9fedf17acaaeb888eac74c9a0499e7", "title": "Connectionist model combination for large vocabulary speech recognition", "authors": ["Mike Hochberg", "Gary D. Cook", "Steve Renals", "Anthony J. Robinson"], "date": "1994", "abstract": "Reports in the statistics and neural networks literature have expounded the benefits of merging multiple models to improve classification and prediction performance. The Cambridge University connectionist speech group has developed a hybrid connectionist-hidden Markov model system for large vocabulary talker independent speech recognition. The performance of this system has been greatly enhanced through the merging of connectionist acoustic models. This paper presents and compares a number of different approaches to connectionist model merging and evaluates them on the TIMIT phone recognition and ARPA Wall Street Journal word recognition tasks.<<ETX>>", "references": ["https://www.semanticscholar.org/paper/Connectionist-probability-estimation-in-the-speech-Renals-Morgan/0d96910d3ac99d939563b484d6180efbcbb5b4a0", "https://www.semanticscholar.org/paper/A-new-paradigm-for-speaker-independent-training-Kubala-Schwartz/0c5aac47adcc1011fa876dc1cdd46f293eeac479", "https://www.semanticscholar.org/paper/The-design-for-the-wall-street-journal-based-CSR-Paul-Baker/8648dbfff9662fa9c62a95622712dd2951b5b3a3", "https://www.semanticscholar.org/paper/Perceptual-linear-predictive-(PLP)-analysis-of-Hermansky/b578f4faeb00b808e8786d897447f2493b12b4e9", "https://www.semanticscholar.org/paper/Hierarchical-mixtures-of-experts-and-the-EM-Jordan-Jacobs/f6d8a7fc2e2d53923832f9404376512068ca2a57", "https://www.semanticscholar.org/paper/Artificial-Intelligence-Frontiers-in-Statistics-Buntine/a7f34747269b042e70cee7ebbb0d7c8cdf0f7b15", "https://www.semanticscholar.org/paper/Maximum-Likelihood-from-Incomplete-Data-via-the-EM-Altri/bdfb57141b2141095ed942b28be24808aeba8d54"]},
{"id": "c6629770cb6a00ad585918e71fe6dbad829ad0d1", "title": "An application of recurrent nets to phone probability estimation", "authors": ["Anthony J. Robinson"], "date": "1994", "abstract": "This paper presents an application of recurrent networks for phone probability estimation in large vocabulary speech recognition. The need for efficient exploitation of context information is discussed; a role for which the recurrent net appears suitable. An overview of early developments of recurrent nets for phone recognition is given along with the more recent improvements that include their integration with Markov models. Recognition results are presented for the DARPA TIMIT and Resource Management tasks, and it is concluded that recurrent nets are competitive with traditional means for performing phone probability estimation.", "references": ["https://www.semanticscholar.org/paper/Connectionist-probability-estimators-in-HMM-speech-Renals-Morgan/a08c99425ad94eed67d059813511fe9ca55e73eb", "https://www.semanticscholar.org/paper/A-recurrent-error-propagation-network-speech-system-Robinson-Fallside/758eae04fc9f4331b0ceab797387c8fc9f00db58", "https://www.semanticscholar.org/paper/Continuous-speech-recognition-using-multilayer-with-Morgan-Bourlard/cd0568b4faa03910ae3c07d00c627666f404305d", "https://www.semanticscholar.org/paper/CDNN%3A-a-context-dependent-neural-network-for-speech-Bourlard-Morgan/c1116b32168ca91607d81e8aa6be64ee7b539449", "https://www.semanticscholar.org/paper/A-Maximum-Likelihood-Approach-to-Continuous-Speech-Bahl-Jelinek/c4775e2f0d27e8be4aae7b5b5c2560b96ce2eb58", "https://www.semanticscholar.org/paper/Alpha-nets%3A-A-recurrent-'neural'-network-with-a-Bridle/c91c2ac02e33caff601b2e4d62a6841b33ca3929", "https://www.semanticscholar.org/paper/Fast-algorithms-for-phone-classification-and-using-Digalakis-Ostendorf/ea705422d5e2292bccb766e9047c9f25880a50c0", "https://www.semanticscholar.org/paper/Hidden-Markov-Models-for-Speech-Recognition-Juang-Rabiner/df682aa90fbbbf665a8b273a57ca87d6cea9ff99", "https://www.semanticscholar.org/paper/Links-Between-Markov-Models-and-Multilayer-Bourlard-Wellekens/ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "https://www.semanticscholar.org/paper/MMI-training-for-continuous-phoneme-recognition-on-Kapadia-Valtchev/fd6387ca1949d61356adee35708dcdbee1e4fd05"]},
{"id": "b4c6cd6879df810fee431ae1cf16ac35d3b2c239", "title": "The BICORD System", "authors": ["Judith L. Klavans", "Evelyne Tzoukermann"], "date": "1990", "abstract": "Semantic Scholar extracted view of \"The BICORD System\" by Judith L. Klavans et al.", "references": []},
{"id": "989001bcd31cf3f2d0088cee665dc25404e3baf6", "title": "Compensation for the effect of the communication channel in auditory-like analysis of speech (RASTA-PLP)", "authors": ["Hynek Hermansky", "Nelson Morgan", "Aruna Bayya", "Phil Kohn"], "date": "1991", "abstract": "Semantic Scholar extracted view of \"Compensation for the effect of the communication channel in auditory-like analysis of speech (RASTA-PLP)\" by Hynek Hermansky et al.", "references": []},
{"id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5", "title": "A Tutorial on Hidden Markov Models and Selected Applications", "authors": ["Lawrence R. Rabiner"], "date": "1989", "abstract": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting.", "references": ["https://www.semanticscholar.org/paper/Speaker-dependent-connected-speech-recognition-via-Bourlard-Kamp/2debd62d68f8c452af957cc0c3a4b38f1e75e85a", "https://www.semanticscholar.org/paper/Maximum-likelihood-estimation-for-mixture-of-Markov-Juang/1c18985b2e70d671f07d784ac9120fdd090569a9", "https://www.semanticscholar.org/paper/A-probabilistic-distance-measure-for-hidden-Markov-Juang-Rabiner/8b7b9d4439849256c1841f39ff6a38ad93d66a94", "https://www.semanticscholar.org/paper/A-minimum-discrimination-information-approach-for-Ephraim-Dembo/f352b1770357f931807c6232ac879f2845980413", "https://www.semanticscholar.org/paper/Maximum-likelihood-estimation-for-multivariate-of-Liporace/664eb4fb59f2ce8f2e019a77653f9ed2cc5df591", "https://www.semanticscholar.org/paper/Some-properties-of-continuous-hidden-Markov-model-Rabiner-Juang/0ad9f79c744bec1f1325b621ff590172db9a511d", "https://www.semanticscholar.org/paper/The-Harpy-Speech-Understanding-System-In-Trends-in-Lowerre-Reddy/242b209f24703f8c97494cb1499312f30d15655f", "https://www.semanticscholar.org/paper/An-introduction-to-the-application-of-the-theory-of-Levinson-Rabiner/090f3ea5bc188bbb03aec02aba9ed9c7b38ff870", "https://www.semanticscholar.org/paper/Recognition-of-isolated-digits-using-hidden-Markov-Rabiner-Juang/e90c15e0de8b5452c6291359e98ddc099e3b93f6", "https://www.semanticscholar.org/paper/An-inequality-with-applications-to-statistical-for-Baum-Eagon/69fc8c03d21e22e30d6642824c37158b314f36c3"]},
{"id": "b63bbb5b5a509b6f8f12df9350cba00aa3fdfc82", "title": "Quality improvement of LPC-processed noisy speech by using spectral subtraction", "authors": ["George S. Kang", "Lawrence J. Fransen"], "date": "1989", "abstract": "Numerous noise-suppression techniques have been developed for operating at the front end of low-bit-rate digital voice terminals. Some of these techniques have been evaluated by standardized intelligibility tests such as the diagnostic rhyme test (DRT). It is well known that the use of a noise suppressor seldom improves the DRT score even though listeners have had the impression that speech quality was enhanced. Unfortunately, noise suppressors have only occasionally been evaluated by standardized quality tests. The authors supplement quality test data for reference purposes. They use the diagnostic acceptability measure (DAM) to evaluate speech quality of the latest 2400-b/s linear-predictive coder (LPC) with a noise suppressor at the front end. They used a spectral subtraction technique for noise suppression. Ten different sets of noisy speech recorded at actual military platforms (such as a helicopter, tank, turboprop, helicopter carrier, or jeep) were input sources. The magnitude of the DAM improvement is substantial: as much as six points on the average, which is large enough to upgrade speech quality somewhat. >", "references": ["https://www.semanticscholar.org/paper/Enhancement-and-bandwidth-compression-of-noisy-Lim-Oppenheim/066779ead800b590b0957aa8c70bc77cc7266fab", "https://www.semanticscholar.org/paper/Enhancement-and-bandwidth-compression-of-noisy-by-Lim/b09ea6a3d29cbf2ce8996c254bdcfc20edf68d82"]},
{"id": "25e14210bb13f8c2db2626e3c0a5ff2e6fdcb05c", "title": "Integrating RASTA-PLP into speech recognition", "authors": ["Joachim Koehler", "Nelson Morgan", "Hynek Hermansky", "Hans-G\u00fcnter Hirsch", "Grace Tong"], "date": "1994", "abstract": "In previous work, we and others have shown that bandpass filtering of temporal trajectories of simple functions of the critical band spectrum can lead to more robust speech recognizers in the presence of additive and convolutional error. In this study we report results on several mechanisms for incorporating this analysis technique into training, in a way that is consistent with on-line approaches to speech recognition. In particular, we show improved robustness to these forms of degradation for a system that maps the filtered spectral points using a linear regression computed from results of the different transformations.<<ETX>>", "references": ["https://www.semanticscholar.org/paper/Estimation-of-noise-spectrum-and-its-application-to-Hirsch/41d8018a5204936230a9a1ef6bc8ea1b736523b3"]},
{"id": "d5bd9ad9a3b49ce936679a3c2bf5ec7277bcb5f5", "title": "A Freely Available Wide Coverage Morphological Analyzer for English", "authors": ["Daniel Karp", "Yves Schabes", "Martin Zaidel", "Dania Egedi"], "date": "1992", "abstract": "This paper presents a morphological lexicon for English that handles more than 317000 inflected forms derived from over 90000 stems. The lexicon is available in two formats. The first can be used by an implementation of a two-level processor for morphological analysis. The second, derived from the first one for efficiency reasons, consists of a disk-based database using a UNIX hash table facility. We also built an X Window tool to facilitate the maintenance and browsing of the lexicon. The package is ready to be integrated into an natural language application such as a parser through hooks written in Lisp and C.", "references": ["https://www.semanticscholar.org/paper/A-General-Computational-Model-For-Word-Form-And-Koskenniemi/e11ce10eff56df1729626bda3ef6109566f779c9", "https://www.semanticscholar.org/paper/XTAG-A-Graphical-Workbench-for-Developing-Grammars-Paroubek-Schabes/22c0440043abc1636a31cc7ed2bd2a6eb04d4d5a", "https://www.semanticscholar.org/paper/Computational-Complexity-And-Natural-Language-Barton-Berwick/1efe8329c9dec8fb276605c388a538220608b608", "https://www.semanticscholar.org/paper/Complexity%2C-two-level-morphology-and-Finnish-Koskenniemi-Church/c63b2456416f1bf952834dc8cd04288bafa2cbd6", "https://www.semanticscholar.org/paper/Book-Reviews%3A-Lecture-on-Contemporary-Syntactic-An-Curteanu/2426795ca0024d7fe65b8a9a23690e0b5f2755c8", "https://www.semanticscholar.org/paper/A-New-Hashing-Package-for-UNIX-Seltzer-Yigit/dac363e9773ed9bc1e77dce431c5d0911eb57033"]},
{"id": "8201b613108b843b026d31a8018943f053bad7d5", "title": "Speaker-independent isolated word recognition based on emphasized spectral dynamics", "authors": ["Sadaoki Furui"], "date": "1986", "abstract": "A new speech analysis technique applicable to speech recognition is proposed considering the auditory mechanism of speech perception which emphasizes spectral dynamics and which compensates for the spectral undershoot associated with coarticulation. A speech wave is represented by the LPC cepstrum and logarithmic energy sequences, and the time sequences over short periods are expanded by the first- and second-order polynomial functions at every frame period. The dynamics of the cepstrum sequences are then emphasized by the linear combination of their polynomial expansion coefficients, that is, derivatives, and their instantaneous values. Speaker-independent word recognition experiments using time functions of the dynamics-emphasized cepstrum and the polynomial coefficient for energy indicate that the error rate can be largely reduced by this method.", "references": []},
{"id": "3e49679db76aae02c07b802e304e3ab142a4390e", "title": "Continuous speech recognition using PLP analysis with multilayer perceptrons", "authors": ["Nelson Morgan", "Hynek Hermansky", "Herv\u00e9 Bourlard", "Philip D. Kohn", "Chuck Wooters"], "date": "1991", "abstract": "The authors investigate the use of continuous features derived by perceptual linear predictive (PLP) analysis, examine the effect of adding temporal features, and compare it to the previously studied use of multiframe input. Comparisons of the MLP (multilayer perceptron) and conventional Gaussian classifiers are also reported. The speaker-dependent portion of the Resource Management database was used for this test. Additionally, some experiments were performed with a perplexity-2200 speaker-independent recognition task on a subset of the TIMIT database. In each case, the PLP features were used as input to the networks. The experiments show the advantage of continuous PLP features and their first and second temporal derivatives.<<ETX>>", "references": []},
{"id": "f9eb0ed8d028171f1e2cf69db0010584ce07b44a", "title": "Peut-on v\u00e9rifier automatiquement la coh\u00e9rence terminologique?", "authors": ["Elliott Macklovitch"], "date": "1996", "abstract": "II arrive souvent, dans les services de traduction, que l'on soit oblige de morceler les textes volumineux. Dans une telle situation, il incombe generalement au reviseur de fusionner les parties traduites par les differents traducteurs et de faire en sorte que le tout soit coherent. Un element important de cette t\u00e2che est d'assurer la coherence terminologique. Intuitivement, ce que l'on veut dire par coherence terminologique ici est assez clair : chaque unite terminologique doit etre traduite de la meme facon partout dans le texte. Au CITI, nous developpons un outil d'aide a l'intention du reviseur (qui peut etre le traducteur lui-meme) dont le but est de valider certaines proprietes d'un texte traduit. Appele TransCheck, le premier prototype de ce systeme est decrit en detail dans Macklovitch (1994). Dans cet article, nous decrivons les premiers essais pour incorporer la verification de la coherence terminologique dans le systeme TransCheck. L'idee de base est assez simple : le reviseur fournit un lexique au systeme; ensuite, TransCheck balaie deux textes alignes et signale chaque occurrence d'un terme source qui n'est pas traduit par le terme cible designe dans le lexique. Les resultats de ces experiences nous ont montre qu'une definition naive de la coherence terminologique, telle que celle donnee ci-dessus, est trop rigide et simpliste; mais la ventilation du bruit genere nous indique clairement comment il faudra assouplir l'application de cette definition par le systeme pour que la validation automatique de la coherence terminologique devienne praticable.", "references": []},
{"id": "e5e44b3ee7a44b56212b1f19a8df042e4e1617cd", "title": "Computational Complexity of One-Tape Turing Machine Computations", "authors": ["Juris Hartmanis"], "date": "1968", "abstract": "The quantitative aspects of one-tape Turing machine computations are considered. It is shown, for instance, that there exists a sharp time bound which must be reached for the recognition of nonregular sets of sequences. It is shown that the computation time can be used to characterize the complexity of recursive sets of sequences, and several results are obtained about this classification. These results are then applied to the recognition speed of context-free languages and it is shown, among other things, that it is recursively undecidable how much time is required to recognize a nonregular context-free language on a one-tape Turing machine. Several unsolved problems are discussed.", "references": ["https://www.semanticscholar.org/paper/One-Tape%2C-Off-Line-Turing-Machine-Computations-Hennie/6407680b5b551bccf50e62eaf94ab036a4d7eaa9"]},
{"id": "039900eaeeddd13752aa8d6c61759f0b0e54f0de", "title": "On a model-robust training method for speech recognition", "authors": ["Arthur N\u00e1das", "David Nahamoo", "Michael Picheny"], "date": "1988", "abstract": "Training methods for designing better decoders are compared. The training problem is considered as a statistical parameter estimation problem. In particular, the conditional maximum likelihood estimate (CMLE), which estimates the parameter values that maximize the conditional probability of words given acoustics during training, is compared to the maximum-likelihood estimate, which is obtained by maximizing the joint probability of the words and acoustics. For minimizing the decoding error rate of the (optimal) maximum a posteriori probability (MAP) decoder, it is shown that the CMLE (or maximum mutual information estimate, MMIE) may be preferable when the model is incorrect. In this sense, the CMLE/MMIE appears more robust than the MLE. >", "references": ["https://www.semanticscholar.org/paper/A-decision-theorectic-formulation-of-a-training-in-Nadas/79eb272eaf061cf4e65b8e61c9f02c027b3b6933"]},
{"id": "caf8a93ff5011dbafce9af352cc70a0a6772c271", "title": "Channel normalization techniques for automatic speech recognition over the telephone", "authors": ["Johan de Veth", "Lou Boves"], "date": "1998", "abstract": "Abstract In this paper we aim to identify the underlying causes that can explain the performance of different channel normalization techniques. To this aim we compared four different channel normalization techniques within the context of connected digit recognition over telephone lines: cepstrum mean subtraction, the dynamic cepstrum representation, RASTA filtering and phase-corrected RASTA. We used context-dependent and context-independent hidden Markov models that were trained using a wide range of different model complexities. The results of our recognition experiments indicate that each channel normalization technique should preserve the modulation frequencies in the range between 2 and 16 Hz in the spectrum of the speech signals. At the same time, DC components in the modulation spectrum should be effectively removed. With context-independent models the channel normalization filter should have a flat phase response. Finally, for our connected digit recognition task it appeared that cepstrum mean subtraction and phase-corrected RASTA performed equally well for context-dependent and context-independent models when equal amounts of model parameters were used.", "references": []},
{"id": "a46bc99d90bd064e9ccf4def70ac89b27537283b", "title": "Functional Centering", "authors": ["Michael Strube", "Udo Hahn"], "date": "1996", "abstract": "Based on empirical evidence from a free word order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering.", "references": ["https://www.semanticscholar.org/paper/Centering-in-Japanese-Discourse-Walker-Iida/544d79868645c0ff4823ad4761a72571bb07144c", "https://www.semanticscholar.org/paper/A-Centering-Approach-to-Pronouns-Brennan-Friedman/834d400f167fcf6ba27642eafa42e63f9e699a94", "https://www.semanticscholar.org/paper/Pronouns%2C-Names%2C-and-the-Centering-of-Attention-in-Gordon-Grosz/124bda80bbd3813aed802610de238456f3672c68", "https://www.semanticscholar.org/paper/Bridging-Textual-Ellipses-Hahn-Strube/046c5b0c8d91e2926fd8bc98d80a843e0b88cfec", "https://www.semanticscholar.org/paper/Incremental-Centering-and-Center-Ambiguity-CenteringActor-CenterActor/b07e3024355b6a1ac8ee101924a8a4e3aced6fec", "https://www.semanticscholar.org/paper/Processing-Complex-Sentences-in-the-Centering-Strube/1d1b4697c7b1c48a56eb2804e7f3626eccf31b92", "https://www.semanticscholar.org/paper/ParseTalk-about-Sentence-and-Text-Level-Anaphora-Strube-Hahn/ae8e97e144a54143b405e161f9aa889889e41bf1", "https://www.semanticscholar.org/paper/Japanese-Discourse-and-the-Process-of-Centering-Walker-Iida/6506d8ffb68c904cd52b4eef74b4db08ea8d338b", "https://www.semanticscholar.org/paper/Providing-a-Unified-Account-of-Definite-Noun-in-Grosz-Joshi/14dd4ab5cda0225b457f425b271888901005b819", "https://www.semanticscholar.org/paper/Centering%3A-A-Framework-for-Modeling-the-Local-of-Grosz-Joshi/2c538ee7c17c71b2aaa53dcf3c9972c00e6e097b"]},
{"id": "124bda80bbd3813aed802610de238456f3672c68", "title": "Pronouns, Names, and the Centering of Attention in Discourse", "authors": ["Peter C. Gordon", "Barbara J. Grosz", "Laura A. Gilliom"], "date": "1993", "abstract": "Centering theory, developed within computational linguistics, provides an account of ways in which patterns of interutterance reference can promote the local coherence of discourse. It states that each utterance in a coherent discourse segment contains a single semantic entity\u2014the backward-looking center\u2014that provides a link to the previous utterance, and an ordered set of entities\u2014the forward-looking centers\u2014that offer potential links to the next utterance. We report five reading-time experiments that test predictions of this theory with respect to the conditions under which it is preferable to realize (refer to) an entity using a pronoun rather than a repeated definite description or name. The experiments show that there is a single backward-looking center that is preferentially realized as a pronoun, and that the backward-looking center is typically realized as the grammatical subject of the utterance. They also provide evidence that there is a set of forward-looking centers that is ranked in terms of prominence, and that a key factor in determining prominence\u2014surface-initial position\u2014does not affect determination of the backward-looking center. This provides evidence for the dissociation of the coherence processes of looking backward and looking forward.", "references": []},
{"id": "9b2244369ff1ae711e614d47c84226d1399597a3", "title": "Automatic Processing of Large Corpora for the Resolution of Anaphora References", "authors": ["Ido Dagan", "Alon Itai"], "date": "1990", "abstract": "Manual acquisition of semantic constraints in broad domains is very expensive. This paper presents an automatic scheme for collecting statistics on cooccurrence patterns in a large corpus. To a large extent, these statistics reflect semantic constraints and thus are used to disambiguate anaphora references and syntactic ambiguities. The scheme was implemented by gathering statistics on the output of other linguistic tools. An experiment was performed to resolve references of the pronoun \"it\" in sentences that were randomly selected from the corpus. The results of the experiment show that in most of the cases the cooccurrence statistics indeed reflect the semantic constraints and thus provide a basis for a useful disambiguation tool.", "references": ["https://www.semanticscholar.org/paper/Resolving-pronoun-references-Hobbs/61c0eaf156647ffad23921a65e3d7b3296f7afd5", "https://www.semanticscholar.org/paper/Discovery-Procedures-for-Sublanguage-Selectional-Grishman-Hirschman/819c80bc13dc40da5d0dd25f496c0cfa7dfcf832", "https://www.semanticscholar.org/paper/Anaphora-resolution%3A-a-multi-strategy-approach-Carbonell-Brown/08c4176dcc331cf479c73f77e40dccb7e15496fb", "https://www.semanticscholar.org/paper/A-statistical-approach-to-language-translation-Brown-Cocke/2166fa493a8c6e40f7f8562d15712dd3c75f03df", "https://www.semanticscholar.org/paper/A-stochastic-parts-program-and-noun-phrase-parser-Church/a7e084fe51a40eeaaf79bf0b78e837d5bc4a8e10", "https://www.semanticscholar.org/paper/Computing-grammatical-fimctions-from-a-parse-tree-Lappin-Golan/d24e47fe85f4bd38e30106cdc7d125a91305be7d", "https://www.semanticscholar.org/paper/PEGASOS%3A-Deriving-predicate-argument-structures-a-Jensen/7896ea87c84686be758b72a5fe64612bd540b8df"]},
{"id": "bcc14d3eb368be2eea3f93c406cf09202e4c727e", "title": "An Integrated Model For Anaphora Resolution", "authors": ["Ruslan Mitkov"], "date": "1994", "abstract": "The paper discusses a new knowledge-based and sublanguage-oriented model for anaphora resolution, which integrates syntactic, semantic, discourse, domain and heuristical knowledge for the sublanguage of computer science. Special attention is paid to a new approach for tracking the center throughout a discourse segment, which plays an important role in proposing the most likely antecedent to the anaphor in case of ambiguity.", "references": ["https://www.semanticscholar.org/paper/Anaphora-resolution%3A-a-multi-strategy-approach-Carbonell-Brown/08c4176dcc331cf479c73f77e40dccb7e15496fb", "https://www.semanticscholar.org/paper/A-Language-Independent-Anaphora-Resolution-System-Aone-McKee/957d8108d740edae9499d4784b3be2427d28871b", "https://www.semanticscholar.org/paper/Anaphora-Resolution-in-Machine-Translation-Susanne-Schmitz/2e3f85b724a01603d250143309a6d857902c0f3e", "https://www.semanticscholar.org/paper/Evaluating-Discourse-Processing-Algorithms-Walker/2f817da1448fbcc34e75b7616693d171348d4b3c", "https://www.semanticscholar.org/paper/Intention-and-the-structure-of-discourse-Grosz-Sidner/7b7c2d16dfaa6bfa1c44600c99578c68f3af95af"]},
{"id": "fd6387ca1949d61356adee35708dcdbee1e4fd05", "title": "MMI training for continuous phoneme recognition on the TIMIT database", "authors": ["S. Kapadia", "V. Valtchev", "Steve J. Young"], "date": "1993", "abstract": "Experiences with a phoneme recognition system for the TIMIT database which uses multiple mixture continuous-density monophone HMMs (hidden Markov models) trained using MMI (maximum mutual information) is reported. A comprehensive set of results are presented comparing the ML (maximum likelihood) and MMI training criteria for both diagonal and full covariance models. These results using simple monophone HMMs show that clear performance gains are achieved by MMI training. These results are comparable with the best reported by others, including those which use context-dependent models. In addition, a number of performance and implementation issues which are crucial to successful MMI training are discussed.<<ETX>>", "references": ["https://www.semanticscholar.org/paper/The-general-use-of-tying-in-phoneme-based-HMM-Young/805d4012921110c2ba95d687a2aab57f82cd92ea", "https://www.semanticscholar.org/paper/Hidden-Markov-models%2C-maximum-mutual-information-Normandin/a27fc84fb14188f7149bd5cae9b24b7eaed3e588", "https://www.semanticscholar.org/paper/Optimal-solution-of-a-training-problem-in-speech-Nadas/3f1cde8cafbe2e682d1b2b41e57e4358c3f4ce5c", "https://www.semanticscholar.org/paper/An-empirical-study-of-learning-speed-in-networks-Fahlman/722a3c365a134a9f9b9ae1511f018d9b1ecff3de", "https://www.semanticscholar.org/paper/A-Tutorial-on-Hidden-Markov-Models-and-Selected-Rabiner/8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5"]},
{"id": "c91c2ac02e33caff601b2e4d62a6841b33ca3929", "title": "Alpha-nets: A recurrent 'neural' network architecture with a hidden Markov model interpretation", "authors": ["John S. Bridle"], "date": "1990", "abstract": "Abstract A hidden Markov model isolated word recogniser using full likelihood scoring for each word model can be treated as a recurrent \u2018neural\u2019 network. The units in the recurrent loop are linear, but the observations enter the loop via a multiplication. Training can use back-propagation of partial derivatives to hill-climb on a measure of discriminability between words. The back-propagation has exactly the same form as the backward pass of the Baum-Welch (EM) algorithm for maximum-likelihood HMM training. The use of a particular error criterion based on relative entropy (equivalent to the so-called Mutual Information criterion which has been used for discriminative training of HMMs) can have derivatives which are interestingly related to the Baum-Welch re-estimates and to Corrective Training.", "references": []},
{"id": "242b209f24703f8c97494cb1499312f30d15655f", "title": "The Harpy Speech Understanding System In Trends in Speech Recognition Speech Science Publications", "authors": ["Bruce T. Lowerre", "Raj Reddy"], "date": "1986", "abstract": "A crop compresser wherein crop is compressed between the intermeshing teeth and grooves of gear-like members with either the bottoms of the grooves or faces of the teeth or both inclined and thus diverging where they mesh relative to each other so that the crop is not only compressed but also the compressed crop is moved outwardly in the direction of divergence provided between each tooth face and cooperating groove bottom when the component of the compressive force acting in the diverging direction overcomes friction on the compressed crop from the tooth face, groove bottom and sides which tends to retain crop therein. Sharp edges may be provided on the teeth to cut the inserted crop. The degree of divergence may be varied or may be in opposite directions from a central high point relative to the member. A plurality of gear-like members may intermesh with a single member and the members may be incorporated in a mobile farm implement which includes a tined pick-up device and a worm or rotating member or both to supply crop to the intermeshing gear-like members. Also, these members may be angularly or axially adjustable, within limits, selectively to increase or decrease the divergence or degree of compression. In a device with a plurality of crop compressors, the crop supplied thereto may be cut by rotary or stationary cutting blades between crop compressing groups.", "references": []},
{"id": "b09ea6a3d29cbf2ce8996c254bdcfc20edf68d82", "title": "Enhancement and bandwidth compression of noisy speech by estimation of speech and its model parameters.", "authors": ["Jae S. Lim"], "date": "1978", "abstract": "Thesis. 1978. Sc.D.--Massachusetts Institute of Technology. Dept. of Electrical Engineering and Computer Science.", "references": []},
{"id": "066779ead800b590b0957aa8c70bc77cc7266fab", "title": "Enhancement and bandwidth compression of noisy speech", "authors": ["J.S. Lim", "A.V. Oppenheim"], "date": "1979", "abstract": "Over the past several years there has been considerable attention focused on the problem of enhancement and bandwidth compression of speech degraded by additive background noise. This interest is motivated by several factors including a broad set of important applications, the apparent lack of robustness in current speech-compression systems and the development of several potentially promising and practical solutions. One objective of this paper is to provide an overview of the variety of techniques that have been proposed for enhancement and bandwidth compression of speech degraded by additive background noise. A second objective is to suggest a unifying framework in terms of which the relationships between these systems is more visible and which hopefully provides a structure which will suggest fruitful directions for further research.", "references": ["https://www.semanticscholar.org/paper/LPC-analysis%2FSynthesis-from-speech-inputs-noise-or-Sambur-Jayant/d20ccb3cecc525d401f7c1399a533eef3d7e497c", "https://www.semanticscholar.org/paper/All-pole-modeling-of-degraded-speech-Lim-Oppenheim/50a641d27ec5788bfcc58e1855f0ff28f4cdde3a", "https://www.semanticscholar.org/paper/Subjective-evaluation-of-SPAC-in-improving-the-of-Nakatsui/2cc35b10de881824c201a1d03b94a65e362f0ddf", "https://www.semanticscholar.org/paper/Adaptive-noise-canceling-for-speech-signals-Sambur/b543bebfc1e6f6985193eafc9d7e74dbee08253e", "https://www.semanticscholar.org/paper/Wide%E2%80%90hand-noise-reduction-of-noisy-speech-Magill-Un/2d93cd2b156df876a84fcf7c7b215f559dfd288b", "https://www.semanticscholar.org/paper/Evaluation-of-an-adaptive-comb-filtering-method-for-Lim-Oppenheim/8a8c112aa3a9b050090d4dca2e4d90c7a0627747", "https://www.semanticscholar.org/paper/Robust-Speech-Processing.-Gold/3aabeda70ff5850697c4e26e7e3358633f1fd0e7", "https://www.semanticscholar.org/paper/Performance-of-LPC-vocoders-in-a-noisy-environment-Teacher-Coulter/fa112c70ebf16efe9263a6ac5d9adc8740047112", "https://www.semanticscholar.org/paper/Parallel-processing-techniques-for-estimating-pitch-Gold-Rabiner/0f3c918a944ffdff7ae0e0661e93539d43751fc4", "https://www.semanticscholar.org/paper/Maximum-likelihood-parameter-estimation-of-noisy-Musicus-Lim/5cddd6f0e14b899771b4946fed5883bf0338b9da"]},
{"id": "ee50abb5aff3e5c43a38f24396b9552d593a9ae0", "title": "Links Between Markov Models and Multilayer Perceptrons", "authors": ["Herv\u00e9 Bourlard", "Christian Wellekens"], "date": "1988", "abstract": "The statistical use of a particular classic form of a connectionist system, the multilayer perceptron (MLP), is described in the context of the recognition of continuous speech. A discriminant hidden Markov model (HMM) is defined, and it is shown how a particular MLP with contextual and extra feedback input units can be considered as a general form of such a Markov model. A link between these discriminant HMMs, trained along the Viterbi algorithm, and any other approach based on least mean square minimization of an error function (LMSE) is established. It is shown theoretically and experimentally that the outputs of the MLP (when trained along the LMSE or the entropy criterion) approximate the probability distribution over output classes conditioned on the input, i.e. the maximum a posteriori probabilities. Results of a series of speech recognition experiments are reported. The possibility of embedding MLP into HMM is described. Relations with other recurrent networks are also explained. >", "references": ["https://www.semanticscholar.org/paper/Speech-pattern-discrimination-and-multilayer-Bourlard-Wellekens/a55d31784aca11871985096644a025f036633569", "https://www.semanticscholar.org/paper/Phoneme-recognition-using-time-delay-neural-Waibel-Hanazawa/cd62c9976534a6a2096a38244f6cbb03635a127e", "https://www.semanticscholar.org/paper/The-acoustic-modeling-problem-in-automatic-speech-Brown/1aa31d5deb45f477a6de45b3b75b62c7f4a213e7", "https://www.semanticscholar.org/paper/An-introduction-to-computing-with-neural-nets-Lippmann/b8778bb692cf105254fe767ef11a3a8afac4a068", "https://www.semanticscholar.org/paper/Speech-Recognition-by-Statistical-Methods-Jelinek/e66aade2a97005b9f0638357421b1f72f770f75f", "https://www.semanticscholar.org/paper/Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg/de996c32045df6f7b404dda2a753b6a9becf3c08", "https://www.semanticscholar.org/paper/Connectionist-Learning-Procedures-Hinton/a57c6d627ffc667ae3547073876c35d6420accff", "https://www.semanticscholar.org/paper/Maximum-mutual-information-estimation-of-hidden-for-Bahl-Brown/b5f09ce0dd760857e0d0e4879f6e2543f04c5d33", "https://www.semanticscholar.org/paper/The-use-of-a-one-stage-dynamic-programming-for-word-Ney/1e4193d03eb3c5a695a3d8b3506f80704f9dfc19", "https://www.semanticscholar.org/paper/Accelerated-Learning-in-Layered-Neural-Networks-Solla-Levin/247698d0a716f0d99c0645050d049525e0b08ec2"]},
{"id": "e11ce10eff56df1729626bda3ef6109566f779c9", "title": "A General Computational Model For Word-Form Recognition And Production", "authors": ["Kimmo Koskenniemi"], "date": "1984", "abstract": "A language independent model for recognition and production of word forms is presented. This \"two-level model\" is based on a new way of describing morphological alternations. All rules describing the morphophonological variations are parallel and relatively independent of each other. Individual rules are implemented as finite state automata, as in an earlier model due to Martin Kay and Ron Kaplan. The two-level model has been implemented as an operational computer programs in several places. A number of operational two-level descriptions have been written or are in progress (Finnish, English, Japanese, Rumanian, French, Swedish, Old Church Slavonic, Greek, Lappish, Arabic, Icelandic). The model is bidirectional and it is capable of both analyzing and synthesizing word-forms.", "references": ["https://www.semanticscholar.org/paper/Two-Level-Model-for-Morphological-Analysis-Koskenniemi/008da127af8fbdfdd9a3f7d36b23d7c3946e491b", "https://www.semanticscholar.org/paper/A-PROCESS-MODEL-OF-MORPHOLOGY-AND-LEXICON-and-Koskenniemi/119ba277a0aeffc8171916756adbe9844b9ff71a", "https://www.semanticscholar.org/paper/A-General-Computational-Model-for-Word-Form-and-Koskenniemi/ba817882de1a7093ee83bfa14a4005e5c8a5977e", "https://www.semanticscholar.org/paper/A-two-level-analysis-of-french-Shan/f4bbc00fdbcd8b96010d1d8db94f3bdf0e03ac48", "https://www.semanticscholar.org/paper/A-two-level-description-of-Old-Church-Slavonic-Lindstedt/0271828a2b08a5fc990e78d0f6803fa2a6a290b1", "https://www.semanticscholar.org/paper/A-two-level-morphological-analysis-of-japanese-Alam/2bad5215293ee18cd251c924ffc4ae3280935102", "https://www.semanticscholar.org/paper/A-two-level-morphological-analysis-of-rumanian-Khan/88b50f5be70b1c2fe40d7cc6bcf478a9d92307f3", "https://www.semanticscholar.org/paper/A-two-level-morphological-description-of-english-Karttunen-Wittenburg/5bedc10d045b5594159cd7f2a1847a94d2e6a493"]},
{"id": "22c0440043abc1636a31cc7ed2bd2a6eb04d4d5a", "title": "XTAG - A Graphical Workbench for Developing Tree-Adjoining Grammars", "authors": ["Patrick Paroubek", "Yves Schabes", "Aravind K. Joshi"], "date": "1992", "abstract": "We describe a workbench (XTAG) for the development of tree-adjoining grammars and their parsers, and discuss some issues that arise in the design of the graphical interface.Contrary to string rewriting grammars generating trees, the elementary objects manipulated by a tree-adjoining grammar are extended trees (i.e. trees of depth one or more) which capture syntactic information of lexical items. The unique characteristics of tree-adjoining grammars, its elementary objects found in the lexicon (extended trees) and the derivational history of derived trees (also a tree), require a specially crafted interface in which the perspective has shifted from a string-based to a tree-based system. XTAG provides such a graphical interface in which the elementary objects are trees (or tree sets) and not symbols (or strings).The kernel of XTAG is a predictive left to right parser for unification-based tree-adjoining grammar [Schabes, 1991]. XTAG includes a graphical editor for trees, a graphical tree printer, utilities for manipulating and displaying feature structures for unification-based tree-adjoining grammar, facilities for keeping track of the derivational history of TAG trees combined with adjoining and substitution, a parser for unification based tree-adjoining grammars, utilities for defining grammars and lexicons for tree-adjoining grammars, a morphological recognizer for English (75 000 stems deriving 280 000 inflected forms) and a tree-adjoining grammar for English that covers a large range of linguistic phenomena.Considerations of portability, efficiency, homogeneity and ease of maintenance, lead us to the use of Common Lisp without its object language addition and to the use of the X Window interface to Common Lisp (CLX) for the implementation of XTAG.XTAG without the large morphological and syntactic lexicons is public domain software. The large morphological and syntactic lexicons can be obtained through an agreement with ACL's Data Collection Initiative.XTAG runs under Common Lisp and X Window (CLX).", "references": ["https://www.semanticscholar.org/paper/Tree-adjoining-grammars-and-lexicalized-grammars-Joshi-Schabes/b43cc74eed9c7815bdc5d3f2edab5ddcea673368", "https://www.semanticscholar.org/paper/Parsing-strategies-with-'lexicalized'-grammars%3A-to-Schabes-Abeill%C3%A9/5cd28d8da08176bc22eb3a33fa4a68d282ef0cd8", "https://www.semanticscholar.org/paper/Feature-structures-based-Tree-Adjoining-Grammars-Vijay-Shanker-Joshi/54cf4641d00b8a177c3b98a6e07d48620dc91388", "https://www.semanticscholar.org/paper/A-Lexicalized-Tree-Adjoining-Grammar-for-French%3A-Abeill%C3%A9/47570031e3dba60df35fff854d3cf1e134245122", "https://www.semanticscholar.org/paper/Synchronous-Tree-Adjoining-Grammars-Shieber-Schabes/5fa64621c77d551362359415b9b0bff10a3f5edd", "https://www.semanticscholar.org/paper/Mathematical-and-computational-aspects-of-grammars-Schabes-Joshi/2210bd1ef7a961220687dd9081424b3558727a38", "https://www.semanticscholar.org/paper/Tree-Adjunct-Grammars-Joshi-Levy/2350cbf16800cac895b58dd5091da595822acdd6", "https://www.semanticscholar.org/paper/Parsing-With-Lexicalized-Tree-Adjoining-Grammar-Schabes-Joshi/c061924ef744ffd207017829b4529eae75d0fd72", "https://www.semanticscholar.org/paper/The-Garnet-toolkit-reference-manuals-%3A-support-for-Myers/dfa8e0b5e04859d37e1c890973c0e89cf2fe1cd3", "https://www.semanticscholar.org/paper/Using-Prototypical-Objects-to-Implement-Shared-in-Lieberman/91b4af9ff2c0f9d7985544d901f6ab2ef01fe271"]},
{"id": "1efe8329c9dec8fb276605c388a538220608b608", "title": "Computational Complexity And Natural Language", "authors": ["G. Edward Barton", "Robert C. Berwick", "Eric Sven Ristad"], "date": "1987", "abstract": "From the Publisher: \nComputational Complexity and Natural Language heralds an entirely new way of looking at grammatical systems. It applies the recently developed computer science tool of complexity theory to the study of natural language. A unified and coherent account emerges of how complexity theory can probe the information-processing structure of grammars, discovering why a grammar is easy or difficult to process and suggesting where to look for additional grammatical constraints. \nFor the linguist or cognitive scientist, the book presents a nontechnical introduction to complexity theory and discusses its strengths, its weaknesses, and how it can be used to study grammars. For the computer scientist, it offers a more sophisticated and efficient computational analysis of linguistic theories. Given the variety of new techniques rising from complexity theory, the authors foresee a developing cooperation among linguists, cognitive scientists, and computer scientists toward understanding the nature of human language. \nThe book also describes a set of case studies that use complexity theory to analyze grammatical problems. And it examines several grammatical systems currently of interest to computational linguists - including spelling-change/dictionary lookup and morphological analysis, agreement processes in natural language, and lexical-functional grammar - demonstrating how complexity analysis can illuminate and improve each one. \nAll of the authors are at the MIT Artificial Intelligence Laboratory. Robert C. Berwick is an Associate Professor in the Department of Electrical Engineering and Computer Science. A Bradford Book.", "references": []},
{"id": "dac363e9773ed9bc1e77dce431c5d0911eb57033", "title": "A New Hashing Package for UNIX", "authors": ["Margo I. Seltzer", "Ozan Yigit"], "date": "1991", "abstract": "UNIX support of disk oriented hashing was originally provided by dbm [ATT79] and subsequently improved upon in ndbm [BSD86]. In AT&T System V, in-memory hashed storage and access support was added in the hsearch library routines [ATT85]. The result is a system with two incompatible hashing schemes, each with its own set of shortcomings. This paper presents the design and performance characteristics of a new hashing package providing a superset of the functionality provided by dbm and hsearch. The new package uses linear hashing to provide efficient support of both memory based and disk based hash tables with performance superior to both dbm and hsearch under most conditions.", "references": []},
{"id": "79eb272eaf061cf4e65b8e61c9f02c027b3b6933", "title": "A decision theorectic formulation of a training problem in speech recognition and a comparison of training by unconditional versus conditional maximum likelihood", "authors": ["Arthur Nadas"], "date": "1983", "abstract": "The choice of method for training a speech recognizer is posed as an optimization problem. The currently used method of maximum likelihood, while heuristic, is shown to be superior under certain assumptions to another heuristic: the method of conditional maximum likelihood.", "references": []},
{"id": "6407680b5b551bccf50e62eaf94ab036a4d7eaa9", "title": "One-Tape, Off-Line Turing Machine Computations", "authors": ["F. C. Hennie"], "date": "1965", "abstract": "This paper has two purposes. The first is to investigate the characteristics of a restricted class of Turing machines, and to develop a simple tool for describing their computations. The second is to present specific problems for which tight lower bounds can be found for the computation times required by Turing machines of this restricted class.", "references": ["https://www.semanticscholar.org/paper/Real-time-computation-Rabin/184e0c2bb2de749db6238e9c7dcd87e7179d1f69", "https://www.semanticscholar.org/paper/On-the-computational-complexity-of-algorithms-Hartmanis-Stearns/930001f394dc4862985da58137c78e7bc3018b0c", "https://www.semanticscholar.org/paper/Real-time-computation-and-recursive-functions-not-Rabin/20e2329d84479c5891e99f2b39878d5af82eda2f"]},
{"id": "1d1b4697c7b1c48a56eb2804e7f3626eccf31b92", "title": "Processing Complex Sentences in the Centering Framework", "authors": ["Michael Strube"], "date": "1996", "abstract": "We extend the centering model for the resolution of intra-sentential anaphora and specify how to handle complex sentences. An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence.", "references": ["https://www.semanticscholar.org/paper/ParseTalk-about-Sentence-and-Text-Level-Anaphora-Strube-Hahn/ae8e97e144a54143b405e161f9aa889889e41bf1", "https://www.semanticscholar.org/paper/A-Centering-Approach-to-Pronouns-Brennan-Friedman/834d400f167fcf6ba27642eafa42e63f9e699a94", "https://www.semanticscholar.org/paper/Functional-Centering-Strube-Hahn/a46bc99d90bd064e9ccf4def70ac89b27537283b", "https://www.semanticscholar.org/paper/Stock-Of-Shared-Knowledge-A-Tool-For-Solving-Hajicov%C3%A1-Kubon/73584bf276ab3c146df639fd584aef278b9492ba", "https://www.semanticscholar.org/paper/An-Algorithm-for-Pronominal-Anaphora-Resolution-Lappin-Leass/f977f5e25adef5f0569cb9eb9b930e5146be2571", "https://www.semanticscholar.org/paper/Centering%3A-A-Framework-for-Modeling-the-Local-of-Grosz-Joshi/2c538ee7c17c71b2aaa53dcf3c9972c00e6e097b", "https://www.semanticscholar.org/paper/RAFT%2FRAPR-and-Centering%3A-A-Comparison-and-of-to-Suri-McCoy/6968236fd7ee396f3e943b9caae34bde0c01036e", "https://www.semanticscholar.org/paper/Evaluating-Discourse-Processing-Algorithms-Walker/2f817da1448fbcc34e75b7616693d171348d4b3c", "https://www.semanticscholar.org/paper/Head-Driven-Phrase-Structure-Grammar-Sag-Pollard/5e15e44506aea30dfeb995d14336b300b35b2321"]},
{"id": "2426795ca0024d7fe65b8a9a23690e0b5f2755c8", "title": "Book Reviews: Lecture on Contemporary Syntactic Theories: An Introduction to Unification-Based Approaches to Grammar", "authors": ["Neculai Curteanu"], "date": "1987", "abstract": "gerrymander, dummy, sample, foist, portray, belie, mirror, depict, typify, hippodrome, describe, describble, lament, paraphrase, blackbox, blazon and other words. Finding the words for this paragraph took an hour, and required looking at many pages of the book. Note that there is no common parent of represent and manifest. What about Wilkins? Represent is under \"Transcendental Relations of Action\" along with manifest (respectively under the subheadings comparate and simple transcendental relations). They are on facing pages and in the immediate vicinity of these two words are declare, show, exhibit, present, reveal, set forth, come to light,. render, demonstrate, and disclose. So I would rather have the older book, which also does not begin with dire warnings about what will happen to anyone who even reads a photocopy, much less makes one.", "references": ["https://www.semanticscholar.org/paper/Generalized-Phrase-Structure-Grammar-Gazdar-Klein/cdfefdebd4686a878e6572cb8ba2da9d8efbe552", "https://www.semanticscholar.org/paper/A-government-binding-parser-for-french-Wehrli/70785ff72f4ec20ddfc16399252ffc41c09f89d3", "https://www.semanticscholar.org/paper/Natural-Language-Understanding-and-Logic-Gabiola-Garc%C3%ADa-Serrano/503817d0b66bf379465c3cdccbcffc524a4fe4f5"]},
{"id": "b07e3024355b6a1ac8ee101924a8a4e3aced6fec", "title": "Incremental Centering and Center Ambiguity", "authors": ["CenteringActor", "CenterActor"], "date": "", "abstract": "of verbs in Japanese, given that the number of candidate antecedents exceeds that of the zero anaphors. Their use of functional notions, viz. topicality and empathy, naturally extend the role preferences of the underlying centering model, but unlike our case, no structural ambiguities are involved. In contrast, our approach applies to the representation of both types of ambiguities. We motivated the treatment of (structural) ambiguities within the centering framework as a consequence of assuming an incremental mode of anaphor resolution, a topic that has not been raised in the centering literature so far. This is surprising insofar as even psycholinguistic studies on centering (Gordon et al., 1993; Brennan, 1995) do not touch upon this issue, though the immediacy of anaphor resolution is a common theme in cognitive text processing studies (Just & Carpenter, 1987; Sanford & Garrod, 1989). Our proposal, based on a dependency-style grammar model (Hahn et al., 1994), claims to integrate both the sentence-level as well as text-level of anaphora analysis. Furthermore, it is also fully integrated with terminological reasoning facilities as needed for in-depth text understanding , and is based on an incremental, single-pass procedure. Thus, it is superior to the work on binding theory as developed within the GB framework (Chomsky, 1981) that is restricted to the sentence-level of analysis; just recently, however , Merlo (1993) has proposed an incremental procedure for computing intrasentential coreferences based on binding theory constraints. Also Haddock (1987) considers an incre-mental mode of anaphora resolution which boils down to a variable binding, i.e., a constraint satisfaction problem in the context of a Combinatory Categorial Grammar. Any of these approaches neglects the important aspect of a preference scaling for properly selecting among several candidate discourse units as antecedents. This drawback in the same way applies to the framework of DRT (Kamp & Reyle, 1993), which is also non-incremental. Conclusions Our approach to anaphora resolution extends the original centering model by embedding the centering approach into an incremental, single-pass processing model, by providing data structures for the centering algorithm which allow for the treatment of local and global (parsing) ambiguities, and by homogeneously integrating the resolution of sentence-level (intrasentential) as well as text-level (intersentential) anaphora based on the strict requirements set up by the binding criteria (adapted to a dependency grammar framework). The anaphora resolution module has been realized as part of a dependency parser for the German language. The parser has been \u2026", "references": ["https://www.semanticscholar.org/paper/What%2C-when%2C-and-how%3A-Questions-of-immediacy-in-Sanford-Garrod/440c2c4c3e7cbb1573a912515607c3f4ac4e7ef4", "https://www.semanticscholar.org/paper/An-On-Line-Computational-Model-of-Human-Sentence-Jurafsky/d633a99c34548c293ee20ddcec8e0dffdd4e5e13", "https://www.semanticscholar.org/paper/Centering-attention-in-discourse-Brennan/1f797388bd59c7da0484fd4598d60613a05462a8", "https://www.semanticscholar.org/paper/Functional-Centering-Strube-Hahn/a46bc99d90bd064e9ccf4def70ac89b27537283b", "https://www.semanticscholar.org/paper/Providing-a-Unified-Account-of-Definite-Noun-in-Grosz-Joshi/14dd4ab5cda0225b457f425b271888901005b819", "https://www.semanticscholar.org/paper/Incremental-Interpretation-and-Combinatory-Grammar-Haddock/b45e2c912b5a686bf09b6573b14148022af57e52", "https://www.semanticscholar.org/paper/ParseTalk-about-Sentence-and-Text-Level-Anaphora-Strube-Hahn/ae8e97e144a54143b405e161f9aa889889e41bf1", "https://www.semanticscholar.org/paper/Restricted-Parallelism-in-Object-Oriented-Lexical-Neuhaus-Hahn/59780454743fd7543f5f3337de3970a6c2966472", "https://www.semanticscholar.org/paper/Actalk%3A-A-Testbed-for-Classifying-and-Designing-in-Briot/26b1852b406c85ebf8e19039d641e59364b1537e"]},
{"id": "d24e47fe85f4bd38e30106cdc7d125a91305be7d", "title": "Computing grammatical fimctions from a configurational parse tree", "authors": ["Shalom Lappin", "Igal Golan", "Mori Rimon"], "date": "1989", "abstract": "Semantic Scholar extracted view of \"Computing grammatical fimctions from a configurational parse tree\" by Shalom Lappin et al.", "references": []},
{"id": "957d8108d740edae9499d4784b3be2427d28871b", "title": "A Language-Independent Anaphora Resolution System for Understanding Multilingual Texts", "authors": ["Chinatsu Aone", "Douglas McKee"], "date": "1993", "abstract": "This paper describes a new discourse module within our multilingual NLP system. Because of its unique data-driven architecture, the discourse module is language-independent. Moreover, the use of hierarchically organized multiple knowledge sources makes the module robust and trainable using discourse-tagged corpora. Separating discourse phenomena from knowledge sources makes the discourse module easily extensible to additional phenomena.", "references": ["https://www.semanticscholar.org/paper/The-MURASAKI-Project%3A-Multilingual-Natural-Language-Aone-Blejer/38c6bff60dd330cfab6ddd063c6f54c8ef60d4a3", "https://www.semanticscholar.org/paper/An-Architecture-for-Anaphora-Resolution-Rich-LuperFoy/9f975473b7b0261db043ea150e52923d1a97eb40", "https://www.semanticscholar.org/paper/Evaluating-Discourse-Processing-Algorithms-Walker/2f817da1448fbcc34e75b7616693d171348d4b3c", "https://www.semanticscholar.org/paper/A-Computational-Mechanism-for-Pronominal-Reference-Ingria-Stallard/8f7737359a20babdee1f3b383a31585b4bf65ce1", "https://www.semanticscholar.org/paper/Anaphora-resolution%3A-a-multi-strategy-approach-Carbonell-Brown/08c4176dcc331cf479c73f77e40dccb7e15496fb", "https://www.semanticscholar.org/paper/Discourse-Entities-in-Janus-Ayuso/7fa3d0c646a433b594ac8c66d4780a8db211f8ec", "https://www.semanticscholar.org/paper/Advances-in-Ma-chine-Translation-Research-in-IBM-Rim%C3%B3n-McCord/c67dfd06409ef9b875fd302514c588fb02e9c56c", "https://www.semanticscholar.org/paper/A-Centering-Approach-to-Pronouns-Brennan-Friedman/834d400f167fcf6ba27642eafa42e63f9e699a94", "https://www.semanticscholar.org/paper/Providing-a-Unified-Account-of-Definite-Noun-in-Grosz-Joshi/14dd4ab5cda0225b457f425b271888901005b819", "https://www.semanticscholar.org/paper/Development-and-Evaluation-of-a-Broad-Coverage-of-Black-Lafferty/f3a19430f47bf8370a894c93857c0be0f913ac3b"]},
{"id": "a27fc84fb14188f7149bd5cae9b24b7eaed3e588", "title": "Hidden Markov models, maximum mutual information estimation, and the speech recognition problem", "authors": ["Yves Normandin"], "date": "1992", "abstract": "Hidden Markov Models (HMMs) are one of the most powerful speech recognition tools available today. Even so, the inadequacies of HMMs as a \"correct\" modeling framework for speech are well known. In that context, we argue that the maximum mutual information estimation (MMIE) formulation for training is more appropriate vis-a-vis maximum likelihood estimation (MLE) for reducing the error rate. We also show how MMIE paves the way for new training possibilities. \nWe introduce Corrective MMIE training, a very efficient new training algorithm which uses a modified version of a discrete reestimation formula recently proposed by Gopalakrishnan et al. We propose reestimation formulas for the case of diagonal Gaussian densities, experimentally demonstrate their convergence properties, and integrate them into our training algorithm. In a connected digit recognition task, MMIE consistently improves the recognition performance of our recognizer.", "references": []},
{"id": "3f1cde8cafbe2e682d1b2b41e57e4358c3f4ce5c", "title": "Optimal solution of a training problem in speech recognition", "authors": ["Arthur Nadas"], "date": "1985", "abstract": "We take the view that the payoff correpsonding to different ways of training a speech recognizer is the probability that the recognizer will correctly recognize a randomly chosen word. In \"A Decision Theoretic Formulation of a Training Problem in Speech Recognition\" we posed the problem of choosing a speech recognizer as an optimization problem with the expected value of the above payoff as the objective function. This correspondence presents the optimal Bayes solution to this optimization problem by maximizing the expected payoff: conditionally on given training data decode the acoustic signal for a word as any word which maximizes the a posteriori expected joint probability of the word and the acoustic signal. Thus the probability estimator which is optimal for mean-squared error produces a decoder which happens to be optimal for recognition rate as well.", "references": []},
{"id": "722a3c365a134a9f9b9ae1511f018d9b1ecff3de", "title": "An empirical study of learning speed in back-propagation networks", "authors": ["Scott E. Fahlman"], "date": "1988", "abstract": "Most connectionist or \"neural network\" learning systems use some form of the back-propagation algorithm. However, back-propagation learning is too slow for many applications, and it scales up poorly as tasks become larger and more complex. The factors governing learning speed are poorly understood. I have begun a systematic, empirical study of learning speed in backprop-like algorithms, measured against a variety of benchmark problems. The goal is twofold: to develop faster learning algorithms and to contribute to the development of a methodology that will be of value in future studies of this kind. This paper is a progress report describing the results obtained during the first six months of this study. To date I have looked only at a limited set of benchmark problems, but the results on these are encouraging: I have developed a new learning algorithm that is faster than standard backprop by an order of magnitude or more and that appears to scale up very well as the problem size increases. This research was sponsored in part by the National Science Foundation under Contract Number EET-8716324 and by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4976 under Contract F33615-87C-1499 and monitored by the Avionics Laboratory, Air Force Wright Aeronautical Laboratories, Aeronautical Systems Division (AFSC), Wright-Patterson AFB, OH 45433-6543. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of these agencies or of the U.S. Government.", "references": ["https://www.semanticscholar.org/paper/Experiments-on-Learning-by-Back-Propagation.-Plaut-Nowlan/4a42b2104ca8ff891ae77c40a915d4c94c8f8428", "https://www.semanticscholar.org/paper/Accelerated-Learning-in-Back-propagation-Nets-Pfeifer-Schreter/4141d168a23d5bc7d65a840159a460dc911a2aef", "https://www.semanticscholar.org/paper/Learning-Algorithms-for-Connectionist-Networks%3A-of-Watrous/934e49dac717a924bfda841bf6e54c32e900f0d1", "https://www.semanticscholar.org/paper/Increased-rates-of-convergence-through-learning-Jacobs/a9ef2995e8e1bd57a74343073219364811c2ace0", "https://www.semanticscholar.org/paper/Connectionist-Architectures-for-Artificial-Fahlman-Hinton/706ae842fb2107f5940cfa39682d52235d269eb7", "https://www.semanticscholar.org/paper/Neural-network-simulation-at-Warp-speed%3A-how-we-got-Pomerleau-Gusciora/953ebd65e9da608c5923005815632aeb9741b2c5", "https://www.semanticscholar.org/paper/Learning-internal-representations-by-error-Rumelhart-Hinton/111fd833a4ae576cfdbb27d87d2f8fc0640af355", "https://www.semanticscholar.org/paper/AN-IMPROVED-THREE-LAYER%2C-BACK-PROPAGATION-ALGORITHM-Stornetta-Huberman/0e7742958d272c241e5b2fa4ad530518a169c684", "https://www.semanticscholar.org/paper/Beyond-Regression-%3A-%22New-Tools-for-Prediction-and-Werbos/56623a496727d5c71491850e04512ddf4152b487", "https://www.semanticscholar.org/paper/Optimal-algorithms-for-adaptive-networks%3A-Second-Parker/42fd9e2dcbed25f31f84ec0c0bb0f6288226fbb5"]},
{"id": "805d4012921110c2ba95d687a2aab57f82cd92ea", "title": "The general use of tying in phoneme-based HMM speech recognisers", "authors": ["Steve J. Young"], "date": "1992", "abstract": "A method of manipulating sets of hidden Markov models (HMMs) by applying various kinds of parameter tying operations is described, the aim being to synthesize compact and robust context dependent models. The method is illustrated via an experiment to build a set of generalized triphone models for the TIMIT database in which triphones are constructed by joining together left and right dependent biphones. Although simple, the method results in good performance and avoids the need to train large numbers of triphones. The use of tying to increase model robustness is also investigated. Tying the center states within triphones of the same phoneme class and tying variances within states is beneficial, but larger-scale tying of variances leads to degraded performance.<<ETX>>", "references": []},
{"id": "2e3f85b724a01603d250143309a6d857902c0f3e", "title": "Anaphora Resolution in Machine Translation", "authors": ["Preusz Susanne", "Birte Schmitz", "Christa Hauenschild", "Carla Umbach"], "date": "1992", "abstract": "In this paper we give an overview of an approach to anaphora resolution that takes a whole variety of different factors into account. These factors concern on the one hand structural information like agreement, proximity, binding principles, subject preference, topic preference, negative preference for free adjuncts and on the other hand information about the contents of a text like conceptual consistency. This led to a twofold text representation: a structural text representation and a referential text representation. The factors are implemented as preference rules with different weights that express the influence each of the factors has in the process of anaphora resolution. For each factor a linguistic motivation as well as a formal representation of the information it works on is given. We show how we integrated the anaphora resolution component into the existing experimental MT system.", "references": ["https://www.semanticscholar.org/paper/Discourse-Processing-in-MT%3A-Problems-in-Pronominal-Wada/ee4a51a30fe6999124284228373ecd27131cafdd", "https://www.semanticscholar.org/paper/An-Architecture-for-Anaphora-Resolution-Rich-LuperFoy/9f975473b7b0261db043ea150e52923d1a97eb40", "https://www.semanticscholar.org/paper/Resolving-pronoun-references-Hobbs/61c0eaf156647ffad23921a65e3d7b3296f7afd5", "https://www.semanticscholar.org/paper/Anaphora-resolution%3A-a-multi-strategy-approach-Carbonell-Brown/08c4176dcc331cf479c73f77e40dccb7e15496fb", "https://www.semanticscholar.org/paper/An-Integrated-Model-For-Anaphora-Resolution-Mitkov/bcc14d3eb368be2eea3f93c406cf09202e4c727e", "https://www.semanticscholar.org/paper/Anaphora-in-Natural-Language-Understanding%3A-A-Hirst/2b445ee22c239c3b089f5d43693b70523a03d4e6", "https://www.semanticscholar.org/paper/A-Computational-Mechanism-for-Pronominal-Reference-Ingria-Stallard/8f7737359a20babdee1f3b383a31585b4bf65ce1", "https://www.semanticscholar.org/paper/CAT-2-Implementing-a-Formalism-for-Multi-Lingual-MT-Sharp/1de01b0497f978da57dadcc831ba291180116b03", "https://www.semanticscholar.org/paper/CAT2%3A-An-experimental-eurotra-alternative-Sharp/9945d2374b2e3a484a1f22c4dfa4625ca8c7f46f", "https://www.semanticscholar.org/paper/Machine-translation-a-knowledge-based-approach-Nirenburg-Carbonell/170bc48d62d15f7c6ad14ba3a3aab47b64f15b29"]},
{"id": "f4bbc00fdbcd8b96010d1d8db94f3bdf0e03ac48", "title": "A two-level analysis of french", "authors": ["Lun Shan"], "date": "1983", "abstract": "Semantic Scholar extracted view of \"A two-level analysis of french\" by Lun Shan", "references": []},
{"id": "2bad5215293ee18cd251c924ffc4ae3280935102", "title": "A two-level morphological analysis of japanese", "authors": ["Yukiko Sasaki Alam"], "date": "1983", "abstract": "Semantic Scholar extracted view of \"A two-level morphological analysis of japanese\" by Yukiko Sasaki Alam", "references": []},
{"id": "2210bd1ef7a961220687dd9081424b3558727a38", "title": "Mathematical and computational aspects of lexicalized grammars", "authors": ["Yves Schabes", "Aravind K. Joshi"], "date": "1990", "abstract": "Most current linguistic theories give lexical accounts of several phenomena that used to be considered purely syntactic. The information put in the lexicon is thereby increased both in amount and complexity. We explore the view that syntactic rules are not separated from lexical items. In this approach, each elementary structure is associated with a lexical item called the anchor. These structures specify extended domains of locality (as compared to context-free grammars) over which constraints can be stated. The 'grammar' consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the anchor. There are 'rules' which tell us how these structures are composed. A grammar of this form will be said to be lexicalized. \nThe process of lexicalization of context-free grammars (CFGs) constrained by linguistic requirements forces us to use operations for combining structures that make the formalism fall in the class of mildly context sensitive languages. We show that substitution, the combining operation corresponding to CFGs, does not allow one to lexicalize CFGs but the combination of substitution and adjunction does. We show how tree-adjoining grammar (TAG) is derived from the lexicalization process of CFGs. Then we show that TAGs are closed under lexicalization and we illustrate the main structures found in a lexicalized TAG for English. The properties of TAGs permit us to encapsulate diverse syntactic phenomena in a very natural way. TAG's extended domain of locality and its factoring of recursion from local dependencies enable us to localize many syntactic dependencies (such as filler-gap) as well as semantic dependencies (such as predicate-arguments). \nWe investigate the processing of lexicalized TAGs. We first present two general practical parsers that follow Earley-style parsing. They are practical parsers for TAGs because, as for CFGs, the average behavior of Earley-type parsers is superior to its worst case complexity. They are both left to right bottom-up parsers that use top-down predictions but they differ in the way the top down prediction is used. \nThen we explain the building of a set of deterministic bottom-up left to right parsers which analyze a subset of tree-adjoining languages. The LR parsing strategy for CFGs is extended to TAG by using a machine, called Bottom-up Embedded Push Down Automaton (BEPDA), that recognizes in a bottom-up fashion the set of tree-adjoining languages (and exactly this set). \nFinally we show how lexicalized grammars suggest a natural two-step parsing strategy. We consider lexicalized TAGs as an instance of lexicalized grammar and we examine the effect of the two-step parsing strategy on main types of parsing algorithms.", "references": []},
{"id": "20e2329d84479c5891e99f2b39878d5af82eda2f", "title": "Real-time computation and recursive functions not real-time computable", "authors": ["Michael O. Rabin"], "date": "1963", "abstract": "Semantic Scholar extracted view of \"Real-time computation and recursive functions not real-time computable\" by Michael O. Rabin", "references": []},
